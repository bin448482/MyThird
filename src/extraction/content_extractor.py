"""
å†…å®¹æå–å™¨

ä½œä¸ºé«˜å±‚åè°ƒå™¨ï¼Œä¸“æ³¨äºï¼š
- æµç¨‹ç®¡ç†ï¼šåè°ƒæ•´ä¸ªæå–è¿‡ç¨‹çš„æ‰§è¡Œæµç¨‹
- æµè§ˆå™¨ä¼šè¯ç®¡ç†ï¼šå¤„ç†æµè§ˆå™¨åˆ›å»ºã€å¯¼èˆªã€ä¼šè¯ä¿æŒ
- ç»„ä»¶åè°ƒï¼šç»Ÿä¸€è°ƒåº¦PageParserã€DataStorageç­‰ç»„ä»¶
- ç»“æœå¤„ç†ï¼šæ±‡æ€»ã€ä¿å­˜å’Œç®¡ç†æå–ç»“æœ

å…·ä½“çš„é¡µé¢è§£æå·¥ä½œå§”æ‰˜ç»™PageParserå®Œæˆ
"""

import time
import logging
import random
from typing import Dict, List, Optional, Any
from urllib.parse import urlparse

from .page_parser import PageParser
from .data_storage import DataStorage
from ..auth.browser_manager import BrowserManager
from ..auth.session_manager import SessionManager
from ..search.url_builder import SearchURLBuilder
from ..utils.behavior_simulator import BehaviorSimulator
from ..core.exceptions import ContentExtractionError


class ContentExtractor:
    """
    å†…å®¹æå–å™¨ - é«˜å±‚åè°ƒå™¨
    
    èŒè´£ï¼š
    1. æµç¨‹ç®¡ç† - æ§åˆ¶æå–æµç¨‹çš„æ‰§è¡Œé¡ºåº
    2. æµè§ˆå™¨ä¼šè¯ - ç®¡ç†æµè§ˆå™¨å®ä¾‹å’Œä¼šè¯çŠ¶æ€
    3. ç»„ä»¶åè°ƒ - ç»Ÿä¸€è°ƒåº¦å„ä¸ªåŠŸèƒ½ç»„ä»¶
    4. ç»“æœå¤„ç† - æ±‡æ€»å’Œä¿å­˜æå–ç»“æœ
    
    ä¸ç›´æ¥è¿›è¡Œé¡µé¢è§£æï¼Œè€Œæ˜¯å§”æ‰˜ç»™ä¸“é—¨çš„ç»„ä»¶
    """
    
    def __init__(self, config: dict, browser_manager: Optional[BrowserManager] = None):
        """
        åˆå§‹åŒ–å†…å®¹æå–å™¨
        
        Args:
            config: é…ç½®å­—å…¸
            browser_manager: æµè§ˆå™¨ç®¡ç†å™¨å®ä¾‹ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨åˆ›å»º
        """
        self.config = config
        self.mode_config = config.get('mode', {})
        self.search_config = config.get('search', {})
        
        # ç»„ä»¶åˆå§‹åŒ–
        self.browser_manager = browser_manager or BrowserManager(config)
        self.session_manager = SessionManager(config)
        self.page_parser = PageParser(config)
        self.data_storage = DataStorage(config)
        self.url_builder = SearchURLBuilder(config)
        self.behavior_simulator = None  # å»¶è¿Ÿåˆå§‹åŒ–ï¼Œéœ€è¦driverå®ä¾‹
        
        self.logger = logging.getLogger(__name__)
        
        # çŠ¶æ€ç®¡ç†
        self.current_keyword = None
        self.extraction_results = []
        self.extraction_start_time = None
    
    def extract_from_search_url(self,
                               search_url: str,
                               keyword: Optional[str] = None,
                               max_results: Optional[int] = None,
                               save_results: bool = True,
                               extract_details: bool = False,
                               max_pages: Optional[int] = None) -> List[Dict[str, Any]]:
        """
        ä»æœç´¢URLæå–èŒä½ä¿¡æ¯ï¼ˆæ”¯æŒå¤šé¡µï¼‰
        
        Args:
            search_url: æœç´¢é¡µé¢URL
            keyword: æœç´¢å…³é”®è¯ï¼ˆç”¨äºè®°å½•ï¼‰
            max_results: æœ€å¤§ç»“æœæ•°é‡
            save_results: æ˜¯å¦ä¿å­˜ç»“æœ
            extract_details: æ˜¯å¦æå–è¯¦æƒ…é¡µå†…å®¹ï¼ˆéœ€è¦ç‚¹å‡»è·å–URLï¼‰
            max_pages: æœ€å¤§é¡µæ•°ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é…ç½®ä¸­çš„å€¼
            
        Returns:
            æå–çš„èŒä½ä¿¡æ¯åˆ—è¡¨
            
        Raises:
            ContentExtractionError: å†…å®¹æå–å¤±è´¥
        """
        try:
            self.logger.info(f"ğŸš€ å¼€å§‹ä»æœç´¢URLæå–å†…å®¹: {search_url}")
            self.extraction_start_time = time.time()
            self.current_keyword = keyword or self._extract_keyword_from_url(search_url)
            max_jobs = max_results or self.search_config.get('strategy', {}).get('max_results_per_keyword', 50)
            
            # è·å–åˆ†é¡µé…ç½®
            strategy_config = self.search_config.get('strategy', {})
            enable_pagination = strategy_config.get('enable_pagination', True)
            max_pages_config = max_pages or strategy_config.get('max_pages', 10)
            page_delay_min = strategy_config.get('page_delay', 2)
            page_delay_max = strategy_config.get('page_delay_max', 5)
            
            self.logger.info(f"ğŸ“„ åˆ†é¡µé…ç½®: å¯ç”¨={enable_pagination}, æœ€å¤§é¡µæ•°={max_pages_config}")
            
            # 1. å‡†å¤‡æµè§ˆå™¨
            driver = self._prepare_browser()
            
            # 2. å¯¼èˆªåˆ°æœç´¢é¡µé¢
            self._navigate_to_page(driver, search_url)
            
            # 3. å¤šé¡µæå–é€»è¾‘
            all_results = []
            current_page = 1
            
            while current_page <= max_pages_config:
                try:
                    self.logger.info(f"ğŸ“Š æ­£åœ¨å¤„ç†ç¬¬ {current_page} é¡µ")
                    
                    # è·å–å½“å‰é¡µé¢ä¿¡æ¯
                    page_info = self.page_parser.get_current_page_info(driver)
                    self.logger.info(f"ğŸ“ å½“å‰é¡µé¢: {page_info.get('current_page', current_page)}")
                    
                    # è§£æå½“å‰é¡µé¢çš„èŒä½ä¿¡æ¯
                    page_results = self.page_parser.parse_job_list(driver, max_jobs)
                    
                    if page_results:
                        # ä¸ºæ¯ä¸ªç»“æœæ·»åŠ é¡µç ä¿¡æ¯
                        for result in page_results:
                            result['page_number'] = current_page
                            result['keyword'] = self.current_keyword
                        
                        all_results.extend(page_results)
                        self.logger.info(f"âœ… ç¬¬ {current_page} é¡µæå–åˆ° {len(page_results)} ä¸ªèŒä½")
                        
                        # æ£€æŸ¥æ˜¯å¦å·²è¾¾åˆ°æœ€å¤§ç»“æœæ•°
                        if len(all_results) >= max_jobs:
                            self.logger.info(f"ğŸ“Š å·²è¾¾åˆ°æœ€å¤§ç»“æœæ•°é™åˆ¶: {max_jobs}")
                            all_results = all_results[:max_jobs]  # æˆªå–åˆ°æŒ‡å®šæ•°é‡
                            break
                    else:
                        self.logger.warning(f"âš ï¸ ç¬¬ {current_page} é¡µæœªæå–åˆ°èŒä½ä¿¡æ¯")
                    
                    # æ£€æŸ¥æ˜¯å¦å¯ç”¨åˆ†é¡µä»¥åŠæ˜¯å¦æœ‰ä¸‹ä¸€é¡µ
                    if not enable_pagination or current_page >= max_pages_config:
                        self.logger.info(f"ğŸ“„ å·²è¾¾åˆ°æœ€å¤§é¡µæ•°é™åˆ¶: {max_pages_config}")
                        break
                    
                    if not self.page_parser.has_next_page(driver):
                        self.logger.info("ğŸ“„ æ²¡æœ‰æ›´å¤šé¡µé¢ï¼Œæå–å®Œæˆ")
                        break
                    
                    # å¯¼èˆªåˆ°ä¸‹ä¸€é¡µ
                    self.logger.info(f"ğŸ”„ å‡†å¤‡å¯¼èˆªåˆ°ç¬¬ {current_page + 1} é¡µ")
                    
                    # é¡µé¢é—´å»¶è¿Ÿ
                    delay_time = random.uniform(page_delay_min, page_delay_max)
                    self.logger.info(f"â³ é¡µé¢é—´å»¶è¿Ÿ {delay_time:.1f} ç§’")
                    time.sleep(delay_time)
                    
                    if not self.page_parser.navigate_to_next_page(driver):
                        self.logger.warning("âŒ å¯¼èˆªåˆ°ä¸‹ä¸€é¡µå¤±è´¥ï¼Œåœæ­¢åˆ†é¡µæå–")
                        break
                    
                    current_page += 1
                    
                except Exception as e:
                    self.logger.error(f"âŒ å¤„ç†ç¬¬ {current_page} é¡µæ—¶å‡ºé”™: {e}")
                    break
            
            # 4. å¦‚æœéœ€è¦è¯¦æƒ…é¡µå†…å®¹ï¼Œå¤„ç†æ‰€æœ‰é¡µé¢çš„ç»“æœ
            if extract_details and all_results:
                self.logger.info("ğŸ”— å¼€å§‹æå–è¯¦æƒ…é¡µURLå’Œå†…å®¹...")
                
                # é‡æ–°å¯¼èˆªåˆ°ç¬¬ä¸€é¡µè¿›è¡Œè¯¦æƒ…æå–ï¼ˆå¦‚æœéœ€è¦ï¼‰
                if current_page > 1:
                    self.logger.info("ğŸ”„ è¿”å›ç¬¬ä¸€é¡µè¿›è¡Œè¯¦æƒ…æå–")
                    self._navigate_to_page(driver, search_url)
                
                # 4.1 é€šè¿‡ç‚¹å‡»è·å–è¯¦æƒ…é¡µURLï¼ˆä»…å¤„ç†å‰å‡ é¡µçš„ç»“æœä»¥é¿å…è¿‡é•¿æ—¶é—´ï¼‰
                detail_extract_limit = min(len(all_results), 20)  # é™åˆ¶è¯¦æƒ…æå–æ•°é‡
                url_results = self.page_parser.extract_job_urls_by_clicking(driver, detail_extract_limit)
                
                if url_results:
                    # 4.2 æå–è¯¦æƒ…é¡µå†…å®¹
                    detail_urls = [item['detail_url'] for item in url_results if item.get('detail_url')]
                    if detail_urls:
                        details = self.extract_job_details(detail_urls)
                        
                        # 4.3 åˆå¹¶åŸºæœ¬ä¿¡æ¯å’Œè¯¦æƒ…ä¿¡æ¯
                        self._merge_job_data(all_results, url_results, details)
            
            # 5. å¤„ç†ç»“æœ
            if all_results:
                self.extraction_results = all_results
                
                # 6. ä¿å­˜ç»“æœï¼ˆå¦‚æœéœ€è¦ï¼‰
                if save_results:
                    self._save_extraction_results(all_results)
                
                self.logger.info(f"âœ… å¤šé¡µå†…å®¹æå–å®Œæˆï¼Œå…± {current_page} é¡µï¼Œæå– {len(all_results)} ä¸ªèŒä½")
            else:
                self.logger.warning("âš ï¸ æœªæå–åˆ°ä»»ä½•èŒä½ä¿¡æ¯")
            
            return all_results
            
        except Exception as e:
            self.logger.error(f"âŒ å†…å®¹æå–å¤±è´¥: {e}")
            raise ContentExtractionError(f"å†…å®¹æå–å¤±è´¥: {e}")
    
    def extract_from_keyword(self,
                           keyword: str,
                           max_results: Optional[int] = None,
                           save_results: bool = True,
                           extract_details: bool = False,
                           max_pages: Optional[int] = None) -> List[Dict[str, Any]]:
        """
        æ ¹æ®å…³é”®è¯æå–èŒä½ä¿¡æ¯ï¼ˆæ”¯æŒå¤šé¡µï¼‰
        
        Args:
            keyword: æœç´¢å…³é”®è¯
            max_results: æœ€å¤§ç»“æœæ•°é‡
            save_results: æ˜¯å¦ä¿å­˜ç»“æœ
            extract_details: æ˜¯å¦æå–è¯¦æƒ…é¡µå†…å®¹
            max_pages: æœ€å¤§é¡µæ•°ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é…ç½®ä¸­çš„å€¼
            
        Returns:
            æå–çš„èŒä½ä¿¡æ¯åˆ—è¡¨
        """
        try:
            # æ„å»ºæœç´¢URL
            search_url = self.url_builder.build_search_url(keyword)
            self.logger.info(f"ğŸ” ä½¿ç”¨å…³é”®è¯ '{keyword}' æ„å»ºæœç´¢URL: {search_url}")
            
            # æå–å†…å®¹
            return self.extract_from_search_url(
                search_url,
                keyword=keyword,
                max_results=max_results,
                save_results=save_results,
                extract_details=extract_details,
                max_pages=max_pages
            )
            
        except Exception as e:
            self.logger.error(f"âŒ åŸºäºå…³é”®è¯çš„å†…å®¹æå–å¤±è´¥: {e}")
            raise ContentExtractionError(f"åŸºäºå…³é”®è¯çš„å†…å®¹æå–å¤±è´¥: {e}")
    
    def extract_job_details(self, job_urls: List[str]) -> List[Dict[str, Any]]:
        """
        æå–èŒä½è¯¦æƒ…ä¿¡æ¯
        
        Args:
            job_urls: èŒä½è¯¦æƒ…é¡µURLåˆ—è¡¨
            
        Returns:
            èŒä½è¯¦æƒ…ä¿¡æ¯åˆ—è¡¨
        """
        try:
            self.logger.info(f"ğŸ“„ å¼€å§‹æå– {len(job_urls)} ä¸ªèŒä½çš„è¯¦æƒ…ä¿¡æ¯")
            
            driver = self._prepare_browser()
            details = []
            
            # éšæœºæ‰“ä¹±URLé¡ºåºï¼Œé¿å…æŒ‰é¡ºåºè®¿é—®çš„æ¨¡å¼
            shuffled_urls = job_urls.copy()
            random.shuffle(shuffled_urls)
            
            for i, job_url in enumerate(shuffled_urls, 1):
                try:
                    self.logger.info(f"ğŸ“ æå–èŒä½è¯¦æƒ… {i}/{len(shuffled_urls)}: {job_url}")
                    
                    # è¯¦æƒ…é¡µï¼šå»¶é•¿ç­‰å¾…æ—¶é—´ï¼Œé¿å…åçˆ¬æ£€æµ‹
                    if self.behavior_simulator:
                        self.behavior_simulator.random_delay(2.0, 5.0)  # è¯¦æƒ…é¡µå»¶é•¿åˆ°2-5ç§’
                    else:
                        time.sleep(random.uniform(3.0, 8.0))  # è¯¦æƒ…é¡µå»¶é•¿åˆ°3-8ç§’
                    
                    # ä½¿ç”¨ç®€åŒ–çš„è¯¦æƒ…é¡µè§£æ
                    detail = self._extract_job_detail_simplified(driver, job_url)
                    if detail:
                        details.append(detail)
                        
                        # ä¿å­˜å•ä¸ªèŒä½è¯¦æƒ…
                        self.data_storage.save_job_detail(detail, job_url)
                        
                        # è¯¦æƒ…é¡µï¼šå¢åŠ é˜…è¯»æ—¶é—´æ¨¡æ‹Ÿ
                        if self.behavior_simulator and random.random() < 0.6:  # å¢åŠ åˆ°60%æ¦‚ç‡æ¨¡æ‹Ÿé˜…è¯»
                            self.behavior_simulator.random_delay(1.0, 3.0)  # å»¶é•¿é˜…è¯»æ—¶é—´
                    
                    # è¯¦æƒ…é¡µï¼šé€‚å½“å¢åŠ ä¼‘æ¯æ—¶é—´
                    if i % random.randint(5, 8) == 0:  # è¯¦æƒ…é¡µä¼‘æ¯é¢‘ç‡é€‚ä¸­
                        rest_time = random.uniform(4.0, 8.0)  # è¯¦æƒ…é¡µä¼‘æ¯æ—¶é—´å»¶é•¿
                        self.logger.info(f"â³ æ¨¡æ‹Ÿç”¨æˆ·ä¼‘æ¯ {rest_time:.1f} ç§’...")
                        time.sleep(rest_time)
                    
                except Exception as e:
                    self.logger.warning(f"âš ï¸ æå–èŒä½è¯¦æƒ…å¤±è´¥ {job_url}: {e}")
                    # è¯¦æƒ…é¡µå¤±è´¥åå»¶é•¿ç­‰å¾…æ—¶é—´
                    time.sleep(random.uniform(10.0, 25.0))  # ä»5-15ç§’å»¶é•¿åˆ°10-25ç§’
                    continue
            
            self.logger.info(f"âœ… èŒä½è¯¦æƒ…æå–å®Œæˆï¼Œå…±æå– {len(details)} ä¸ªè¯¦æƒ…")
            return details
            
        except Exception as e:
            self.logger.error(f"âŒ èŒä½è¯¦æƒ…æå–å¤±è´¥: {e}")
            raise ContentExtractionError(f"èŒä½è¯¦æƒ…æå–å¤±è´¥: {e}")
    
    def _extract_job_detail_simplified(self, driver, job_url: str) -> Optional[Dict[str, Any]]:
        """
        ç®€åŒ–çš„èŒä½è¯¦æƒ…æå–æ–¹æ³•
        
        Args:
            driver: WebDriverå®ä¾‹
            job_url: èŒä½è¯¦æƒ…URL
            
        Returns:
            èŒä½è¯¦æƒ…æ•°æ®
        """
        try:
            # ç›´æ¥å¯¼èˆªï¼Œå‡å°‘å¤æ‚çš„æ¨¡æ‹Ÿ
            driver.get(job_url)
            
            # è¯¦æƒ…é¡µï¼šå»¶é•¿ç­‰å¾…æ—¶é—´
            time.sleep(random.uniform(2.0, 4.0))  # ä»1-2.5ç§’å»¶é•¿åˆ°2-4ç§’
            
            # æ£€æŸ¥æ˜¯å¦è¢«é‡å®šå‘æˆ–é˜»æ­¢
            current_url = driver.current_url
            if 'error' in current_url.lower() or 'block' in current_url.lower() or 'captcha' in current_url.lower():
                self.logger.warning(f"æ£€æµ‹åˆ°å¯èƒ½çš„åçˆ¬é¡µé¢: {current_url}")
                return None
            
            # ç®€åŒ–çš„é¡µé¢äº¤äº’ï¼šåªåšåŸºæœ¬æ»šåŠ¨
            if self.behavior_simulator and random.random() < 0.5:  # 50%æ¦‚ç‡è¿›è¡Œç®€å•æ»šåŠ¨
                self.behavior_simulator.simulate_scroll('down', random.randint(300, 500))
                time.sleep(random.uniform(0.5, 1.0))
                driver.execute_script("window.scrollTo(0, 0);")
            
            # è§£æé¡µé¢å†…å®¹
            detail_data = self.page_parser.parse_job_detail(driver, job_url)
            
            return detail_data
            
        except Exception as e:
            self.logger.error(f"ç®€åŒ–æå–èŒä½è¯¦æƒ…å¤±è´¥: {e}")
            return None
    
        
    def extract_job_urls_by_clicking(self,
                                   search_url: str,
                                   max_jobs: int = 10,
                                   save_results: bool = True) -> List[Dict[str, Any]]:
        """
        é€šè¿‡æ¨¡æ‹Ÿç‚¹å‡»æå–èŒä½è¯¦æƒ…é¡µURL
        
        ContentExtractorä½œä¸ºåè°ƒå™¨ï¼Œè´Ÿè´£æµç¨‹ç®¡ç†å’Œæµè§ˆå™¨ä¼šè¯
        å…·ä½“çš„é¡µé¢è§£æå·¥ä½œå§”æ‰˜ç»™PageParser
        
        Args:
            search_url: æœç´¢é¡µé¢URL
            max_jobs: æœ€å¤§æå–æ•°é‡
            save_results: æ˜¯å¦ä¿å­˜ç»“æœ
            
        Returns:
            èŒä½URLä¿¡æ¯åˆ—è¡¨
        """
        try:
            self.logger.info(f"ğŸ”— å¼€å§‹æå–èŒä½è¯¦æƒ…é¡µURL: {search_url}")
            
            # 1. å‡†å¤‡æµè§ˆå™¨ä¼šè¯ï¼ˆContentExtractorçš„èŒè´£ï¼‰
            driver = self._prepare_browser()
            
            # 2. å¯¼èˆªåˆ°æœç´¢é¡µé¢ï¼ˆContentExtractorçš„èŒè´£ï¼‰
            self._navigate_to_page(driver, search_url)
            
            # 3. å§”æ‰˜PageParserè¿›è¡Œé¡µé¢è§£æå’ŒURLæå–
            extracted_urls = self.page_parser.extract_job_urls_by_clicking(driver, max_jobs)
            
            # 4. å¤„ç†ç»“æœä¿å­˜ï¼ˆContentExtractorçš„èŒè´£ï¼‰
            if save_results and extracted_urls:
                filename = self._save_url_extraction_results(extracted_urls)
                if filename:
                    self.logger.info(f"ğŸ’¾ URLæå–ç»“æœå·²ä¿å­˜: {filename}")
            
            return extracted_urls
            
        except Exception as e:
            self.logger.error(f"âŒ URLæå–å¤±è´¥: {e}")
            raise ContentExtractionError(f"URLæå–å¤±è´¥: {e}")
    
    def extract_job_urls_from_keyword(self,
                                    keyword: str,
                                    max_jobs: int = 10,
                                    save_results: bool = True) -> List[Dict[str, Any]]:
        """
        æ ¹æ®å…³é”®è¯æå–èŒä½è¯¦æƒ…é¡µURL
        
        Args:
            keyword: æœç´¢å…³é”®è¯
            max_jobs: æœ€å¤§æå–æ•°é‡
            save_results: æ˜¯å¦ä¿å­˜ç»“æœ
            
        Returns:
            èŒä½URLä¿¡æ¯åˆ—è¡¨
        """
        try:
            # æ„å»ºæœç´¢URL
            search_url = self.url_builder.build_search_url(keyword)
            self.logger.info(f"ğŸ” ä½¿ç”¨å…³é”®è¯ '{keyword}' æ„å»ºæœç´¢URL: {search_url}")
            
            # æå–URL
            return self.extract_job_urls_by_clicking(
                search_url,
                max_jobs=max_jobs,
                save_results=save_results
            )
            
        except Exception as e:
            self.logger.error(f"âŒ åŸºäºå…³é”®è¯çš„URLæå–å¤±è´¥: {e}")
            raise ContentExtractionError(f"åŸºäºå…³é”®è¯çš„URLæå–å¤±è´¥: {e}")
    
    def _merge_job_data(self,
                       basic_jobs: List[Dict[str, Any]],
                       url_jobs: List[Dict[str, Any]],
                       detail_jobs: List[Dict[str, Any]]) -> None:
        """
        åˆå¹¶èŒä½åŸºæœ¬ä¿¡æ¯ã€URLä¿¡æ¯å’Œè¯¦æƒ…ä¿¡æ¯
        
        Args:
            basic_jobs: åŸºæœ¬èŒä½ä¿¡æ¯åˆ—è¡¨
            url_jobs: URLä¿¡æ¯åˆ—è¡¨
            detail_jobs: è¯¦æƒ…ä¿¡æ¯åˆ—è¡¨
        """
        try:
            # åˆ›å»ºURLå’Œè¯¦æƒ…çš„æ˜ å°„
            url_map = {item['title']: item for item in url_jobs}
            detail_map = {item['url']: item for item in detail_jobs if item.get('url')}
            
            # åˆå¹¶æ•°æ®
            for job in basic_jobs:
                job_title = job.get('title', '')
                
                # æ·»åŠ URLä¿¡æ¯
                if job_title in url_map:
                    url_info = url_map[job_title]
                    job['detail_url'] = url_info.get('detail_url', '')
                    job['url_extracted_at'] = url_info.get('extracted_at', '')
                
                # æ·»åŠ è¯¦æƒ…ä¿¡æ¯
                detail_url = job.get('detail_url', '')
                if detail_url in detail_map:
                    detail_info = detail_map[detail_url]
                    job.update({
                        'description': detail_info.get('description', ''),
                        'requirements': detail_info.get('requirements', ''),
                        'company_info': detail_info.get('company_info', ''),
                        'benefits': detail_info.get('benefits', ''),
                        'detail_extracted_at': detail_info.get('extracted_at', '')
                    })
                
                # æ ‡è®°ä¸ºåŒ…å«è¯¦æƒ…ä¿¡æ¯
                job['has_details'] = bool(detail_url and detail_url in detail_map)
            
            self.logger.info(f"âœ… æ•°æ®åˆå¹¶å®Œæˆï¼Œ{len([j for j in basic_jobs if j.get('has_details')])} ä¸ªèŒä½åŒ…å«è¯¦æƒ…ä¿¡æ¯")
            
        except Exception as e:
            self.logger.error(f"âŒ åˆå¹¶èŒä½æ•°æ®å¤±è´¥: {e}")
    
    def _save_url_extraction_results(self, extracted_urls: List[Dict[str, Any]]) -> str:
        """
        ä¿å­˜URLæå–ç»“æœ
        
        Args:
            extracted_urls: æå–çš„URLåˆ—è¡¨
            
        Returns:
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        try:
            if not extracted_urls:
                return ""
            
            import json
            from datetime import datetime
            
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"extracted_job_urls_{timestamp}.json"
            
            result = {
                'timestamp': datetime.now().isoformat(),
                'total_urls': len(extracted_urls),
                'urls': extracted_urls
            }
            
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=2)
            
            return filename
            
        except Exception as e:
            self.logger.error(f"âŒ ä¿å­˜URLæå–ç»“æœå¤±è´¥: {e}")
            return ""
    
    def _prepare_browser(self):
        """
        å‡†å¤‡æµè§ˆå™¨
        
        Returns:
            WebDriverå®ä¾‹
        """
        try:
            # æ£€æŸ¥æ˜¯å¦éœ€è¦ä½¿ç”¨ä¿å­˜çš„ä¼šè¯
            if self.mode_config.get('use_saved_session', False):
                if self._try_load_session():
                    driver = self.browser_manager.get_driver()
                    if driver and self.browser_manager.is_driver_alive():
                        self.logger.info("âœ… ä½¿ç”¨å·²æœ‰çš„æµè§ˆå™¨ä¼šè¯")
                        # åˆå§‹åŒ–è¡Œä¸ºæ¨¡æ‹Ÿå™¨
                        if not self.behavior_simulator:
                            self.behavior_simulator = BehaviorSimulator(driver, self.config)
                        return driver
            
            # åˆ›å»ºæ–°çš„æµè§ˆå™¨å®ä¾‹
            driver = self.browser_manager.create_driver()
            
            # åˆå§‹åŒ–è¡Œä¸ºæ¨¡æ‹Ÿå™¨
            self.behavior_simulator = BehaviorSimulator(driver, self.config)
            
            # å¦‚æœé…ç½®äº†è·³è¿‡ç™»å½•ï¼Œç›´æ¥è¿”å›
            if self.mode_config.get('skip_login', False):
                self.logger.info("ğŸ”„ å¼€å‘æ¨¡å¼ï¼šè·³è¿‡ç™»å½•æ£€æŸ¥")
                return driver
            
            # å°è¯•åŠ è½½ä¼šè¯
            if self.mode_config.get('use_saved_session', True):
                self._try_load_session()
            
            return driver
            
        except Exception as e:
            self.logger.error(f"âŒ å‡†å¤‡æµè§ˆå™¨å¤±è´¥: {e}")
            raise ContentExtractionError(f"å‡†å¤‡æµè§ˆå™¨å¤±è´¥: {e}")
    
    def _try_load_session(self) -> bool:
        """
        å°è¯•åŠ è½½ä¿å­˜çš„ä¼šè¯
        
        Returns:
            æ˜¯å¦åŠ è½½æˆåŠŸ
        """
        try:
            session_file = self.mode_config.get('session_file')
            session_info = self.session_manager.get_session_info(session_file)
            
            if not session_info or session_info.get('is_expired', True):
                self.logger.info("æ²¡æœ‰æœ‰æ•ˆçš„ä¿å­˜ä¼šè¯")
                return False
            
            driver = self.browser_manager.get_driver()
            if not driver:
                driver = self.browser_manager.create_driver()
            
            if self.session_manager.load_session(driver, session_file):
                if self.session_manager.is_session_valid(driver):
                    self.logger.info("âœ… ä¼šè¯åŠ è½½æˆåŠŸ")
                    return True
                else:
                    self.logger.warning("âš ï¸ ä¼šè¯æ— æ•ˆ")
                    return False
            
            return False
            
        except Exception as e:
            self.logger.warning(f"åŠ è½½ä¼šè¯å¤±è´¥: {e}")
            return False
    
    def _navigate_to_page(self, driver, url: str) -> None:
        """
        å¯¼èˆªåˆ°æŒ‡å®šé¡µé¢ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰
        
        Args:
            driver: WebDriverå®ä¾‹
            url: ç›®æ ‡URL
        """
        try:
            self.logger.info(f"ğŸŒ å¯¼èˆªåˆ°é¡µé¢: {url}")
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºå¼€å‘/è°ƒè¯•æ¨¡å¼
            is_debug_mode = self.mode_config.get('development', False) or self.mode_config.get('debug', False)
            
            # æ ¹æ®æ¨¡å¼é€‰æ‹©å¯¼èˆªç­–ç•¥
            if is_debug_mode:
                # è°ƒè¯•æ¨¡å¼ï¼šå¿«é€Ÿå¯¼èˆªï¼Œæœ€å°ç­‰å¾…
                self.logger.info("ğŸ”§ è°ƒè¯•æ¨¡å¼ï¼šä½¿ç”¨å¿«é€Ÿå¯¼èˆª")
                driver.get(url)
                
                # æœ€å°ç­‰å¾…æ—¶é—´
                wait = self.browser_manager.create_wait(5)
                wait.until(lambda d: d.execute_script("return document.readyState") == "complete")
                
                # è°ƒè¯•æ¨¡å¼ä¸éœ€è¦é¢å¤–ç­‰å¾…
                time.sleep(0.2)
                
            elif self.behavior_simulator and random.random() < 0.2:  # åªæœ‰20%æ¦‚ç‡ä½¿ç”¨å¤æ‚å¯¼èˆª
                # ç”Ÿäº§æ¨¡å¼ï¼šå¶å°”ä½¿ç”¨è‡ªç„¶å¯¼èˆª
                success = self.behavior_simulator.natural_navigate_to_url(url)
                if not success:
                    self.logger.warning("è‡ªç„¶å¯¼èˆªå¤±è´¥ï¼Œä½¿ç”¨æ ‡å‡†å¯¼èˆª")
                    driver.get(url)
                    wait = self.browser_manager.create_wait(8)
                    wait.until(lambda d: d.execute_script("return document.readyState") == "complete")
                    time.sleep(random.uniform(0.5, 1.0))
            else:
                # æ ‡å‡†å¯¼èˆªæ–¹å¼
                driver.get(url)
                
                # å‡å°‘ç­‰å¾…æ—¶é—´
                wait = self.browser_manager.create_wait(8)  # ä»20ç§’å‡å°‘åˆ°8ç§’
                wait.until(lambda d: d.execute_script("return document.readyState") == "complete")
                
                # å¤§å¹…å‡å°‘é¢å¤–ç­‰å¾…æ—¶é—´
                time.sleep(random.uniform(0.3, 1.0))  # ä»3ç§’å‡å°‘åˆ°0.3-1ç§’
            
            self.logger.info("âœ… é¡µé¢å¯¼èˆªå®Œæˆ")
            
        except Exception as e:
            self.logger.error(f"âŒ é¡µé¢å¯¼èˆªå¤±è´¥: {e}")
            raise ContentExtractionError(f"é¡µé¢å¯¼èˆªå¤±è´¥: {e}")
    
    def _extract_keyword_from_url(self, url: str) -> str:
        """
        ä»URLä¸­æå–å…³é”®è¯
        
        Args:
            url: æœç´¢URL
            
        Returns:
            æå–çš„å…³é”®è¯
        """
        try:
            from urllib.parse import parse_qs, urlparse
            
            parsed_url = urlparse(url)
            query_params = parse_qs(parsed_url.query)
            
            # å°è¯•ä»ä¸åŒçš„å‚æ•°ä¸­æå–å…³é”®è¯
            keyword_params = ['keyword', 'kw', 'q', 'search']
            
            for param in keyword_params:
                if param in query_params and query_params[param]:
                    return query_params[param][0]
            
            # å¦‚æœæ— æ³•æå–ï¼Œè¿”å›é»˜è®¤å€¼
            return "æœªçŸ¥å…³é”®è¯"
            
        except Exception as e:
            self.logger.warning(f"ä»URLæå–å…³é”®è¯å¤±è´¥: {e}")
            return "æœªçŸ¥å…³é”®è¯"
    
    def _save_extraction_results(self, results: List[Dict[str, Any]]) -> None:
        """
        ä¿å­˜æå–ç»“æœ
        
        Args:
            results: æå–ç»“æœåˆ—è¡¨
        """
        try:
            if not results:
                return
            
            # ä¿å­˜æœç´¢ç»“æœ
            saved_file = self.data_storage.save_search_results(
                results, 
                self.current_keyword,
                format='both'  # åŒæ—¶ä¿å­˜JSONå’ŒCSV
            )
            
            # ä¿å­˜è¯¦ç»†ä¿¡æ¯åˆ°æ•°æ®åº“
            self.data_storage.save_job_details(results, self.current_keyword)
            
            if saved_file:
                self.logger.info(f"ğŸ’¾ æå–ç»“æœå·²ä¿å­˜: {saved_file}")
            
        except Exception as e:
            self.logger.error(f"âŒ ä¿å­˜æå–ç»“æœå¤±è´¥: {e}")
    
    def extract_multiple_keywords(self,
                                keywords: List[str],
                                max_results_per_keyword: Optional[int] = None,
                                delay_between_keywords: int = 5,
                                max_pages_per_keyword: Optional[int] = None) -> Dict[str, List[Dict[str, Any]]]:
        """
        æ‰¹é‡æå–å¤šä¸ªå…³é”®è¯çš„èŒä½ä¿¡æ¯ï¼ˆæ”¯æŒå¤šé¡µï¼‰
        
        Args:
            keywords: å…³é”®è¯åˆ—è¡¨
            max_results_per_keyword: æ¯ä¸ªå…³é”®è¯çš„æœ€å¤§ç»“æœæ•°
            delay_between_keywords: å…³é”®è¯ä¹‹é—´çš„å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰
            max_pages_per_keyword: æ¯ä¸ªå…³é”®è¯çš„æœ€å¤§é¡µæ•°
            
        Returns:
            æŒ‰å…³é”®è¯åˆ†ç»„çš„æå–ç»“æœ
        """
        try:
            self.logger.info(f"ğŸ” å¼€å§‹æ‰¹é‡æå– {len(keywords)} ä¸ªå…³é”®è¯çš„èŒä½ä¿¡æ¯")
            
            all_results = {}
            
            for i, keyword in enumerate(keywords, 1):
                try:
                    self.logger.info(f"ğŸ“Š å¤„ç†å…³é”®è¯ {i}/{len(keywords)}: {keyword}")
                    
                    results = self.extract_from_keyword(
                        keyword,
                        max_results=max_results_per_keyword,
                        save_results=True,
                        max_pages=max_pages_per_keyword
                    )
                    
                    all_results[keyword] = results
                    
                    # æ·»åŠ å»¶è¿Ÿï¼ˆé™¤äº†æœ€åä¸€ä¸ªå…³é”®è¯ï¼‰
                    if i < len(keywords):
                        self.logger.info(f"â³ ç­‰å¾… {delay_between_keywords} ç§’åå¤„ç†ä¸‹ä¸€ä¸ªå…³é”®è¯...")
                        time.sleep(delay_between_keywords)
                    
                except Exception as e:
                    self.logger.error(f"âŒ å¤„ç†å…³é”®è¯ '{keyword}' å¤±è´¥: {e}")
                    all_results[keyword] = []
                    continue
            
            # ç»Ÿè®¡æ€»ç»“æœ
            total_results = sum(len(results) for results in all_results.values())
            self.logger.info(f"âœ… æ‰¹é‡æå–å®Œæˆï¼Œå…±æå– {total_results} ä¸ªèŒä½")
            
            return all_results
            
        except Exception as e:
            self.logger.error(f"âŒ æ‰¹é‡æå–å¤±è´¥: {e}")
            raise ContentExtractionError(f"æ‰¹é‡æå–å¤±è´¥: {e}")
    
    def get_extraction_summary(self) -> Dict[str, Any]:
        """
        è·å–æå–æ‘˜è¦ä¿¡æ¯
        
        Returns:
            æå–æ‘˜è¦å­—å…¸
        """
        elapsed_time = None
        if self.extraction_start_time:
            elapsed_time = time.time() - self.extraction_start_time
        
        return {
            'keyword': self.current_keyword,
            'total_results': len(self.extraction_results),
            'extraction_time': time.strftime('%Y-%m-%d %H:%M:%S'),
            'elapsed_seconds': elapsed_time,
            'browser_alive': self.browser_manager.is_driver_alive(),
            'config_mode': {
                'skip_login': self.mode_config.get('skip_login', False),
                'use_saved_session': self.mode_config.get('use_saved_session', False),
                'development': self.mode_config.get('development', False)
            }
        }
    
    def get_extraction_results(self) -> List[Dict[str, Any]]:
        """
        è·å–å½“å‰æå–ç»“æœ
        
        Returns:
            æå–ç»“æœåˆ—è¡¨
        """
        return self.extraction_results.copy()
    
    def close(self) -> None:
        """å…³é—­å†…å®¹æå–å™¨ï¼Œæ¸…ç†èµ„æº"""
        try:
            self.logger.info("ğŸ§¹ å…³é—­å†…å®¹æå–å™¨")
            
            # å¦‚æœé…ç½®äº†é‡ç”¨ä¼šè¯ï¼Œä¸å…³é—­æµè§ˆå™¨
            if not self.mode_config.get('close_on_complete', True):
                self.logger.info("ğŸ’¡ é…ç½®ä¸ºä¿æŒæµè§ˆå™¨ä¼šè¯ï¼Œä¸å…³é—­æµè§ˆå™¨")
                return
            
            # å…³é—­æµè§ˆå™¨
            self.browser_manager.quit_driver()
            
            self.logger.info("âœ… å†…å®¹æå–å™¨å·²å…³é—­")
            
        except Exception as e:
            self.logger.error(f"âŒ å…³é—­å†…å®¹æå–å™¨æ—¶å‡ºé”™: {e}")
    
    def __enter__(self):
        """ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£"""
        self.close()