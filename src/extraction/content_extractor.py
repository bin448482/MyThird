"""
å†…å®¹æå–å™¨

ä½œä¸ºé«˜å±‚åè°ƒå™¨ï¼Œä¸“æ³¨äºï¼š
- æµç¨‹ç®¡ç†ï¼šåè°ƒæ•´ä¸ªæå–è¿‡ç¨‹çš„æ‰§è¡Œæµç¨‹
- æµè§ˆå™¨ä¼šè¯ç®¡ç†ï¼šå¤„ç†æµè§ˆå™¨åˆ›å»ºã€å¯¼èˆªã€ä¼šè¯ä¿æŒ
- ç»„ä»¶åè°ƒï¼šç»Ÿä¸€è°ƒåº¦PageParserã€DataStorageç­‰ç»„ä»¶
- ç»“æœå¤„ç†ï¼šæ±‡æ€»ã€ä¿å­˜å’Œç®¡ç†æå–ç»“æœ

å…·ä½“çš„é¡µé¢è§£æå·¥ä½œå§”æ‰˜ç»™PageParserå®Œæˆ
"""

import time
import logging
import random
from typing import Dict, List, Optional, Any
from urllib.parse import urlparse

from .page_parser import PageParser
from .data_storage import DataStorage
from ..auth.browser_manager import BrowserManager
from ..auth.session_manager import SessionManager
from ..search.url_builder import SearchURLBuilder
from ..utils.behavior_simulator import BehaviorSimulator
from ..utils.fingerprint import generate_job_fingerprint, extract_job_key_info
from ..database.operations import DatabaseManager
from ..core.exceptions import ContentExtractionError


class ContentExtractor:
    """
    å†…å®¹æå–å™¨ - é«˜å±‚åè°ƒå™¨
    
    èŒè´£ï¼š
    1. æµç¨‹ç®¡ç† - æ§åˆ¶æå–æµç¨‹çš„æ‰§è¡Œé¡ºåº
    2. æµè§ˆå™¨ä¼šè¯ - ç®¡ç†æµè§ˆå™¨å®ä¾‹å’Œä¼šè¯çŠ¶æ€
    3. ç»„ä»¶åè°ƒ - ç»Ÿä¸€è°ƒåº¦å„ä¸ªåŠŸèƒ½ç»„ä»¶
    4. ç»“æœå¤„ç† - æ±‡æ€»å’Œä¿å­˜æå–ç»“æœ
    
    ä¸ç›´æ¥è¿›è¡Œé¡µé¢è§£æï¼Œè€Œæ˜¯å§”æ‰˜ç»™ä¸“é—¨çš„ç»„ä»¶
    """
    
    def __init__(self, config: dict, browser_manager: Optional[BrowserManager] = None):
        """
        åˆå§‹åŒ–å†…å®¹æå–å™¨
        
        Args:
            config: é…ç½®å­—å…¸
            browser_manager: æµè§ˆå™¨ç®¡ç†å™¨å®ä¾‹ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨åˆ›å»º
        """
        self.config = config
        self.mode_config = config.get('mode', {})
        self.search_config = config.get('search', {})
        
        # ç»„ä»¶åˆå§‹åŒ–
        self.browser_manager = browser_manager or BrowserManager(config)
        self.session_manager = SessionManager(config)
        self.page_parser = PageParser(config)
        self.data_storage = DataStorage(config)
        self.url_builder = SearchURLBuilder(config)
        self.behavior_simulator = None  # å»¶è¿Ÿåˆå§‹åŒ–ï¼Œéœ€è¦driverå®ä¾‹
        
        self.logger = logging.getLogger(__name__)
        
        # çŠ¶æ€ç®¡ç†
        self.current_keyword = None
        self.extraction_results = []
        self.extraction_start_time = None
    
   
    
    def extract_from_keyword(self,
                           keyword: str,
                           max_results: Optional[int] = None,
                           save_results: bool = True,
                           extract_details: bool = False,
                           max_pages: Optional[int] = None) -> List[Dict[str, Any]]:
        """
        æ ¹æ®å…³é”®è¯æå–èŒä½ä¿¡æ¯ï¼ˆæ”¯æŒå¤šé¡µï¼‰
        
        Args:
            keyword: æœç´¢å…³é”®è¯
            max_results: æœ€å¤§ç»“æœæ•°é‡
            save_results: æ˜¯å¦ä¿å­˜ç»“æœ
            extract_details: æ˜¯å¦æå–è¯¦æƒ…é¡µå†…å®¹
            max_pages: æœ€å¤§é¡µæ•°ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é…ç½®ä¸­çš„å€¼
            
        Returns:
            æå–çš„èŒä½ä¿¡æ¯åˆ—è¡¨
        """
        try:
            # æ„å»ºæœç´¢URL
            search_url = self.url_builder.build_search_url(keyword)
            self.logger.info(f"ğŸ” ä½¿ç”¨å…³é”®è¯ '{keyword}' æ„å»ºæœç´¢URL: {search_url}")
            
            # æå–å†…å®¹
            return self.extract_from_search_url(
                search_url,
                keyword=keyword,
                max_results=max_results,
                save_results=save_results,
                extract_details=extract_details,
                max_pages=max_pages
            )
            
        except Exception as e:
            self.logger.error(f"âŒ åŸºäºå…³é”®è¯çš„å†…å®¹æå–å¤±è´¥: {e}")
            raise ContentExtractionError(f"åŸºäºå…³é”®è¯çš„å†…å®¹æå–å¤±è´¥: {e}")
    
    def extract_job_details(self, job_urls: List[str]) -> List[Dict[str, Any]]:
        """
        æå–èŒä½è¯¦æƒ…ä¿¡æ¯ï¼ˆå¸¦æŒ‡çº¹éªŒè¯å»é‡ï¼‰
        
        Args:
            job_urls: èŒä½è¯¦æƒ…é¡µURLåˆ—è¡¨
            
        Returns:
            èŒä½è¯¦æƒ…ä¿¡æ¯åˆ—è¡¨
        """
        try:
            self.logger.info(f"ğŸ“„ å¼€å§‹æå– {len(job_urls)} ä¸ªèŒä½çš„è¯¦æƒ…ä¿¡æ¯")
            
            # é¢„å…ˆè¿‡æ»¤å·²å­˜åœ¨çš„èŒä½URLï¼ˆåŸºäºæŒ‡çº¹éªŒè¯ï¼‰
            filtered_urls = self._filter_existing_job_urls(job_urls)
            skipped_count = len(job_urls) - len(filtered_urls)
            
            if skipped_count > 0:
                self.logger.info(f"ğŸ” æŒ‡çº¹éªŒè¯å®Œæˆï¼šè·³è¿‡ {skipped_count} ä¸ªå·²å­˜åœ¨çš„èŒä½ï¼Œå¾…æå– {len(filtered_urls)} ä¸ª")
            
            if not filtered_urls:
                self.logger.info("âœ… æ‰€æœ‰èŒä½éƒ½å·²å­˜åœ¨ï¼Œæ— éœ€é‡å¤æå–")
                return []
            
            driver = self._prepare_browser()
            details = []
            
            # éšæœºæ‰“ä¹±URLé¡ºåºï¼Œé¿å…æŒ‰é¡ºåºè®¿é—®çš„æ¨¡å¼
            shuffled_urls = filtered_urls.copy()
            random.shuffle(shuffled_urls)
            
            for i, job_url in enumerate(shuffled_urls, 1):
                try:
                    self.logger.info(f"ğŸ“ æå–èŒä½è¯¦æƒ… {i}/{len(shuffled_urls)}: {job_url}")
                    
                    # è¯¦æƒ…é¡µï¼šå»¶é•¿ç­‰å¾…æ—¶é—´ï¼Œé¿å…åçˆ¬æ£€æµ‹ - COMMENTED FOR SPEED
                    # if self.behavior_simulator:
                    #     self.behavior_simulator.random_delay(1.0, 5.0)  # è¯¦æƒ…é¡µå»¶é•¿åˆ°2-5ç§’
                    # else:
                    #     time.sleep(random.uniform(3.0, 8.0))  # è¯¦æƒ…é¡µå»¶é•¿åˆ°3-8ç§’
                    
                    # ä½¿ç”¨ç®€åŒ–çš„è¯¦æƒ…é¡µè§£æ
                    detail = self._extract_job_detail_simplified(driver, job_url)
                    if detail:
                        details.append(detail)
                        
                        # ä¿å­˜å•ä¸ªèŒä½è¯¦æƒ…åˆ°æ•°æ®åº“ï¼ˆæ›¿ä»£JSONæ–‡ä»¶ï¼‰
                        success = self.data_storage.save_job_detail(detail, job_url)
                        if success:
                            self.logger.debug(f"ğŸ’¾ èŒä½è¯¦æƒ…å·²ä¿å­˜åˆ°æ•°æ®åº“: {detail.get('title', '')}")
                        
                        # è¯¦æƒ…é¡µï¼šå¢åŠ é˜…è¯»æ—¶é—´æ¨¡æ‹Ÿ - COMMENTED FOR SPEED
                        # if self.behavior_simulator and random.random() < 0.6:  # å¢åŠ åˆ°60%æ¦‚ç‡æ¨¡æ‹Ÿé˜…è¯»
                        #     self.behavior_simulator.random_delay(1.0, 3.0)  # å»¶é•¿é˜…è¯»æ—¶é—´
                    
                    # è¯¦æƒ…é¡µï¼šé€‚å½“å¢åŠ ä¼‘æ¯æ—¶é—´ - COMMENTED FOR SPEED
                    # if i % random.randint(5, 8) == 0:  # è¯¦æƒ…é¡µä¼‘æ¯é¢‘ç‡é€‚ä¸­
                    #     rest_time = random.uniform(4.0, 8.0)  # è¯¦æƒ…é¡µä¼‘æ¯æ—¶é—´å»¶é•¿
                    #     self.logger.info(f"â³ æ¨¡æ‹Ÿç”¨æˆ·ä¼‘æ¯ {rest_time:.1f} ç§’...")
                    #     time.sleep(rest_time)
                    
                except Exception as e:
                    self.logger.warning(f"âš ï¸ æå–èŒä½è¯¦æƒ…å¤±è´¥ {job_url}: {e}")
                    # è¯¦æƒ…é¡µå¤±è´¥åå»¶é•¿ç­‰å¾…æ—¶é—´ - COMMENTED FOR SPEED
                    # time.sleep(random.uniform(10.0, 25.0))  # ä»5-15ç§’å»¶é•¿åˆ°10-25ç§’
                    continue
            
            self.logger.info(f"âœ… èŒä½è¯¦æƒ…æå–å®Œæˆï¼Œå…±æå– {len(details)} ä¸ªè¯¦æƒ…ï¼Œè·³è¿‡é‡å¤ {skipped_count} ä¸ª")
            return details
            
        except Exception as e:
            self.logger.error(f"âŒ èŒä½è¯¦æƒ…æå–å¤±è´¥: {e}")
            raise ContentExtractionError(f"èŒä½è¯¦æƒ…æå–å¤±è´¥: {e}")
    
    def _extract_job_detail_simplified(self, driver, job_url: str) -> Optional[Dict[str, Any]]:
        """
        ç®€åŒ–çš„èŒä½è¯¦æƒ…æå–æ–¹æ³•ï¼ˆå¸¦æŒ‡çº¹éªŒè¯ï¼‰
        
        Args:
            driver: WebDriverå®ä¾‹
            job_url: èŒä½è¯¦æƒ…URL
            
        Returns:
            èŒä½è¯¦æƒ…æ•°æ®
        """
        try:
            # ç›´æ¥å¯¼èˆªï¼Œå‡å°‘å¤æ‚çš„æ¨¡æ‹Ÿ
            driver.get(job_url)
            
            # è¯¦æƒ…é¡µï¼šå»¶é•¿ç­‰å¾…æ—¶é—´ - COMMENTED FOR SPEED
            # time.sleep(random.uniform(2.0, 4.0))  # ä»1-2.5ç§’å»¶é•¿åˆ°2-4ç§’
            
            # æ£€æŸ¥æ˜¯å¦è¢«é‡å®šå‘æˆ–é˜»æ­¢
            current_url = driver.current_url
            if 'error' in current_url.lower() or 'block' in current_url.lower() or 'captcha' in current_url.lower():
                self.logger.warning(f"æ£€æµ‹åˆ°å¯èƒ½çš„åçˆ¬é¡µé¢: {current_url}")
                return None
            
            # ç®€åŒ–çš„é¡µé¢äº¤äº’ï¼šåªåšåŸºæœ¬æ»šåŠ¨
            if self.behavior_simulator and random.random() < 0.5:  # 50%æ¦‚ç‡è¿›è¡Œç®€å•æ»šåŠ¨
                self.behavior_simulator.simulate_scroll('down', random.randint(300, 500))
                # time.sleep(random.uniform(0.5, 1.0))  # COMMENTED FOR SPEED
                driver.execute_script("window.scrollTo(0, 0);")
            
            # è§£æé¡µé¢å†…å®¹
            detail_data = self.page_parser.parse_job_detail(driver, job_url)
            
            if detail_data:
                # ä¸ºè¯¦æƒ…æ•°æ®ç”ŸæˆæŒ‡çº¹
                job_fingerprint = generate_job_fingerprint(
                    detail_data.get('title', ''),
                    detail_data.get('company', ''),
                    detail_data.get('salary', ''),
                    detail_data.get('location', '')
                )
                detail_data['job_fingerprint'] = job_fingerprint
                
                # å†æ¬¡æ£€æŸ¥æŒ‡çº¹æ˜¯å¦å·²å­˜åœ¨ï¼ˆåŒé‡ä¿é™©ï¼‰
                if self.data_storage.check_job_fingerprint_exists(
                    detail_data.get('title', ''),
                    detail_data.get('company', ''),
                    detail_data.get('salary', ''),
                    detail_data.get('location', '')
                ):
                    self.logger.debug(f"èŒä½è¯¦æƒ…å·²å­˜åœ¨ï¼ˆæŒ‡çº¹éªŒè¯ï¼‰ï¼Œè·³è¿‡: {detail_data.get('title', '')}")
                    return None
            
            return detail_data
            
        except Exception as e:
            self.logger.error(f"ç®€åŒ–æå–èŒä½è¯¦æƒ…å¤±è´¥: {e}")
            return None
    
        
    
    def _merge_job_data(self,
                       basic_jobs: List[Dict[str, Any]],
                       url_jobs: List[Dict[str, Any]],
                       detail_jobs: List[Dict[str, Any]]) -> None:
        """
        åˆå¹¶èŒä½åŸºæœ¬ä¿¡æ¯ã€URLä¿¡æ¯å’Œè¯¦æƒ…ä¿¡æ¯ï¼Œå¹¶æ›´æ–°æ•°æ®åº“
        
        Args:
            basic_jobs: åŸºæœ¬èŒä½ä¿¡æ¯åˆ—è¡¨
            url_jobs: URLä¿¡æ¯åˆ—è¡¨
            detail_jobs: è¯¦æƒ…ä¿¡æ¯åˆ—è¡¨
        """
        try:
            self.logger.info(f"ğŸ”„ å¼€å§‹åˆå¹¶æ•°æ®: åŸºæœ¬ä¿¡æ¯{len(basic_jobs)}ä¸ª, URLä¿¡æ¯{len(url_jobs)}ä¸ª, è¯¦æƒ…ä¿¡æ¯{len(detail_jobs)}ä¸ª")
            
            # åˆ›å»ºæ›´ç²¾ç¡®çš„æ˜ å°„å…³ç³»
            # ä½¿ç”¨å¤šç§åŒ¹é…ç­–ç•¥ï¼šæ ‡é¢˜åŒ¹é…ã€æ ‡é¢˜+å…¬å¸åŒ¹é…ã€ç´¢å¼•åŒ¹é…
            url_map_by_title = {item['title']: item for item in url_jobs}
            url_map_by_index = {item.get('index', i+1): item for i, item in enumerate(url_jobs)}
            detail_map = {item['url']: item for item in detail_jobs if item.get('url')}
            
            # åˆå¹¶æ•°æ®å¹¶æ›´æ–°æ•°æ®åº“
            updated_count = 0
            matched_count = 0
            
            for i, job in enumerate(basic_jobs):
                job_title = job.get('title', '').strip()
                job_company = job.get('company', '').strip()
                
                self.logger.debug(f"ğŸ” å¤„ç†èŒä½ {i+1}: {job_title} @ {job_company}")
                
                # å°è¯•å¤šç§åŒ¹é…ç­–ç•¥æ‰¾åˆ°å¯¹åº”çš„URLä¿¡æ¯
                url_info = None
                match_method = ""
                
                # ç­–ç•¥1: ç²¾ç¡®æ ‡é¢˜åŒ¹é…
                if job_title in url_map_by_title:
                    url_info = url_map_by_title[job_title]
                    match_method = "æ ‡é¢˜åŒ¹é…"
                
                # ç­–ç•¥2: ç´¢å¼•åŒ¹é…ï¼ˆåŸºäºæå–é¡ºåºï¼‰
                elif (i + 1) in url_map_by_index:
                    url_info = url_map_by_index[i + 1]
                    match_method = "ç´¢å¼•åŒ¹é…"
                
                # ç­–ç•¥3: æ¨¡ç³Šæ ‡é¢˜åŒ¹é…ï¼ˆå¤„ç†æ ‡é¢˜ç•¥æœ‰å·®å¼‚çš„æƒ…å†µï¼‰
                else:
                    for url_title, url_data in url_map_by_title.items():
                        if self._titles_similar(job_title, url_title):
                            url_info = url_data
                            match_method = "æ¨¡ç³ŠåŒ¹é…"
                            break
                
                if url_info:
                    matched_count += 1
                    detail_url = url_info.get('detail_url', '')
                    
                    self.logger.info(f"âœ… åŒ¹é…æˆåŠŸ ({match_method}): {job_title} -> {detail_url}")
                    
                    # æ›´æ–°jobå¯¹è±¡ä¸­çš„URLä¿¡æ¯
                    job['detail_url'] = detail_url
                    job['url'] = detail_url  # é‡è¦ï¼šåŒæ—¶æ›´æ–°urlå­—æ®µ
                    job['url_extracted_at'] = url_info.get('extracted_at', '')
                    
                    # æ›´æ–°æ•°æ®åº“ä¸­çš„URL
                    if detail_url:
                        success = self.data_storage.update_job_with_detail_url(
                            job_title, job_company, detail_url
                        )
                        if success:
                            updated_count += 1
                else:
                    self.logger.warning(f"âš ï¸ æœªæ‰¾åˆ°åŒ¹é…çš„URLä¿¡æ¯: {job_title} @ {job_company}")
                
                # æ·»åŠ è¯¦æƒ…ä¿¡æ¯
                detail_url = job.get('detail_url', '') or job.get('url', '')
                if detail_url and detail_url in detail_map:
                    detail_info = detail_map[detail_url]
                    job.update({
                        'description': detail_info.get('description', ''),
                        'requirements': detail_info.get('requirements', ''),
                        'company_info': detail_info.get('company_info', ''),
                        'benefits': detail_info.get('benefits', ''),
                        'detail_extracted_at': detail_info.get('extracted_at', '')
                    })
                    self.logger.debug(f"ğŸ“„ æ·»åŠ è¯¦æƒ…ä¿¡æ¯: {job_title}")
                
                # æ ‡è®°ä¸ºåŒ…å«è¯¦æƒ…ä¿¡æ¯
                job['has_details'] = bool(detail_url and detail_url in detail_map)
            
            self.logger.info(f"âœ… æ•°æ®åˆå¹¶å®Œæˆ:")
            self.logger.info(f"   - æˆåŠŸåŒ¹é…URL: {matched_count}/{len(basic_jobs)} ä¸ªèŒä½")
            self.logger.info(f"   - åŒ…å«è¯¦æƒ…ä¿¡æ¯: {len([j for j in basic_jobs if j.get('has_details')])} ä¸ªèŒä½")
            self.logger.info(f"   - æ•°æ®åº“URLæ›´æ–°: {updated_count} ä¸ªèŒä½")
            
        except Exception as e:
            self.logger.error(f"âŒ åˆå¹¶èŒä½æ•°æ®å¤±è´¥: {e}")
            import traceback
            self.logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
    
    def _titles_similar(self, title1: str, title2: str, threshold: float = 0.8) -> bool:
        """
        æ£€æŸ¥ä¸¤ä¸ªèŒä½æ ‡é¢˜æ˜¯å¦ç›¸ä¼¼
        
        Args:
            title1: ç¬¬ä¸€ä¸ªæ ‡é¢˜
            title2: ç¬¬äºŒä¸ªæ ‡é¢˜
            threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
            
        Returns:
            æ˜¯å¦ç›¸ä¼¼
        """
        try:
            if not title1 or not title2:
                return False
            
            # ç®€å•çš„ç›¸ä¼¼åº¦æ£€æŸ¥ï¼šå»é™¤ç©ºæ ¼åæ¯”è¾ƒ
            clean_title1 = title1.strip().lower().replace(' ', '')
            clean_title2 = title2.strip().lower().replace(' ', '')
            
            # å®Œå…¨åŒ¹é…
            if clean_title1 == clean_title2:
                return True
            
            # åŒ…å«å…³ç³»æ£€æŸ¥
            if clean_title1 in clean_title2 or clean_title2 in clean_title1:
                return True
            
            # ç®€å•çš„ç¼–è¾‘è·ç¦»æ£€æŸ¥
            def levenshtein_ratio(s1, s2):
                if len(s1) == 0:
                    return len(s2)
                if len(s2) == 0:
                    return len(s1)
                
                # åˆ›å»ºçŸ©é˜µ
                matrix = [[0] * (len(s2) + 1) for _ in range(len(s1) + 1)]
                
                # åˆå§‹åŒ–ç¬¬ä¸€è¡Œå’Œç¬¬ä¸€åˆ—
                for i in range(len(s1) + 1):
                    matrix[i][0] = i
                for j in range(len(s2) + 1):
                    matrix[0][j] = j
                
                # å¡«å……çŸ©é˜µ
                for i in range(1, len(s1) + 1):
                    for j in range(1, len(s2) + 1):
                        if s1[i-1] == s2[j-1]:
                            cost = 0
                        else:
                            cost = 1
                        matrix[i][j] = min(
                            matrix[i-1][j] + 1,      # åˆ é™¤
                            matrix[i][j-1] + 1,      # æ’å…¥
                            matrix[i-1][j-1] + cost  # æ›¿æ¢
                        )
                
                # è®¡ç®—ç›¸ä¼¼åº¦
                max_len = max(len(s1), len(s2))
                if max_len == 0:
                    return 1.0
                return 1.0 - (matrix[len(s1)][len(s2)] / max_len)
            
            similarity = levenshtein_ratio(clean_title1, clean_title2)
            return similarity >= threshold
            
        except Exception as e:
            self.logger.debug(f"æ ‡é¢˜ç›¸ä¼¼åº¦æ£€æŸ¥å¤±è´¥: {e}")
            return False
    
    
    def _prepare_browser(self):
        """
        å‡†å¤‡æµè§ˆå™¨
        
        Returns:
            WebDriverå®ä¾‹
        """
        try:
            # æ£€æŸ¥æ˜¯å¦éœ€è¦ä½¿ç”¨ä¿å­˜çš„ä¼šè¯
            if self.mode_config.get('use_saved_session', False):
                if self._try_load_session():
                    driver = self.browser_manager.get_driver()
                    if driver and self.browser_manager.is_driver_alive():
                        self.logger.info("âœ… ä½¿ç”¨å·²æœ‰çš„æµè§ˆå™¨ä¼šè¯")
                        # åˆå§‹åŒ–è¡Œä¸ºæ¨¡æ‹Ÿå™¨
                        if not self.behavior_simulator:
                            self.behavior_simulator = BehaviorSimulator(driver, self.config)
                        return driver
            
            # åˆ›å»ºæ–°çš„æµè§ˆå™¨å®ä¾‹
            driver = self.browser_manager.create_driver()
            
            # åˆå§‹åŒ–è¡Œä¸ºæ¨¡æ‹Ÿå™¨
            self.behavior_simulator = BehaviorSimulator(driver, self.config)
            
            # å¦‚æœé…ç½®äº†è·³è¿‡ç™»å½•ï¼Œç›´æ¥è¿”å›
            if self.mode_config.get('skip_login', False):
                self.logger.info("ğŸ”„ å¼€å‘æ¨¡å¼ï¼šè·³è¿‡ç™»å½•æ£€æŸ¥")
                return driver
            
            # å°è¯•åŠ è½½ä¼šè¯
            if self.mode_config.get('use_saved_session', True):
                self._try_load_session()
            
            return driver
            
        except Exception as e:
            self.logger.error(f"âŒ å‡†å¤‡æµè§ˆå™¨å¤±è´¥: {e}")
            raise ContentExtractionError(f"å‡†å¤‡æµè§ˆå™¨å¤±è´¥: {e}")
    
    def _try_load_session(self) -> bool:
        """
        å°è¯•åŠ è½½ä¿å­˜çš„ä¼šè¯
        
        Returns:
            æ˜¯å¦åŠ è½½æˆåŠŸ
        """
        try:
            session_file = self.mode_config.get('session_file')
            session_info = self.session_manager.get_session_info(session_file)
            
            if not session_info or session_info.get('is_expired', True):
                self.logger.info("æ²¡æœ‰æœ‰æ•ˆçš„ä¿å­˜ä¼šè¯")
                return False
            
            driver = self.browser_manager.get_driver()
            if not driver:
                driver = self.browser_manager.create_driver()
            
            if self.session_manager.load_session(driver, session_file):
                if self.session_manager.is_session_valid(driver):
                    self.logger.info("âœ… ä¼šè¯åŠ è½½æˆåŠŸ")
                    return True
                else:
                    self.logger.warning("âš ï¸ ä¼šè¯æ— æ•ˆ")
                    return False
            
            return False
            
        except Exception as e:
            self.logger.warning(f"åŠ è½½ä¼šè¯å¤±è´¥: {e}")
            return False
    
    def _navigate_to_page(self, driver, url: str) -> None:
        """
        å¯¼èˆªåˆ°æŒ‡å®šé¡µé¢ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰
        
        Args:
            driver: WebDriverå®ä¾‹
            url: ç›®æ ‡URL
        """
        try:
            self.logger.info(f"ğŸŒ å¯¼èˆªåˆ°é¡µé¢: {url}")
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºå¼€å‘/è°ƒè¯•æ¨¡å¼
            is_debug_mode = self.mode_config.get('development', False) or self.mode_config.get('debug', False)
            
            # æ ¹æ®æ¨¡å¼é€‰æ‹©å¯¼èˆªç­–ç•¥
            if is_debug_mode:
                # è°ƒè¯•æ¨¡å¼ï¼šå¿«é€Ÿå¯¼èˆªï¼Œæœ€å°ç­‰å¾…
                self.logger.info("ğŸ”§ è°ƒè¯•æ¨¡å¼ï¼šä½¿ç”¨å¿«é€Ÿå¯¼èˆª")
                driver.get(url)
                
                # æœ€å°ç­‰å¾…æ—¶é—´
                wait = self.browser_manager.create_wait(5)
                wait.until(lambda d: d.execute_script("return document.readyState") == "complete")
                
                # è°ƒè¯•æ¨¡å¼ä¸éœ€è¦é¢å¤–ç­‰å¾… - COMMENTED FOR SPEED
                # time.sleep(0.2)
                
            elif self.behavior_simulator and random.random() < 0.2:  # åªæœ‰20%æ¦‚ç‡ä½¿ç”¨å¤æ‚å¯¼èˆª
                # ç”Ÿäº§æ¨¡å¼ï¼šå¶å°”ä½¿ç”¨è‡ªç„¶å¯¼èˆª
                success = self.behavior_simulator.natural_navigate_to_url(url)
                if not success:
                    self.logger.warning("è‡ªç„¶å¯¼èˆªå¤±è´¥ï¼Œä½¿ç”¨æ ‡å‡†å¯¼èˆª")
                    driver.get(url)
                    wait = self.browser_manager.create_wait(8)
                    wait.until(lambda d: d.execute_script("return document.readyState") == "complete")
                    # time.sleep(random.uniform(0.5, 1.0))  # COMMENTED FOR SPEED
            else:
                # æ ‡å‡†å¯¼èˆªæ–¹å¼
                driver.get(url)
                
                # å‡å°‘ç­‰å¾…æ—¶é—´
                wait = self.browser_manager.create_wait(8)  # ä»20ç§’å‡å°‘åˆ°8ç§’
                wait.until(lambda d: d.execute_script("return document.readyState") == "complete")
                
                # å¤§å¹…å‡å°‘é¢å¤–ç­‰å¾…æ—¶é—´ - COMMENTED FOR SPEED
                # time.sleep(random.uniform(0.3, 1.0))  # ä»3ç§’å‡å°‘åˆ°0.3-1ç§’
            
            self.logger.info("âœ… é¡µé¢å¯¼èˆªå®Œæˆ")
            
        except Exception as e:
            self.logger.error(f"âŒ é¡µé¢å¯¼èˆªå¤±è´¥: {e}")
            raise ContentExtractionError(f"é¡µé¢å¯¼èˆªå¤±è´¥: {e}")
    
    def _extract_keyword_from_url(self, url: str) -> str:
        """
        ä»URLä¸­æå–å…³é”®è¯
        
        Args:
            url: æœç´¢URL
            
        Returns:
            æå–çš„å…³é”®è¯
        """
        try:
            from urllib.parse import parse_qs, urlparse
            
            parsed_url = urlparse(url)
            query_params = parse_qs(parsed_url.query)
            
            # å°è¯•ä»ä¸åŒçš„å‚æ•°ä¸­æå–å…³é”®è¯
            keyword_params = ['keyword', 'kw', 'q', 'search']
            
            for param in keyword_params:
                if param in query_params and query_params[param]:
                    return query_params[param][0]
            
            # å¦‚æœæ— æ³•æå–ï¼Œè¿”å›é»˜è®¤å€¼
            return "æœªçŸ¥å…³é”®è¯"
            
        except Exception as e:
            self.logger.warning(f"ä»URLæå–å…³é”®è¯å¤±è´¥: {e}")
            return "æœªçŸ¥å…³é”®è¯"
    
    def _save_extraction_results(self, results: List[Dict[str, Any]]) -> None:
        """
        ä¿å­˜æå–ç»“æœ
        
        Args:
            results: æå–ç»“æœåˆ—è¡¨
        """
        try:
            if not results:
                return
            
            # ä¿å­˜æœç´¢ç»“æœ
            saved_file = self.data_storage.save_search_results(
                results, 
                self.current_keyword,
                format='both'  # åŒæ—¶ä¿å­˜JSONå’ŒCSV
            )
            
            # ä¿å­˜è¯¦ç»†ä¿¡æ¯åˆ°æ•°æ®åº“
            self.data_storage.save_job_details(results, self.current_keyword)
            
            if saved_file:
                self.logger.info(f"ğŸ’¾ æå–ç»“æœå·²ä¿å­˜: {saved_file}")
            
        except Exception as e:
            self.logger.error(f"âŒ ä¿å­˜æå–ç»“æœå¤±è´¥: {e}")
    
    def extract_multiple_keywords(self,
                                keywords: List[str],
                                max_results_per_keyword: Optional[int] = None,
                                delay_between_keywords: int = 5,
                                max_pages_per_keyword: Optional[int] = None) -> Dict[str, List[Dict[str, Any]]]:
        """
        æ‰¹é‡æå–å¤šä¸ªå…³é”®è¯çš„èŒä½ä¿¡æ¯ï¼ˆæ”¯æŒå¤šé¡µï¼‰
        
        Args:
            keywords: å…³é”®è¯åˆ—è¡¨
            max_results_per_keyword: æ¯ä¸ªå…³é”®è¯çš„æœ€å¤§ç»“æœæ•°
            delay_between_keywords: å…³é”®è¯ä¹‹é—´çš„å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰
            max_pages_per_keyword: æ¯ä¸ªå…³é”®è¯çš„æœ€å¤§é¡µæ•°
            
        Returns:
            æŒ‰å…³é”®è¯åˆ†ç»„çš„æå–ç»“æœ
        """
        try:
            self.logger.info(f"ğŸ” å¼€å§‹æ‰¹é‡æå– {len(keywords)} ä¸ªå…³é”®è¯çš„èŒä½ä¿¡æ¯")
            
            all_results = {}
            
            for i, keyword in enumerate(keywords, 1):
                try:
                    self.logger.info(f"ğŸ“Š å¤„ç†å…³é”®è¯ {i}/{len(keywords)}: {keyword}")
                    
                    results = self.extract_from_keyword(
                        keyword,
                        max_results=max_results_per_keyword,
                        save_results=True,
                        max_pages=max_pages_per_keyword
                    )
                    
                    all_results[keyword] = results
                    
                    # æ·»åŠ å»¶è¿Ÿï¼ˆé™¤äº†æœ€åä¸€ä¸ªå…³é”®è¯ï¼‰
                    if i < len(keywords):
                        self.logger.info(f"â³ ç­‰å¾… {delay_between_keywords} ç§’åå¤„ç†ä¸‹ä¸€ä¸ªå…³é”®è¯... - COMMENTED FOR SPEED")
                        # time.sleep(delay_between_keywords)
                    
                except Exception as e:
                    self.logger.error(f"âŒ å¤„ç†å…³é”®è¯ '{keyword}' å¤±è´¥: {e}")
                    all_results[keyword] = []
                    continue
            
            # ç»Ÿè®¡æ€»ç»“æœ
            total_results = sum(len(results) for results in all_results.values())
            self.logger.info(f"âœ… æ‰¹é‡æå–å®Œæˆï¼Œå…±æå– {total_results} ä¸ªèŒä½")
            
            return all_results
            
        except Exception as e:
            self.logger.error(f"âŒ æ‰¹é‡æå–å¤±è´¥: {e}")
            raise ContentExtractionError(f"æ‰¹é‡æå–å¤±è´¥: {e}")
    
    def get_extraction_summary(self) -> Dict[str, Any]:
        """
        è·å–æå–æ‘˜è¦ä¿¡æ¯
        
        Returns:
            æå–æ‘˜è¦å­—å…¸
        """
        elapsed_time = None
        if self.extraction_start_time:
            elapsed_time = time.time() - self.extraction_start_time
        
        return {
            'keyword': self.current_keyword,
            'total_results': len(self.extraction_results),
            'extraction_time': time.strftime('%Y-%m-%d %H:%M:%S'),
            'elapsed_seconds': elapsed_time,
            'browser_alive': self.browser_manager.is_driver_alive(),
            'config_mode': {
                'skip_login': self.mode_config.get('skip_login', False),
                'use_saved_session': self.mode_config.get('use_saved_session', False),
                'development': self.mode_config.get('development', False)
            }
        }
    
    def get_extraction_results(self) -> List[Dict[str, Any]]:
        """
        è·å–å½“å‰æå–ç»“æœ
        
        Returns:
            æå–ç»“æœåˆ—è¡¨
        """
        return self.extraction_results.copy()
    
    def close(self) -> None:
        """å…³é—­å†…å®¹æå–å™¨ï¼Œæ¸…ç†èµ„æº"""
        try:
            self.logger.info("ğŸ§¹ å…³é—­å†…å®¹æå–å™¨")
            
            # å¦‚æœé…ç½®äº†é‡ç”¨ä¼šè¯ï¼Œä¸å…³é—­æµè§ˆå™¨
            if not self.mode_config.get('close_on_complete', True):
                self.logger.info("ğŸ’¡ é…ç½®ä¸ºä¿æŒæµè§ˆå™¨ä¼šè¯ï¼Œä¸å…³é—­æµè§ˆå™¨")
                return
            
            # å…³é—­æµè§ˆå™¨
            self.browser_manager.quit_driver()
            
            self.logger.info("âœ… å†…å®¹æå–å™¨å·²å…³é—­")
            
        except Exception as e:
            self.logger.error(f"âŒ å…³é—­å†…å®¹æå–å™¨æ—¶å‡ºé”™: {e}")
    
    def __enter__(self):
        """ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£"""
        self.close()
    
    
    def _filter_duplicate_jobs(self, job_results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        è¿‡æ»¤é‡å¤èŒä½ï¼ˆåŸºäºæŒ‡çº¹ï¼‰
        
        Args:
            job_results: èŒä½ç»“æœåˆ—è¡¨
            
        Returns:
            å»é‡åçš„èŒä½åˆ—è¡¨
        """
        try:
            if not job_results:
                return []
            
            # æå–æ‰€æœ‰æŒ‡çº¹
            fingerprints = [job.get('job_fingerprint') for job in job_results if job.get('job_fingerprint')]
            
            if not fingerprints:
                self.logger.warning("èŒä½åˆ—è¡¨ä¸­æ²¡æœ‰æŒ‡çº¹ä¿¡æ¯ï¼Œè·³è¿‡å»é‡æ£€æŸ¥")
                return job_results
            
            # æ‰¹é‡æ£€æŸ¥æŒ‡çº¹æ˜¯å¦å­˜åœ¨
            db_manager = DatabaseManager(self.data_storage.db_path)
            existing_fingerprints = db_manager.batch_check_fingerprints(fingerprints)
            
            # è¿‡æ»¤é‡å¤èŒä½
            filtered_results = []
            for job in job_results:
                fingerprint = job.get('job_fingerprint')
                if fingerprint and existing_fingerprints.get(fingerprint, False):
                    self.logger.debug(f"è·³è¿‡é‡å¤èŒä½: {job.get('title', '')} - {job.get('company', '')}")
                    continue
                filtered_results.append(job)
            
            return filtered_results
            
        except Exception as e:
            self.logger.error(f"è¿‡æ»¤é‡å¤èŒä½å¤±è´¥: {e}")
            return job_results
    
    def _filter_existing_job_urls(self, job_urls: List[str]) -> List[str]:
        """
        è¿‡æ»¤å·²å­˜åœ¨çš„èŒä½URLï¼ˆåŸºäºæŒ‡çº¹éªŒè¯ï¼‰
        
        Args:
            job_urls: èŒä½URLåˆ—è¡¨
            
        Returns:
            è¿‡æ»¤åçš„URLåˆ—è¡¨
        """
        try:
            if not job_urls:
                return []
            
            filtered_urls = []
            
            for job_url in job_urls:
                try:
                    # ä»URLä¸­å°è¯•æå–åŸºæœ¬ä¿¡æ¯è¿›è¡ŒæŒ‡çº¹éªŒè¯
                    # è¿™é‡Œæˆ‘ä»¬éœ€è¦å…ˆè®¿é—®é¡µé¢è·å–åŸºæœ¬ä¿¡æ¯ï¼Œæˆ–è€…ä½¿ç”¨å…¶ä»–æ–¹å¼
                    # ä¸ºäº†ç®€åŒ–ï¼Œæˆ‘ä»¬å¯ä»¥æ£€æŸ¥URLæ˜¯å¦å·²ç»åœ¨æ•°æ®åº“ä¸­
                    
                    # ç”ŸæˆåŸºäºURLçš„ä¸´æ—¶job_idæ¥æ£€æŸ¥
                    temp_job_id = self.data_storage._generate_job_id(job_url, "", "")
                    
                    # ä½¿ç”¨DatabaseManageræ£€æŸ¥æ˜¯å¦å­˜åœ¨
                    db_manager = DatabaseManager(self.data_storage.db_path)
                    if not db_manager.job_exists(temp_job_id):
                        filtered_urls.append(job_url)
                    else:
                        self.logger.debug(f"è·³è¿‡å·²å­˜åœ¨çš„èŒä½URL: {job_url}")
                        
                except Exception as e:
                    self.logger.warning(f"æ£€æŸ¥èŒä½URLæ—¶å‡ºé”™ï¼Œå°†å…¶åŒ…å«åœ¨æå–åˆ—è¡¨ä¸­: {job_url} - {e}")
                    filtered_urls.append(job_url)  # å‡ºé”™æ—¶ä¿å®ˆå¤„ç†ï¼ŒåŒ…å«è¯¥URL
            
            return filtered_urls
            
        except Exception as e:
            self.logger.error(f"è¿‡æ»¤èŒä½URLå¤±è´¥: {e}")
            return job_urls  # å‡ºé”™æ—¶è¿”å›åŸå§‹åˆ—è¡¨
    
    def _save_extraction_results(self, results: List[Dict[str, Any]]) -> None:
        """
        ä¿å­˜æå–ç»“æœï¼ˆä¿®å¤ç‰ˆæœ¬ï¼Œç¡®ä¿URLå’ŒåŸºæœ¬ä¿¡æ¯ä¸€èµ·ä¿å­˜ï¼‰
        
        Args:
            results: æå–ç»“æœåˆ—è¡¨
        """
        try:
            if not results:
                return
            
            self.logger.info(f"ğŸ’¾ å¼€å§‹ä¿å­˜æå–ç»“æœ: {len(results)} æ¡è®°å½•")
            
            # éªŒè¯æ•°æ®å®Œæ•´æ€§
            complete_jobs = []
            incomplete_jobs = []
            
            for job in results:
                title = job.get('title', '').strip()
                company = job.get('company', '').strip()
                url = job.get('url', '').strip()
                
                if title and company:
                    complete_jobs.append(job)
                    if url:
                        self.logger.debug(f"âœ… å®Œæ•´èŒä½: {title} @ {company} - {url[:50]}...")
                    else:
                        self.logger.debug(f"âš ï¸ ç¼ºå°‘URL: {title} @ {company}")
                else:
                    incomplete_jobs.append(job)
                    self.logger.warning(f"âŒ ä¸å®Œæ•´èŒä½: title='{title}', company='{company}'")
            
            self.logger.info(f"ğŸ“Š æ•°æ®éªŒè¯: å®Œæ•´{len(complete_jobs)}ä¸ª, ä¸å®Œæ•´{len(incomplete_jobs)}ä¸ª")
            
            # ä¸»è¦ä¿å­˜åˆ°æ•°æ®åº“
            database_config = self.config.get('database', {})
            if database_config.get('enabled', True):
                # åªä¿å­˜å®Œæ•´çš„èŒä½æ•°æ®
                if complete_jobs:
                    self.data_storage._save_to_database(complete_jobs, self.current_keyword)
                    self.logger.info(f"ğŸ’¾ å®Œæ•´èŒä½å·²ä¿å­˜åˆ°æ•°æ®åº“: {len(complete_jobs)} æ¡è®°å½•")
                
                # å¯¹äºä¸å®Œæ•´çš„æ•°æ®ï¼Œè®°å½•ä½†ä¸ä¿å­˜
                if incomplete_jobs:
                    self.logger.warning(f"âš ï¸ è·³è¿‡ä¸å®Œæ•´çš„èŒä½æ•°æ®: {len(incomplete_jobs)} æ¡")
            
            # å¯é€‰ï¼šä»ç„¶ä¿å­˜æ–‡ä»¶ä½œä¸ºå¤‡ä»½ï¼ˆå¦‚æœé…ç½®å¯ç”¨ï¼‰
            backup_files = self.config.get('backup', {}).get('save_files', False)
            if backup_files and complete_jobs:
                saved_file = self.data_storage.save_search_results(
                    complete_jobs,
                    self.current_keyword,
                    format='json'  # åªä¿å­˜JSONä½œä¸ºå¤‡ä»½
                )
                if saved_file:
                    self.logger.info(f"ğŸ“ å¤‡ä»½æ–‡ä»¶å·²ä¿å­˜: {saved_file}")
            
        except Exception as e:
            self.logger.error(f"âŒ ä¿å­˜æå–ç»“æœå¤±è´¥: {e}")
            import traceback
            self.logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
    
    def get_deduplication_summary(self) -> Dict[str, Any]:
        """
        è·å–å»é‡æ‘˜è¦ä¿¡æ¯
        
        Returns:
            å»é‡æ‘˜è¦å­—å…¸
        """
        try:
            # è·å–å»é‡ç»Ÿè®¡
            dedup_stats = self.data_storage.get_deduplication_stats()
            
            # è·å–åŸºæœ¬æå–æ‘˜è¦
            basic_summary = self.get_extraction_summary()
            
            # åˆå¹¶ä¿¡æ¯
            return {
                **basic_summary,
                'deduplication': dedup_stats,
                'efficiency_improvement': {
                    'duplicate_rate': dedup_stats.get('deduplication_rate', 0),
                    'unique_jobs': dedup_stats.get('unique_fingerprints', 0),
                    'total_processed': dedup_stats.get('total_jobs', 0)
                }
            }
            
        except Exception as e:
            self.logger.error(f"è·å–å»é‡æ‘˜è¦å¤±è´¥: {e}")
            return self.get_extraction_summary()
    
    def _filter_new_jobs_by_fingerprint_with_elements(self, page_jobs: List[Dict[str, Any]], job_elements: List) -> List[Dict[str, Any]]:
        """
        é€šè¿‡æŒ‡çº¹è¿‡æ»¤å‡ºæ–°èŒä½ï¼ŒåŒæ—¶ä¿ç•™å¯¹åº”çš„é¡µé¢å…ƒç´ 
        
        Args:
            page_jobs: é¡µé¢èŒä½åˆ—è¡¨
            job_elements: å¯¹åº”çš„é¡µé¢DOMå…ƒç´ åˆ—è¡¨
            
        Returns:
            åŒ…å«èŒä½æ•°æ®å’Œå¯¹åº”å…ƒç´ çš„åˆ—è¡¨ï¼Œæ ¼å¼: [{'job_data': job, 'element': element}, ...]
        """
        try:
            if not page_jobs:
                return []
            
            # ç¡®ä¿èŒä½æ•°æ®å’Œå…ƒç´ æ•°é‡ä¸€è‡´
            if len(page_jobs) != len(job_elements):
                self.logger.warning(f"âš ï¸ èŒä½æ•°æ®({len(page_jobs)})å’Œé¡µé¢å…ƒç´ ({len(job_elements)})æ•°é‡ä¸åŒ¹é…")
                min_count = min(len(page_jobs), len(job_elements))
                page_jobs = page_jobs[:min_count]
                job_elements = job_elements[:min_count]
            
            # ä¸ºæ¯ä¸ªèŒä½ç”ŸæˆæŒ‡çº¹
            for job in page_jobs:
                if not job.get('job_fingerprint'):
                    job['job_fingerprint'] = generate_job_fingerprint(
                        job.get('title', ''),
                        job.get('company', ''),
                        job.get('salary', ''),
                        job.get('location', '')
                    )
            
            # æå–æ‰€æœ‰æŒ‡çº¹
            fingerprints = [job['job_fingerprint'] for job in page_jobs]
            
            # æ‰¹é‡æ£€æŸ¥æŒ‡çº¹æ˜¯å¦å­˜åœ¨
            db_manager = DatabaseManager(self.data_storage.db_path)
            existing_fingerprints = db_manager.batch_check_fingerprints(fingerprints)
            
            # è¿‡æ»¤å‡ºæ–°èŒä½ï¼ŒåŒæ—¶ä¿ç•™å¯¹åº”çš„é¡µé¢å…ƒç´ 
            new_jobs_with_elements = []
            for i, job in enumerate(page_jobs):
                fingerprint = job['job_fingerprint']
                if not existing_fingerprints.get(fingerprint, False):
                    new_jobs_with_elements.append({
                        'job_data': job,
                        'element': job_elements[i]
                    })
                else:
                    self.logger.debug(f"è·³è¿‡å·²å­˜åœ¨èŒä½: {job.get('title', '')} - {job.get('company', '')}")
            
            self.logger.info(f"ğŸ” æŒ‡çº¹è¿‡æ»¤: æ€»æ•° {len(page_jobs)}, æ–°èŒä½ {len(new_jobs_with_elements)}, é‡å¤ {len(page_jobs) - len(new_jobs_with_elements)}")
            return new_jobs_with_elements
            
        except Exception as e:
            self.logger.error(f"æŒ‡çº¹è¿‡æ»¤å¤±è´¥: {e}")
            # å‡ºé”™æ—¶è¿”å›åŸå§‹åˆ—è¡¨ï¼Œå°½é‡ä¿æŒæ•°æ®å’Œå…ƒç´ çš„å¯¹åº”å…³ç³»
            fallback_result = []
            min_count = min(len(page_jobs), len(job_elements))
            for i in range(min_count):
                fallback_result.append({
                    'job_data': page_jobs[i],
                    'element': job_elements[i]
                })
            return fallback_result
    
    def _extract_new_jobs_details_immediately(self, driver, new_jobs_with_elements: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        ç«‹å³æå–æ–°èŒä½çš„è¯¦æƒ…ä¿¡æ¯ï¼ˆä¿®å¤é€»è¾‘é”™è¯¯ï¼‰
        
        Args:
            driver: WebDriverå®ä¾‹
            new_jobs_with_elements: åŒ…å«èŒä½æ•°æ®å’Œå¯¹åº”é¡µé¢å…ƒç´ çš„åˆ—è¡¨
                                   æ ¼å¼: [{'job_data': job, 'element': element}, ...]
            
        Returns:
            åŒ…å«è¯¦æƒ…ä¿¡æ¯çš„å®Œæ•´èŒä½åˆ—è¡¨
        """
        try:
            if not new_jobs_with_elements:
                return []
            
            # éœ€è¦å¯¼å…¥seleniumç›¸å…³æ¨¡å—
            from selenium.webdriver.common.by import By
            from selenium.webdriver.common.action_chains import ActionChains
            
            self.logger.info(f"ğŸ“„ å¼€å§‹ç«‹å³æå– {len(new_jobs_with_elements)} ä¸ªæ–°èŒä½çš„è¯¦æƒ…ä¿¡æ¯")
            results = []
            
            # å¤„ç†æ¯ä¸ªæ–°èŒä½ï¼ˆç°åœ¨æœ‰äº†æ­£ç¡®çš„æ•°æ®å’Œå…ƒç´ å¯¹åº”å…³ç³»ï¼‰
            for job_index, job_item in enumerate(new_jobs_with_elements):
                job = job_item['job_data']
                job_element = job_item['element']
                job_title = job.get('title', '').strip()
                
                self.logger.info(f"ğŸ¯ å¤„ç†ç¬¬ {job_index+1}/{len(new_jobs_with_elements)} ä¸ªæ–°èŒä½: {job_title}")
                
                try:
                    # è®°å½•å½“å‰çª—å£å¥æŸ„
                    original_windows = driver.window_handles
                    
                    # æ¨¡æ‹Ÿäººç±»æ»šåŠ¨è¡Œä¸º
                    self._simulate_scroll_to_element(driver, job_element)
                    
                    # æ¨¡æ‹Ÿé¼ æ ‡æ‚¬åœï¼ˆå¯é€‰ï¼‰
                    if random.random() < 0.3:  # 30%æ¦‚ç‡æ‚¬åœ
                        ActionChains(driver).move_to_element(job_element).perform()
                        # time.sleep(random.uniform(0.2, 0.8))  # COMMENTED FOR SPEED
                    
                    # ç‚¹å‡»èŒä½æ ‡é¢˜
                    ActionChains(driver).click(job_element).perform()
                    
                    # ç­‰å¾…æ–°çª—å£æ‰“å¼€ - COMMENTED FOR SPEED
                    # wait_time = random.uniform(1.0, 2.0)
                    # time.sleep(wait_time)
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰æ–°çª—å£æ‰“å¼€
                    new_windows = driver.window_handles
                    if len(new_windows) > len(original_windows):
                        # åˆ‡æ¢åˆ°æ–°çª—å£
                        new_window = [w for w in new_windows if w not in original_windows][0]
                        driver.switch_to.window(new_window)
                        
                        # çŸ­æš‚ç­‰å¾…é¡µé¢åŠ è½½ - COMMENTED FOR SPEED
                        time.sleep(random.uniform(0.5, 3.0))
                        
                        # è·å–è¯¦æƒ…é¡µURL
                        detail_url = driver.current_url
                        job['url'] = detail_url
                        
                        # æå–è¯¦æƒ…ä¿¡æ¯
                        detail_info = self.page_parser.parse_job_detail(driver, detail_url)
                        
                        if detail_info:
                            # åˆå¹¶åˆ—è¡¨ä¿¡æ¯å’Œè¯¦æƒ…ä¿¡æ¯
                            complete_job = {**job, **detail_info}
                            
                            # ç«‹å³ä¿å­˜åˆ°æ•°æ®åº“
                            success = self.data_storage.save_job_detail(complete_job, detail_url)
                            if success:
                                results.append(complete_job)
                                self.logger.info(f"âœ… æˆåŠŸå¤„ç†å¹¶ä¿å­˜: {job.get('title', '')}")
                            else:
                                self.logger.warning(f"âš ï¸ ä¿å­˜å¤±è´¥: {job.get('title', '')}")
                        else:
                            self.logger.warning(f"âš ï¸ è¯¦æƒ…æå–å¤±è´¥: {job.get('title', '')}")
                        
                        # å…³é—­æ–°çª—å£å¹¶åˆ‡æ¢å›åŸçª—å£
                        driver.close()
                        driver.switch_to.window(original_windows[0])
                        
                        # æ€è€ƒæ—¶é—´ - COMMENTED FOR SPEED
                        # think_time = random.uniform(0.5, 2.0)
                        # time.sleep(think_time)
                        
                    else:
                        self.logger.warning(f"âš ï¸ ç‚¹å‡» {job.get('title', '')} æœªæ‰“å¼€æ–°çª—å£")
                    
                    # æ¯å¤„ç†å‡ ä¸ªèŒä½åï¼Œæ¨¡æ‹Ÿæ›´é•¿çš„ä¼‘æ¯ - COMMENTED FOR SPEED
                    # if (i + 1) % random.randint(3, 6) == 0:
                    #     rest_time = random.uniform(2.0, 5.0)
                    #     self.logger.info(f"â³ æ¨¡æ‹Ÿç”¨æˆ·ä¼‘æ¯ {rest_time:.1f} ç§’")
                    #     time.sleep(rest_time)
                        
                except Exception as e:
                    self.logger.warning(f"âŒ å¤„ç†èŒä½ {job_index+1} æ—¶å‡ºé”™: {e}")
                    # ç¡®ä¿å›åˆ°åŸçª—å£
                    if len(driver.window_handles) > 1:
                        driver.switch_to.window(driver.window_handles[0])
                    
                    # é”™è¯¯åç­‰å¾…æ—¶é—´ - COMMENTED FOR SPEED
                    # error_wait = random.uniform(3.0, 8.0)
                    # time.sleep(error_wait)
                    continue
            
            self.logger.info(f"ğŸ‰ è¯¦æƒ…æå–å®Œæˆï¼ŒæˆåŠŸå¤„ç† {len(results)} ä¸ªèŒä½")
            return results
            
        except Exception as e:
            self.logger.error(f"âŒ ç«‹å³æå–è¯¦æƒ…ä¿¡æ¯å¤±è´¥: {e}")
            return []
    
    def _save_list_jobs_immediately(self, new_jobs: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        ç«‹å³ä¿å­˜åˆ—è¡¨èŒä½ä¿¡æ¯ï¼ˆä¸æå–è¯¦æƒ…ï¼‰
        
        Args:
            new_jobs: æ–°èŒä½åˆ—è¡¨
            
        Returns:
            ä¿å­˜æˆåŠŸçš„èŒä½åˆ—è¡¨
        """
        try:
            if not new_jobs:
                return []
            
            self.logger.info(f"ğŸ’¾ å¼€å§‹ç«‹å³ä¿å­˜ {len(new_jobs)} ä¸ªåˆ—è¡¨èŒä½ä¿¡æ¯")
            results = []
            
            for job in new_jobs:
                try:
                    # ä½¿ç”¨æ•°æ®å­˜å‚¨å™¨ä¿å­˜èŒä½ä¿¡æ¯
                    success = self.data_storage.save_job_detail(job, job.get('url', ''))
                    if success:
                        results.append(job)
                        self.logger.debug(f"âœ… ä¿å­˜åˆ—è¡¨èŒä½: {job.get('title', '')} - {job.get('company', '')}")
                    else:
                        self.logger.warning(f"âš ï¸ ä¿å­˜å¤±è´¥: {job.get('title', '')} - {job.get('company', '')}")
                        
                except Exception as e:
                    self.logger.warning(f"âŒ ä¿å­˜èŒä½æ—¶å‡ºé”™: {e}")
                    continue
            
            self.logger.info(f"ğŸ’¾ åˆ—è¡¨èŒä½ä¿å­˜å®Œæˆï¼ŒæˆåŠŸä¿å­˜ {len(results)} ä¸ªèŒä½")
            return results
            
        except Exception as e:
            self.logger.error(f"âŒ ä¿å­˜åˆ—è¡¨èŒä½å¤±è´¥: {e}")
            return []
    
    def _simulate_scroll_to_element(self, driver, target_element) -> None:
        """
        æ¨¡æ‹Ÿæ»šåŠ¨åˆ°å…ƒç´ ä½ç½®
        
        Args:
            driver: WebDriverå®ä¾‹
            target_element: ç›®æ ‡å…ƒç´ 
        """
        try:
            # æ»šåŠ¨åˆ°å…ƒç´ ä½ç½®
            driver.execute_script("arguments[0].scrollIntoView({behavior: 'smooth', block: 'center'});", target_element)
            # time.sleep(random.uniform(0.3, 0.8))  # COMMENTED FOR SPEED
            
        except Exception as e:
            self.logger.debug(f"æ¨¡æ‹Ÿæ»šåŠ¨å¤±è´¥: {e}")
            # å›é€€åˆ°ç®€å•æ»šåŠ¨
            try:
                driver.execute_script("arguments[0].scrollIntoView(true);", target_element)
                # time.sleep(0.5)  # COMMENTED FOR SPEED
            except:
                pass
    
    
    def extract_from_search_url(self,
                               search_url: str,
                               keyword: Optional[str] = None,
                               max_results: Optional[int] = None,
                               save_results: bool = True,
                               extract_details: bool = False,
                               max_pages: Optional[int] = None) -> List[Dict[str, Any]]:
        """
        ä»æœç´¢URLæå–èŒä½ä¿¡æ¯ï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼‰
        
        Args:
            search_url: æœç´¢é¡µé¢URL
            keyword: æœç´¢å…³é”®è¯
            max_results: æœ€å¤§ç»“æœæ•°é‡
            save_results: æ˜¯å¦ä¿å­˜ç»“æœ
            extract_details: æ˜¯å¦æå–è¯¦æƒ…é¡µå†…å®¹
            max_pages: æœ€å¤§é¡µæ•°
            
        Returns:
            æå–çš„èŒä½ä¿¡æ¯åˆ—è¡¨
        """
        try:
            self.logger.info(f"ğŸš€ å¼€å§‹ä»æœç´¢URLæå–èŒä½ä¿¡æ¯: {search_url}")
            self.logger.info(f"ğŸ“Š å‚æ•°: å…³é”®è¯='{keyword}', æœ€å¤§ç»“æœ={max_results}, æœ€å¤§é¡µæ•°={max_pages}, æå–è¯¦æƒ…={extract_details}")
            
            # è®¾ç½®å½“å‰å…³é”®è¯å’Œå¼€å§‹æ—¶é—´
            self.current_keyword = keyword or self._extract_keyword_from_url(search_url)
            self.extraction_start_time = time.time()
            self.extraction_results = []
            
            # å‡†å¤‡æµè§ˆå™¨
            driver = self._prepare_browser()
            
            # å¯¼èˆªåˆ°æœç´¢é¡µé¢
            self._navigate_to_page(driver, search_url)
            
            # è·å–é…ç½®å‚æ•°
            max_pages = max_pages or self.search_config.get('strategy', {}).get('max_pages', 5)
            
            # ä¸»å¾ªç¯ï¼šé€é¡µå¤„ç†
            current_page = 1
            all_results = []
            
            self.logger.info(f"ğŸ”„ å¼€å§‹é€é¡µå¤„ç†ï¼Œæœ€å¤§é¡µæ•°: {max_pages}")
            
            while current_page <= max_pages:
                try:
                    self.logger.info(f"ğŸ“„ å¤„ç†ç¬¬ {current_page} é¡µ")
                    
                    # ä½¿ç”¨åŒæ­¥æ–¹æ³•è§£æèŒä½åˆ—è¡¨
                    page_jobs = self.page_parser.parse_job_list(driver, max_results)
                    
                    if not page_jobs:
                        self.logger.warning(f"âš ï¸ ç¬¬ {current_page} é¡µæœªæ‰¾åˆ°èŒä½ä¿¡æ¯")
                        break
                    
                    self.logger.info(f"ğŸ“‹ ç¬¬ {current_page} é¡µæ‰¾åˆ° {len(page_jobs)} ä¸ªèŒä½")
                    
                    # ä¸ºæ¯ä¸ªèŒä½æ·»åŠ é¡µé¢ä¿¡æ¯
                    for job in page_jobs:
                        job['page_number'] = current_page
                        job['search_keyword'] = self.current_keyword
                    
                    # è¿‡æ»¤æ–°èŒä½å¹¶å¤„ç†
                    from selenium.webdriver.common.by import By
                    job_elements = driver.find_elements(By.CSS_SELECTOR, ".jname")
                    new_jobs_with_elements = self._filter_new_jobs_by_fingerprint_with_elements(page_jobs, job_elements)
                    
                    if not new_jobs_with_elements:
                        self.logger.info(f"âœ… ç¬¬ {current_page} é¡µæ‰€æœ‰èŒä½éƒ½å·²å­˜åœ¨ï¼Œè·³è¿‡è¯¦æƒ…æå–")
                    else:
                        self.logger.info(f"ğŸ†• ç¬¬ {current_page} é¡µå‘ç° {len(new_jobs_with_elements)} ä¸ªæ–°èŒä½")
                        
                        # å¯¹æ–°èŒä½ç«‹å³æå–è¯¦æƒ…å¹¶ä¿å­˜
                        if extract_details:
                            page_results = self._extract_new_jobs_details_immediately(driver, new_jobs_with_elements)
                        else:
                            # å¦‚æœä¸æå–è¯¦æƒ…ï¼Œç›´æ¥ä¿å­˜åˆ—è¡¨ä¿¡æ¯
                            new_jobs_only = [item['job_data'] for item in new_jobs_with_elements]
                            page_results = self._save_list_jobs_immediately(new_jobs_only)
                        
                        all_results.extend(page_results)
                        
                        self.logger.info(f"ğŸ’¾ ç¬¬ {current_page} é¡µæˆåŠŸå¤„ç† {len(page_results)} ä¸ªèŒä½")
                    
                    # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€å¤§ç»“æœæ•°é‡
                    if max_results and len(all_results) >= max_results:
                        self.logger.info(f"ğŸ“Š å·²è¾¾åˆ°æœ€å¤§ç»“æœæ•°é‡é™åˆ¶: {max_results}")
                        break
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰ä¸‹ä¸€é¡µ
                    if not self.page_parser.has_next_page(driver):
                        self.logger.info("ğŸ“„ å·²åˆ°è¾¾æœ€åä¸€é¡µ")
                        break
                    
                    # å¯¼èˆªåˆ°ä¸‹ä¸€é¡µ
                    if current_page < max_pages:
                        self.logger.info(f"â¡ï¸ å‡†å¤‡è¿›å…¥ç¬¬ {current_page + 1} é¡µ")
                        
                        # å¯¼èˆªåˆ°ä¸‹ä¸€é¡µ
                        if not self.page_parser.navigate_to_next_page(driver):
                            self.logger.warning("âš ï¸ å¯¼èˆªåˆ°ä¸‹ä¸€é¡µå¤±è´¥ï¼Œç»“æŸæå–")
                            break
                    
                    current_page += 1
                    
                except Exception as e:
                    self.logger.error(f"âŒ å¤„ç†ç¬¬ {current_page} é¡µæ—¶å‡ºé”™: {e}")
                    # å°è¯•ç»§ç»­å¤„ç†ä¸‹ä¸€é¡µ
                    current_page += 1
                    continue
            
            # æ›´æ–°æå–ç»“æœ
            self.extraction_results = all_results
            
            # æ³¨é‡Šæ‰é‡å¤ä¿å­˜ï¼Œå› ä¸ºèŒä½å·²åœ¨ç«‹å³æå–æ—¶ä¿å­˜
            # if save_results and all_results:
            #     self._save_extraction_results(all_results)
            self.logger.info(f"ğŸ’¾ èŒä½å·²åœ¨æå–è¿‡ç¨‹ä¸­ç«‹å³ä¿å­˜ï¼Œè·³è¿‡é‡å¤ä¿å­˜")
            
            # è¾“å‡ºæœ€ç»ˆç»Ÿè®¡
            elapsed_time = time.time() - self.extraction_start_time
            self.logger.info(f"âœ… æå–å®Œæˆ!")
            self.logger.info(f"ğŸ“Š æ€»è®¡å¤„ç† {current_page - 1} é¡µï¼Œæå– {len(all_results)} ä¸ªèŒä½")
            self.logger.info(f"â±ï¸ æ€»è€—æ—¶: {elapsed_time:.2f} ç§’")
            
            return all_results
            
        except Exception as e:
            self.logger.error(f"âŒ ä»æœç´¢URLæå–èŒä½ä¿¡æ¯å¤±è´¥: {e}")
            raise ContentExtractionError(f"ä»æœç´¢URLæå–èŒä½ä¿¡æ¯å¤±è´¥: {e}")