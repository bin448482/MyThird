"""
é¡µé¢è§£æå™¨

è´Ÿè´£æ‰€æœ‰é¡µé¢è§£æå·¥ä½œï¼ŒåŒ…æ‹¬ï¼š
- è§£æèŒä½åˆ—è¡¨é¡µé¢ï¼Œæå–èŒä½åŸºæœ¬ä¿¡æ¯
- è§£æèŒä½è¯¦æƒ…é¡µé¢ï¼Œæå–è¯¦ç»†ä¿¡æ¯
- é€šè¿‡ç‚¹å‡»æ“ä½œè·å–èŒä½è¯¦æƒ…é¡µURL
- é¡µé¢ç»“æ„åˆ†æå’Œè°ƒè¯•
"""

import time
import logging
import random
from datetime import datetime
from typing import Dict, List, Optional, Any
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.action_chains import ActionChains
from selenium.common.exceptions import TimeoutException, NoSuchElementException

from ..core.exceptions import PageParseError


class PageParser:
    """é¡µé¢è§£æå™¨"""
    
    def __init__(self, config: dict):
        """
        åˆå§‹åŒ–é¡µé¢è§£æå™¨
        
        Args:
            config: é…ç½®å­—å…¸
        """
        self.config = config
        self.selectors = config.get('selectors', {})
        self.search_selectors = self.selectors.get('search_page', {})
        self.detail_selectors = self.selectors.get('job_detail', {})
        self.logger = logging.getLogger(__name__)
    
    def parse_job_list(self, driver: webdriver.Chrome, max_results: Optional[int] = None) -> List[Dict[str, Any]]:
        """
        è§£æèŒä½åˆ—è¡¨é¡µé¢
        
        Args:
            driver: WebDriverå®ä¾‹
            max_results: æœ€å¤§ç»“æœæ•°é‡
            
        Returns:
            èŒä½ä¿¡æ¯åˆ—è¡¨
            
        Raises:
            PageParseError: é¡µé¢è§£æå¤±è´¥
        """
        try:
            self.logger.info("ğŸ“Š å¼€å§‹è§£æèŒä½åˆ—è¡¨é¡µé¢")
            
            # ç­‰å¾…é¡µé¢ç¨³å®š
            self._wait_for_page_stable(driver)
            
            # è·å–èŒä½åˆ—è¡¨å…ƒç´ 
            job_elements = self._get_job_elements(driver)
            
            if not job_elements:
                self.logger.warning("âš ï¸ æœªæ‰¾åˆ°èŒä½åˆ—è¡¨å…ƒç´ ")
                self._debug_page_structure(driver)
                return []
            
            self.logger.info(f"ğŸ“‹ æ‰¾åˆ° {len(job_elements)} ä¸ªèŒä½å…ƒç´ ")
            
            # è§£ææ¯ä¸ªèŒä½
            jobs = []
            processed_count = 0
            
            for i, job_element in enumerate(job_elements, 1):
                if max_results and processed_count >= max_results:
                    self.logger.info(f"ğŸ“Š å·²è¾¾åˆ°æœ€å¤§ç»“æœæ•°é‡é™åˆ¶: {max_results}")
                    break
                
                try:
                    job_data = self._parse_job_element(job_element)
                    if job_data:
                        jobs.append(job_data)
                        processed_count += 1
                        
                        self.logger.debug(
                            f"ğŸ“ èŒä½ {processed_count}: {job_data.get('title', 'æœªçŸ¥')} - "
                            f"{job_data.get('company', 'æœªçŸ¥')} - {job_data.get('salary', 'é¢è®®')}"
                        )
                    
                except Exception as e:
                    self.logger.warning(f"âš ï¸ è§£æèŒä½ {i} æ—¶å‡ºé”™: {e}")
                    continue
            
            self.logger.info(f"âœ… èŒä½åˆ—è¡¨è§£æå®Œæˆï¼Œå…±è§£æ {len(jobs)} ä¸ªèŒä½")
            return jobs
            
        except Exception as e:
            self.logger.error(f"âŒ è§£æèŒä½åˆ—è¡¨å¤±è´¥: {e}")
            raise PageParseError(f"è§£æèŒä½åˆ—è¡¨å¤±è´¥: {e}")
    
    def _wait_for_page_stable(self, driver: webdriver.Chrome, timeout: int = 10) -> None:
        """
        ç­‰å¾…é¡µé¢ç¨³å®šåŠ è½½ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰
        
        Args:
            driver: WebDriverå®ä¾‹
            timeout: è¶…æ—¶æ—¶é—´
        """
        try:
            # ç­‰å¾…é¡µé¢åŠ è½½å®Œæˆ
            WebDriverWait(driver, timeout).until(
                lambda d: d.execute_script("return document.readyState") == "complete"
            )
            
            # ä¼˜åŒ–ï¼šå‡å°‘é¢å¤–ç­‰å¾…æ—¶é—´
            time.sleep(0.5)  # ä»2ç§’å‡å°‘åˆ°0.5ç§’
            
            self.logger.debug("é¡µé¢å·²ç¨³å®šåŠ è½½")
            
        except TimeoutException:
            self.logger.warning("é¡µé¢åŠ è½½è¶…æ—¶ï¼Œç»§ç»­å°è¯•è§£æ")
    
    def _get_job_elements(self, driver: webdriver.Chrome) -> List:
        """
        è·å–èŒä½åˆ—è¡¨å…ƒç´ 
        
        Args:
            driver: WebDriverå®ä¾‹
            
        Returns:
            èŒä½å…ƒç´ åˆ—è¡¨
        """
        job_list_selector = self.search_selectors.get('job_list', '.job-list-item')
        
        try:
            # å¦‚æœjob_list_selectoræ˜¯å®¹å™¨é€‰æ‹©å™¨ï¼ˆå¦‚.joblistï¼‰ï¼Œéœ€è¦æŸ¥æ‰¾å…¶å­å…ƒç´ 
            if job_list_selector == '.joblist':
                # åœ¨.joblistå®¹å™¨å†…æŸ¥æ‰¾èŒä½é¡¹
                job_item_selectors = [
                    '.joblist .joblist-item',
                    '.joblist > div',
                    '.joblist li',
                    '.joblist [data-jobid]',
                    '.joblist .job-item'
                ]
                
                for selector in job_item_selectors:
                    try:
                        elements = driver.find_elements(By.CSS_SELECTOR, selector)
                        if elements:
                            self.logger.debug(f"åœ¨å®¹å™¨å†…æ‰¾åˆ°èŒä½é¡¹: {selector} (æ•°é‡: {len(elements)})")
                            return elements
                    except:
                        continue
                
                # å¦‚æœæ‰¾ä¸åˆ°å­å…ƒç´ ï¼Œå°è¯•ç›´æ¥ä½¿ç”¨å®¹å™¨çš„ç›´æ¥å­å…ƒç´ 
                try:
                    container = driver.find_element(By.CSS_SELECTOR, job_list_selector)
                    # è·å–å®¹å™¨çš„æ‰€æœ‰ç›´æ¥å­å…ƒç´ 
                    job_elements = container.find_elements(By.XPATH, "./*")
                    if job_elements:
                        self.logger.debug(f"ä½¿ç”¨å®¹å™¨ç›´æ¥å­å…ƒç´ ï¼Œæ‰¾åˆ° {len(job_elements)} ä¸ª")
                        return job_elements
                except:
                    pass
            else:
                # ç›´æ¥ä½¿ç”¨é…ç½®çš„é€‰æ‹©å™¨
                job_elements = driver.find_elements(By.CSS_SELECTOR, job_list_selector)
                if job_elements:
                    self.logger.debug(f"ä½¿ç”¨é€‰æ‹©å™¨ '{job_list_selector}' æ‰¾åˆ° {len(job_elements)} ä¸ªå…ƒç´ ")
                    return job_elements
            
            # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œå°è¯•å…¶ä»–å¯èƒ½çš„é€‰æ‹©å™¨
            alternative_selectors = [
                '.job-item',
                '.position-item',
                '.search-result-item',
                '.list-item',
                '[data-testid*="job"]',
                '.joblist-item',
                '.job-box',
                '.position-box'
            ]
            
            for selector in alternative_selectors:
                try:
                    elements = driver.find_elements(By.CSS_SELECTOR, selector)
                    if elements:
                        self.logger.info(f"ä½¿ç”¨å¤‡ç”¨é€‰æ‹©å™¨ '{selector}' æ‰¾åˆ° {len(elements)} ä¸ªå…ƒç´ ")
                        return elements
                except:
                    continue
            
            return []
            
        except Exception as e:
            self.logger.error(f"è·å–èŒä½å…ƒç´ å¤±è´¥: {e}")
            return []
    
    def _parse_job_element(self, job_element) -> Optional[Dict[str, Any]]:
        """
        è§£æå•ä¸ªèŒä½å…ƒç´ 
        
        Args:
            job_element: èŒä½å…ƒç´ 
            
        Returns:
            èŒä½æ•°æ®å­—å…¸
        """
        job_data = {
            'extracted_at': time.strftime('%Y-%m-%d %H:%M:%S'),
            'source': 'qiancheng'
        }
        
        try:
            # èŒä½æ ‡é¢˜å’Œé“¾æ¥
            title_info = self._extract_title_and_url(job_element)
            job_data.update(title_info)
            
            # å…¬å¸åç§°
            job_data['company'] = self._extract_text_by_selector(
                job_element, 
                self.search_selectors.get('company_name', '.company-name'),
                default="æœªçŸ¥å…¬å¸"
            )
            
            # è–ªèµ„
            job_data['salary'] = self._extract_text_by_selector(
                job_element,
                self.search_selectors.get('salary', '.salary'),
                default="è–ªèµ„é¢è®®"
            )
            
            # åœ°ç‚¹
            job_data['location'] = self._extract_text_by_selector(
                job_element,
                self.search_selectors.get('location', '.location'),
                default="æœªçŸ¥åœ°ç‚¹"
            )
            
            # ç»éªŒè¦æ±‚
            job_data['experience'] = self._extract_text_by_selector(
                job_element,
                self.search_selectors.get('experience', '.experience'),
                default="ç»éªŒä¸é™"
            )
            
            # å­¦å†è¦æ±‚
            job_data['education'] = self._extract_text_by_selector(
                job_element,
                self.search_selectors.get('education', '.education'),
                default="å­¦å†ä¸é™"
            )
            
            # æå–é¢å¤–ä¿¡æ¯
            extra_info = self._extract_extra_info(job_element)
            job_data.update(extra_info)
            
            return job_data
            
        except Exception as e:
            self.logger.warning(f"è§£æèŒä½å…ƒç´ å¤±è´¥: {e}")
            return None
    
    def _extract_title_and_url(self, job_element) -> Dict[str, str]:
        """
        æå–èŒä½æ ‡é¢˜å’Œé“¾æ¥
        
        Args:
            job_element: èŒä½å…ƒç´ 
            
        Returns:
            åŒ…å«titleå’Œurlçš„å­—å…¸
        """
        try:
            # æ–¹æ³•1: æŸ¥æ‰¾ .jname å…ƒç´ ï¼ˆèŒä½æ ‡é¢˜ï¼‰
            job_title = None
            job_url = None
            
            # å°è¯•æ‰¾åˆ°èŒä½æ ‡é¢˜å…ƒç´ 
            jname_selectors = ['.jname', '.job-name', '.position-title']
            for selector in jname_selectors:
                try:
                    jname_element = job_element.find_element(By.CSS_SELECTOR, selector)
                    job_title = jname_element.text.strip()
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰titleå±æ€§
                    title_attr = jname_element.get_attribute('title')
                    if title_attr:
                        job_title = title_attr.strip()
                    
                    if job_title:
                        self.logger.debug(f"æ‰¾åˆ°èŒä½æ ‡é¢˜: {job_title}")
                        break
                except:
                    continue
            
            # æ–¹æ³•2: æŸ¥æ‰¾å¯¹åº”çš„é“¾æ¥URL
            # é¦–å…ˆå°è¯•åœ¨åŒä¸€ä¸ªçˆ¶å…ƒç´ ä¸­æŸ¥æ‰¾é“¾æ¥
            all_links = job_element.find_elements(By.TAG_NAME, "a")
            
            for link in all_links:
                href = link.get_attribute('href')
                if href and 'jobs.51job.com' in href:
                    job_url = href
                    
                    # å¦‚æœé“¾æ¥æ–‡æœ¬åŒ…å«èŒä½å…³é”®è¯ï¼Œä¼˜å…ˆä½¿ç”¨
                    link_text = link.text.strip()
                    job_keywords = ['å·¥ç¨‹å¸ˆ', 'å¼€å‘', 'æ¶æ„å¸ˆ', 'ç»ç†', 'ä¸“å‘˜', 'ä¸»ç®¡', 'æ€»ç›‘', 'AI', 'ç®—æ³•', 'æ•°æ®']
                    if any(keyword in link_text for keyword in job_keywords):
                        if not job_title:  # å¦‚æœæ²¡æœ‰ä».jnameæ‰¾åˆ°æ ‡é¢˜ï¼Œä½¿ç”¨é“¾æ¥æ–‡æœ¬
                            job_title = link_text
                        break
            
            # å¦‚æœè¿˜æ²¡æ‰¾åˆ°URLï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªæœ‰æ•ˆçš„jobs.51job.comé“¾æ¥
            if not job_url:
                for link in all_links:
                    href = link.get_attribute('href')
                    if href and 'jobs.51job.com' in href:
                        job_url = href
                        break
            
            # æ–¹æ³•3: å¦‚æœæ²¡æœ‰æ‰¾åˆ°.jnameï¼Œå°è¯•ä»é“¾æ¥ä¸­æå–èŒä½æ ‡é¢˜
            if not job_title:
                for link in all_links:
                    href = link.get_attribute('href')
                    text = link.text.strip()
                    
                    if href and 'jobs.51job.com' in href and text:
                        # æ’é™¤æ˜æ˜¾çš„å…¬å¸åç§°
                        company_keywords = ['å…¬å¸', 'ä¼ä¸š', 'é›†å›¢', 'æœ‰é™', 'è‚¡ä»½']
                        is_company = any(keyword in text for keyword in company_keywords)
                        
                        if not is_company:
                            job_title = text
                            if not job_url:
                                job_url = href
                            break
            
            # è¿”å›ç»“æœ
            if job_title and job_url:
                return {
                    'title': job_title,
                    'url': job_url,
                    'needs_click_extraction': False
                }
            elif job_url:
                # æœ‰URLä½†æ²¡æœ‰æ ‡é¢˜ï¼Œä½¿ç”¨é»˜è®¤æ ‡é¢˜
                return {
                    'title': "èŒä½ä¿¡æ¯",
                    'url': job_url,
                    'needs_click_extraction': False
                }
            elif job_title:
                # æœ‰æ ‡é¢˜ä½†æ— URLï¼Œéœ€è¦ç‚¹å‡»æå–
                return {
                    'title': job_title,
                    'url': "",
                    'needs_click_extraction': True
                }
            else:
                return {
                    'title': "æœªçŸ¥èŒä½",
                    'url': "",
                    'needs_click_extraction': False
                }
                
        except Exception as e:
            self.logger.warning(f"æå–èŒä½æ ‡é¢˜å’Œé“¾æ¥å¤±è´¥: {e}")
            return {'title': "æœªçŸ¥èŒä½", 'url': ""}
    
    def _extract_text_by_selector(self, parent_element, selector: str, default: str = "") -> str:
        """
        é€šè¿‡é€‰æ‹©å™¨æå–æ–‡æœ¬
        
        Args:
            parent_element: çˆ¶å…ƒç´ 
            selector: CSSé€‰æ‹©å™¨
            default: é»˜è®¤å€¼
            
        Returns:
            æå–çš„æ–‡æœ¬
        """
        try:
            element = parent_element.find_element(By.CSS_SELECTOR, selector)
            text = element.text.strip()
            return text if text else default
        except:
            return default
    
    def _extract_extra_info(self, job_element) -> Dict[str, Any]:
        """
        æå–é¢å¤–ä¿¡æ¯
        
        Args:
            job_element: èŒä½å…ƒç´ 
            
        Returns:
            é¢å¤–ä¿¡æ¯å­—å…¸
        """
        extra_info = {}
        
        try:
            # å°è¯•æå–å‘å¸ƒæ—¶é—´
            time_selectors = ['.publish-time', '.update-time', '.time', '[data-time]']
            for selector in time_selectors:
                try:
                    time_element = job_element.find_element(By.CSS_SELECTOR, selector)
                    extra_info['publish_time'] = time_element.text.strip()
                    break
                except:
                    continue
            
            # å°è¯•æå–å…¬å¸è§„æ¨¡
            scale_selectors = ['.company-scale', '.company-size', '.scale']
            for selector in scale_selectors:
                try:
                    scale_element = job_element.find_element(By.CSS_SELECTOR, selector)
                    extra_info['company_scale'] = scale_element.text.strip()
                    break
                except:
                    continue
            
            # å°è¯•æå–è¡Œä¸šä¿¡æ¯
            industry_selectors = ['.industry', '.company-industry', '.business']
            for selector in industry_selectors:
                try:
                    industry_element = job_element.find_element(By.CSS_SELECTOR, selector)
                    extra_info['industry'] = industry_element.text.strip()
                    break
                except:
                    continue
            
        except Exception as e:
            self.logger.debug(f"æå–é¢å¤–ä¿¡æ¯æ—¶å‡ºé”™: {e}")
        
        return extra_info
    
    def parse_job_detail(self, driver: webdriver.Chrome, job_url: str) -> Optional[Dict[str, Any]]:
        """
        è§£æèŒä½è¯¦æƒ…é¡µé¢
        
        Args:
            driver: WebDriverå®ä¾‹
            job_url: èŒä½è¯¦æƒ…URL
            
        Returns:
            èŒä½è¯¦æƒ…æ•°æ®
        """
        try:
            self.logger.info(f"ğŸ“„ è§£æèŒä½è¯¦æƒ…: {job_url}")
            
            # ä¼˜åŒ–ï¼šå‡å°‘è¯¦æƒ…é¡µç­‰å¾…æ—¶é—´
            self._wait_for_page_stable(driver, timeout=8)  # ä»15ç§’å‡å°‘åˆ°8ç§’
            
            # æ£€æŸ¥é¡µé¢æ˜¯å¦æ­£å¸¸åŠ è½½
            page_title = driver.title
            if not page_title or '404' in page_title or 'error' in page_title.lower():
                self.logger.warning(f"é¡µé¢å¯èƒ½æœªæ­£å¸¸åŠ è½½: {page_title}")
                return None
            
            detail_data = {
                'url': job_url,
                'extracted_at': time.strftime('%Y-%m-%d %H:%M:%S'),
                'page_title': page_title
            }
            
            # å°è¯•å¤šç§é€‰æ‹©å™¨æå–èŒä½æè¿°
            description_selectors = [
                self.detail_selectors.get('job_description', '.job-description'),
                '.job-detail-content',
                '.job-desc',
                '.position-detail',
                '.job-content',
                '[class*="description"]',
                '[class*="detail"]'
            ]
            
            for selector in description_selectors:
                description = self._extract_text_by_selector(driver, selector, default="")
                if description and len(description) > 50:  # ç¡®ä¿è·å–åˆ°æœ‰æ„ä¹‰çš„å†…å®¹
                    detail_data['description'] = description
                    break
            else:
                detail_data['description'] = ""
            
            # å°è¯•å¤šç§é€‰æ‹©å™¨æå–èŒä½è¦æ±‚
            requirements_selectors = [
                self.detail_selectors.get('requirements', '.job-requirements'),
                '.job-require',
                '.position-require',
                '.job-demand',
                '[class*="requirement"]',
                '[class*="require"]'
            ]
            
            for selector in requirements_selectors:
                requirements = self._extract_text_by_selector(driver, selector, default="")
                if requirements and len(requirements) > 20:
                    detail_data['requirements'] = requirements
                    break
            else:
                detail_data['requirements'] = ""
            
            # å…¬å¸ä¿¡æ¯
            company_selectors = [
                self.detail_selectors.get('company_info', '.company-info'),
                '.company-detail',
                '.company-desc',
                '[class*="company"]'
            ]
            
            for selector in company_selectors:
                company_info = self._extract_text_by_selector(driver, selector, default="")
                if company_info:
                    detail_data['company_info'] = company_info
                    break
            else:
                detail_data['company_info'] = ""
            
            # ç¦åˆ©å¾…é‡
            benefits_selectors = [
                self.detail_selectors.get('benefits', '.job-benefits'),
                '.welfare',
                '.benefits',
                '.job-welfare',
                '[class*="benefit"]',
                '[class*="welfare"]'
            ]
            
            for selector in benefits_selectors:
                benefits = self._extract_text_by_selector(driver, selector, default="")
                if benefits:
                    detail_data['benefits'] = benefits
                    break
            else:
                detail_data['benefits'] = ""
            
            # æå–å…¶ä»–å¯èƒ½çš„ä¿¡æ¯
            try:
                # è–ªèµ„ä¿¡æ¯
                salary_selectors = ['.salary', '.pay', '[class*="salary"]', '[class*="pay"]']
                for selector in salary_selectors:
                    salary = self._extract_text_by_selector(driver, selector, default="")
                    if salary:
                        detail_data['salary_detail'] = salary
                        break
                
                # å·¥ä½œåœ°ç‚¹
                location_selectors = ['.location', '.address', '[class*="location"]', '[class*="address"]']
                for selector in location_selectors:
                    location = self._extract_text_by_selector(driver, selector, default="")
                    if location:
                        detail_data['location_detail'] = location
                        break
                
            except Exception as e:
                self.logger.debug(f"æå–é¢å¤–ä¿¡æ¯æ—¶å‡ºé”™: {e}")
            
            # éªŒè¯æ˜¯å¦æå–åˆ°æœ‰æ•ˆå†…å®¹
            content_fields = ['description', 'requirements', 'company_info', 'benefits']
            has_content = any(detail_data.get(field) for field in content_fields)
            
            if not has_content:
                self.logger.warning(f"æœªæå–åˆ°æœ‰æ•ˆçš„è¯¦æƒ…å†…å®¹: {job_url}")
                # è°ƒè¯•é¡µé¢ç»“æ„
                self._debug_page_structure(driver)
                return None
            
            return detail_data
            
        except Exception as e:
            self.logger.error(f"âŒ è§£æèŒä½è¯¦æƒ…å¤±è´¥: {e}")
            return None
    
    def _debug_page_structure(self, driver: webdriver.Chrome) -> None:
        """
        è°ƒè¯•é¡µé¢ç»“æ„
        
        Args:
            driver: WebDriverå®ä¾‹
        """
        self.logger.info("ğŸ” è°ƒè¯•é¡µé¢ç»“æ„...")
        
        try:
            # è·å–é¡µé¢åŸºæœ¬ä¿¡æ¯
            self.logger.info(f"å½“å‰URL: {driver.current_url}")
            self.logger.info(f"é¡µé¢æ ‡é¢˜: {driver.title}")
            
            # æŸ¥æ‰¾å¯èƒ½çš„èŒä½åˆ—è¡¨å…ƒç´ 
            possible_selectors = [
                ".job-list",
                ".job-item",
                ".position-list",
                ".search-result",
                "[data-testid*='job']",
                ".list-item",
                ".joblist",
                ".job-box"
            ]
            
            for selector in possible_selectors:
                try:
                    elements = driver.find_elements(By.CSS_SELECTOR, selector)
                    if elements:
                        self.logger.info(f"æ‰¾åˆ°å¯èƒ½çš„èŒä½åˆ—è¡¨: {selector} (æ•°é‡: {len(elements)})")
                except:
                    continue
            
            # è¾“å‡ºé¡µé¢æºç ç‰‡æ®µç”¨äºåˆ†æ
            page_source = driver.page_source[:2000]
            self.logger.debug(f"é¡µé¢æºç ç‰‡æ®µ: {page_source}...")
            
        except Exception as e:
            self.logger.error(f"è°ƒè¯•é¡µé¢ç»“æ„å¤±è´¥: {e}")
    
    def get_page_info(self, driver: webdriver.Chrome) -> Dict[str, Any]:
        """
        è·å–é¡µé¢åŸºæœ¬ä¿¡æ¯
        
        Args:
            driver: WebDriverå®ä¾‹
            
        Returns:
            é¡µé¢ä¿¡æ¯å­—å…¸
        """
        try:
            return {
                'url': driver.current_url,
                'title': driver.title,
                'ready_state': driver.execute_script("return document.readyState"),
                'page_height': driver.execute_script("return document.body.scrollHeight"),
                'viewport_height': driver.execute_script("return window.innerHeight"),
                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
            }
        except Exception as e:
            self.logger.error(f"è·å–é¡µé¢ä¿¡æ¯å¤±è´¥: {e}")
            return {}
    
    def extract_job_urls_by_clicking(self,
                                   driver: webdriver.Chrome,
                                   max_jobs: int = 10) -> List[Dict[str, Any]]:
        """
        é€šè¿‡æ¨¡æ‹Ÿç‚¹å‡»èŒä½æ ‡é¢˜æå–è¯¦æƒ…é¡µURL
        
        è¿™æ˜¯PageParserçš„æ ¸å¿ƒåŠŸèƒ½ä¹‹ä¸€ï¼Œè´Ÿè´£ï¼š
        1. è¯†åˆ«é¡µé¢ä¸­çš„èŒä½æ ‡é¢˜å…ƒç´ 
        2. æ¨¡æ‹Ÿäººç±»ç‚¹å‡»è¡Œä¸ºè·å–è¯¦æƒ…é¡µURL
        3. å¤„ç†é¡µé¢äº¤äº’å’Œçª—å£åˆ‡æ¢
        4. è¿”å›ç»“æ„åŒ–çš„URLæ•°æ®
        
        Args:
            driver: WebDriverå®ä¾‹ï¼ˆç”±ContentExtractoræä¾›ï¼‰
            max_jobs: æœ€å¤§æå–æ•°é‡
            
        Returns:
            èŒä½URLä¿¡æ¯åˆ—è¡¨ï¼ŒåŒ…å«æ ‡é¢˜ã€URLã€æå–æ—¶é—´ç­‰
        """
        try:
            self.logger.info(f"ğŸš€ å¼€å§‹é€šè¿‡ç‚¹å‡»æå–èŒä½URLï¼Œæœ€å¤§æ•°é‡: {max_jobs}")
            
            # ç­‰å¾…é¡µé¢åŠ è½½å®Œæˆ
            WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, ".jname"))
            )
            
            # æ¨¡æ‹Ÿäººç±»æµè§ˆè¡Œä¸º - éšæœºç­‰å¾…
            initial_wait = random.uniform(2.0, 4.0)
            time.sleep(initial_wait)
            self.logger.debug(f"åˆå§‹ç­‰å¾… {initial_wait:.1f} ç§’")
            
            # æŸ¥æ‰¾æ‰€æœ‰èŒä½æ ‡é¢˜å…ƒç´ 
            job_title_elements = driver.find_elements(By.CSS_SELECTOR, ".jname")
            self.logger.info(f"âœ… æ‰¾åˆ° {len(job_title_elements)} ä¸ªèŒä½æ ‡é¢˜")
            
            if not job_title_elements:
                self.logger.warning("âš ï¸ æœªæ‰¾åˆ°èŒä½æ ‡é¢˜å…ƒç´ ")
                return []
            
            # é™åˆ¶æå–æ•°é‡
            jobs_to_process = min(len(job_title_elements), max_jobs)
            extracted_urls = []
            
            for i in range(jobs_to_process):
                try:
                    # é‡æ–°è·å–å…ƒç´ ï¼ˆé¿å…stale elementå¼‚å¸¸ï¼‰
                    job_title_elements = driver.find_elements(By.CSS_SELECTOR, ".jname")
                    if i >= len(job_title_elements):
                        break
                        
                    job_element = job_title_elements[i]
                    job_title = job_element.text.strip()
                    
                    self.logger.info(f"ğŸ¯ å¤„ç†ç¬¬ {i+1} ä¸ªèŒä½: {job_title}")
                    
                    # è®°å½•å½“å‰çª—å£å¥æŸ„
                    original_windows = driver.window_handles
                    
                    # æ¨¡æ‹Ÿäººç±»æ»šåŠ¨è¡Œä¸º
                    self._simulate_human_scroll(driver, job_element)
                    
                    # æ¨¡æ‹Ÿé¼ æ ‡æ‚¬åœï¼ˆå¯é€‰ï¼‰
                    if random.random() < 0.3:  # 30%æ¦‚ç‡æ‚¬åœ
                        ActionChains(driver).move_to_element(job_element).perform()
                        time.sleep(random.uniform(0.2, 0.8))
                    
                    # ç‚¹å‡»èŒä½æ ‡é¢˜
                    ActionChains(driver).click(job_element).perform()
                    
                    # ç­‰å¾…æ–°çª—å£æ‰“å¼€ - éšæœºç­‰å¾…æ—¶é—´
                    wait_time = random.uniform(1.5, 3.0)
                    time.sleep(wait_time)
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰æ–°çª—å£æ‰“å¼€
                    new_windows = driver.window_handles
                    if len(new_windows) > len(original_windows):
                        # åˆ‡æ¢åˆ°æ–°çª—å£
                        new_window = [w for w in new_windows if w not in original_windows][0]
                        driver.switch_to.window(new_window)
                        
                        # çŸ­æš‚ç­‰å¾…é¡µé¢åŠ è½½
                        time.sleep(random.uniform(0.5, 1.5))
                        
                        # è·å–è¯¦æƒ…é¡µURL
                        detail_url = driver.current_url
                        
                        job_info = {
                            'index': i + 1,
                            'title': job_title,
                            'detail_url': detail_url,
                            'extracted_at': datetime.now().isoformat()
                        }
                        
                        extracted_urls.append(job_info)
                        self.logger.info(f"âœ… æˆåŠŸæå–: {detail_url}")
                        
                        # å…³é—­æ–°çª—å£å¹¶åˆ‡æ¢å›åŸçª—å£
                        driver.close()
                        driver.switch_to.window(original_windows[0])
                        
                        # æ¨¡æ‹Ÿç”¨æˆ·æ€è€ƒæ—¶é—´
                        think_time = random.uniform(0.5, 2.0)
                        time.sleep(think_time)
                        
                    else:
                        self.logger.warning(f"âš ï¸ ç‚¹å‡» {job_title} æœªæ‰“å¼€æ–°çª—å£")
                    
                    # æ¯å¤„ç†å‡ ä¸ªèŒä½åï¼Œæ¨¡æ‹Ÿæ›´é•¿çš„ä¼‘æ¯
                    if (i + 1) % random.randint(3, 6) == 0:
                        rest_time = random.uniform(2.0, 5.0)
                        self.logger.debug(f"â³ æ¨¡æ‹Ÿç”¨æˆ·ä¼‘æ¯ {rest_time:.1f} ç§’")
                        time.sleep(rest_time)
                        
                except Exception as e:
                    self.logger.warning(f"âŒ å¤„ç†èŒä½ {i+1} æ—¶å‡ºé”™: {e}")
                    # ç¡®ä¿å›åˆ°åŸçª—å£
                    if len(driver.window_handles) > 1:
                        driver.switch_to.window(driver.window_handles[0])
                    
                    # å‡ºé”™åç­‰å¾…æ›´é•¿æ—¶é—´
                    error_wait = random.uniform(3.0, 8.0)
                    time.sleep(error_wait)
                    continue
            
            self.logger.info(f"ğŸ‰ æˆåŠŸæå– {len(extracted_urls)} ä¸ªèŒä½URL")
            return extracted_urls
            
        except Exception as e:
            self.logger.error(f"âŒ ç‚¹å‡»æå–URLè¿‡ç¨‹å‡ºé”™: {e}")
            raise PageParseError(f"ç‚¹å‡»æå–URLå¤±è´¥: {e}")
    
    def _simulate_human_scroll(self, driver: webdriver.Chrome, target_element) -> None:
        """
        æ¨¡æ‹Ÿäººç±»æ»šåŠ¨è¡Œä¸º
        
        Args:
            driver: WebDriverå®ä¾‹
            target_element: ç›®æ ‡å…ƒç´ 
        """
        try:
            # è·å–å…ƒç´ ä½ç½®
            element_location = target_element.location_once_scrolled_into_view
            
            # æ¨¡æ‹Ÿåˆ†æ­¥æ»šåŠ¨è€Œä¸æ˜¯ç›´æ¥æ»šåŠ¨åˆ°å…ƒç´ 
            current_scroll = driver.execute_script("return window.pageYOffset;")
            target_scroll = element_location['y'] - random.randint(100, 300)  # ç•™ä¸€äº›ä½™é‡
            
            if abs(target_scroll - current_scroll) > 100:
                # åˆ†å¤šæ­¥æ»šåŠ¨
                steps = random.randint(2, 4)
                scroll_step = (target_scroll - current_scroll) / steps
                
                for step in range(steps):
                    scroll_to = current_scroll + scroll_step * (step + 1)
                    driver.execute_script(f"window.scrollTo(0, {scroll_to});")
                    time.sleep(random.uniform(0.1, 0.3))
            
            # æœ€åç²¾ç¡®æ»šåŠ¨åˆ°å…ƒç´ 
            driver.execute_script("arguments[0].scrollIntoView({behavior: 'smooth', block: 'center'});", target_element)
            time.sleep(random.uniform(0.3, 0.8))
            
        except Exception as e:
            self.logger.debug(f"æ¨¡æ‹Ÿæ»šåŠ¨å¤±è´¥: {e}")
            # å›é€€åˆ°ç®€å•æ»šåŠ¨
            driver.execute_script("arguments[0].scrollIntoView(true);", target_element)
            time.sleep(0.5)
    
    def has_next_page(self, driver: webdriver.Chrome) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦æœ‰ä¸‹ä¸€é¡µ
        
        Args:
            driver: WebDriverå®ä¾‹
            
        Returns:
            æ˜¯å¦æœ‰ä¸‹ä¸€é¡µ
        """
        try:
            # å°è¯•å¤šç§ä¸‹ä¸€é¡µæŒ‰é’®é€‰æ‹©å™¨
            next_page_selectors = [
                self.search_selectors.get('next_page', '.btn_next'),
                '.next-page',
                '.page-next',
                '.btn-next',
                '.pager-next',
                'a[title*="ä¸‹ä¸€é¡µ"]',
                'a[title*="next"]',
                '.pagination .next',
                '.pagination-next',
                '.page-item.next a',
                '.page-link[aria-label*="Next"]'
            ]
            
            for selector in next_page_selectors:
                try:
                    next_button = driver.find_element(By.CSS_SELECTOR, selector)
                    
                    # æ£€æŸ¥æŒ‰é’®æ˜¯å¦å¯ç”¨ï¼ˆä¸æ˜¯ç¦ç”¨çŠ¶æ€ï¼‰
                    if next_button.is_enabled() and next_button.is_displayed():
                        # æ£€æŸ¥æ˜¯å¦æœ‰disabledç±»æˆ–å±æ€§
                        classes = next_button.get_attribute('class') or ''
                        disabled_attr = next_button.get_attribute('disabled')
                        
                        if 'disabled' not in classes.lower() and not disabled_attr:
                            self.logger.debug(f"æ‰¾åˆ°å¯ç”¨çš„ä¸‹ä¸€é¡µæŒ‰é’®: {selector}")
                            return True
                    
                except NoSuchElementException:
                    continue
                except Exception as e:
                    self.logger.debug(f"æ£€æŸ¥ä¸‹ä¸€é¡µæŒ‰é’®æ—¶å‡ºé”™ {selector}: {e}")
                    continue
            
            self.logger.debug("æœªæ‰¾åˆ°å¯ç”¨çš„ä¸‹ä¸€é¡µæŒ‰é’®")
            return False
            
        except Exception as e:
            self.logger.error(f"æ£€æŸ¥ä¸‹ä¸€é¡µå¤±è´¥: {e}")
            return False
    
    def navigate_to_next_page(self, driver: webdriver.Chrome) -> bool:
        """
        å¯¼èˆªåˆ°ä¸‹ä¸€é¡µ
        
        Args:
            driver: WebDriverå®ä¾‹
            
        Returns:
            æ˜¯å¦æˆåŠŸå¯¼èˆªåˆ°ä¸‹ä¸€é¡µ
        """
        try:
            self.logger.info("ğŸ”„ å°è¯•å¯¼èˆªåˆ°ä¸‹ä¸€é¡µ")
            
            # è®°å½•å½“å‰é¡µé¢URLç”¨äºéªŒè¯
            current_url = driver.current_url
            
            # å°è¯•å¤šç§ä¸‹ä¸€é¡µæŒ‰é’®é€‰æ‹©å™¨
            next_page_selectors = [
                self.search_selectors.get('next_page', '.btn_next'),
                '.next-page',
                '.page-next',
                '.btn-next',
                '.pager-next',
                'a[title*="ä¸‹ä¸€é¡µ"]',
                'a[title*="next"]',
                '.pagination .next',
                '.pagination-next',
                '.page-item.next a',
                '.page-link[aria-label*="Next"]'
            ]
            
            for selector in next_page_selectors:
                try:
                    next_button = driver.find_element(By.CSS_SELECTOR, selector)
                    
                    # æ£€æŸ¥æŒ‰é’®æ˜¯å¦å¯ç”¨
                    if next_button.is_enabled() and next_button.is_displayed():
                        classes = next_button.get_attribute('class') or ''
                        disabled_attr = next_button.get_attribute('disabled')
                        
                        if 'disabled' not in classes.lower() and not disabled_attr:
                            self.logger.info(f"ç‚¹å‡»ä¸‹ä¸€é¡µæŒ‰é’®: {selector}")
                            
                            # æ»šåŠ¨åˆ°æŒ‰é’®ä½ç½®
                            driver.execute_script("arguments[0].scrollIntoView({behavior: 'smooth', block: 'center'});", next_button)
                            time.sleep(random.uniform(0.5, 1.0))
                            
                            # æ¨¡æ‹Ÿäººç±»ç‚¹å‡»è¡Œä¸º
                            if random.random() < 0.3:  # 30%æ¦‚ç‡å…ˆæ‚¬åœ
                                ActionChains(driver).move_to_element(next_button).perform()
                                time.sleep(random.uniform(0.2, 0.5))
                            
                            # ç‚¹å‡»ä¸‹ä¸€é¡µæŒ‰é’®
                            ActionChains(driver).click(next_button).perform()
                            
                            # ç­‰å¾…é¡µé¢åŠ è½½
                            wait_time = random.uniform(2.0, 4.0)
                            time.sleep(wait_time)
                            
                            # ç­‰å¾…é¡µé¢ç¨³å®š
                            self._wait_for_page_stable(driver, timeout=10)
                            
                            # éªŒè¯æ˜¯å¦æˆåŠŸè·³è½¬åˆ°ä¸‹ä¸€é¡µ
                            new_url = driver.current_url
                            if new_url != current_url:
                                self.logger.info(f"âœ… æˆåŠŸå¯¼èˆªåˆ°ä¸‹ä¸€é¡µ: {new_url}")
                                return True
                            else:
                                # URLæ²¡å˜ï¼Œå¯èƒ½æ˜¯AJAXåŠ è½½ï¼Œæ£€æŸ¥é¡µé¢å†…å®¹æ˜¯å¦å˜åŒ–
                                time.sleep(1.0)
                                self.logger.info("âœ… é¡µé¢å†…å®¹å·²æ›´æ–°ï¼ˆAJAXåŠ è½½ï¼‰")
                                return True
                    
                except NoSuchElementException:
                    continue
                except Exception as e:
                    self.logger.warning(f"ç‚¹å‡»ä¸‹ä¸€é¡µæŒ‰é’®å¤±è´¥ {selector}: {e}")
                    continue
            
            self.logger.warning("âš ï¸ æœªæ‰¾åˆ°å¯ç”¨çš„ä¸‹ä¸€é¡µæŒ‰é’®")
            return False
            
        except Exception as e:
            self.logger.error(f"âŒ å¯¼èˆªåˆ°ä¸‹ä¸€é¡µå¤±è´¥: {e}")
            return False
    
    def get_current_page_info(self, driver: webdriver.Chrome) -> Dict[str, Any]:
        """
        è·å–å½“å‰é¡µé¢ä¿¡æ¯
        
        Args:
            driver: WebDriverå®ä¾‹
            
        Returns:
            é¡µé¢ä¿¡æ¯å­—å…¸
        """
        try:
            # å°è¯•è·å–é¡µç ä¿¡æ¯
            page_info = {
                'url': driver.current_url,
                'title': driver.title,
                'current_page': 1,
                'total_pages': None,
                'has_next': self.has_next_page(driver)
            }
            
            # å°è¯•ä»URLä¸­æå–é¡µç 
            import re
            from urllib.parse import urlparse, parse_qs
            
            parsed_url = urlparse(driver.current_url)
            query_params = parse_qs(parsed_url.query)
            
            # å¸¸è§çš„é¡µç å‚æ•°
            page_params = ['page', 'p', 'pageNum', 'pageIndex', 'currentPage']
            for param in page_params:
                if param in query_params:
                    try:
                        page_info['current_page'] = int(query_params[param][0])
                        break
                    except (ValueError, IndexError):
                        continue
            
            # å°è¯•ä»é¡µé¢å…ƒç´ ä¸­è·å–é¡µç ä¿¡æ¯
            page_selectors = [
                '.current-page',
                '.active-page',
                '.pagination .active',
                '.page-current',
                '.pager-current'
            ]
            
            for selector in page_selectors:
                try:
                    page_element = driver.find_element(By.CSS_SELECTOR, selector)
                    page_text = page_element.text.strip()
                    if page_text.isdigit():
                        page_info['current_page'] = int(page_text)
                        break
                except:
                    continue
            
            return page_info
            
        except Exception as e:
            self.logger.error(f"è·å–é¡µé¢ä¿¡æ¯å¤±è´¥: {e}")
            return {'current_page': 1, 'has_next': False}