"""
RAGæ•°æ®æŠ½å–å’Œå¯¼å…¥æµæ°´çº¿

å®Œæ•´çš„æ•°æ®å¤„ç†æµæ°´çº¿ï¼Œä»æ•°æ®åº“è¯»å–èŒä½æ•°æ®ï¼Œè¿›è¡ŒRAGå¤„ç†ï¼Œå¹¶å¯¼å…¥åˆ°å‘é‡æ•°æ®åº“
"""

import asyncio
import logging
from datetime import datetime
from typing import Dict, Any, List, Optional, Callable
from pathlib import Path
import json

from .rag_system_coordinator import RAGSystemCoordinator
from ..core.exceptions import RAGSystemError, DataStorageError

logger = logging.getLogger(__name__)


class RAGDataPipeline:
    """RAGæ•°æ®å¤„ç†æµæ°´çº¿"""
    
    def __init__(self, config: Dict, progress_callback: Optional[Callable] = None):
        """
        åˆå§‹åŒ–æ•°æ®æµæ°´çº¿
        
        Args:
            config: ç³»ç»Ÿé…ç½®
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
        """
        self.config = config
        self.progress_callback = progress_callback
        
        # åˆå§‹åŒ–RAGç³»ç»Ÿåè°ƒå™¨
        self.coordinator = RAGSystemCoordinator(config)
        
        # æµæ°´çº¿çŠ¶æ€
        self.pipeline_stats = {
            'start_time': None,
            'end_time': None,
            'total_jobs': 0,
            'processed_jobs': 0,
            'failed_jobs': 0,
            'skipped_jobs': 0,
            'processing_rate': 0.0,
            'current_batch': 0,
            'total_batches': 0,
            'status': 'initialized'
        }
        
        logger.info("RAGæ•°æ®æµæ°´çº¿åˆå§‹åŒ–å®Œæˆ")
    
    async def run_full_pipeline(self, 
                              batch_size: int = 50,
                              max_jobs: int = None,
                              force_reprocess: bool = False,
                              save_progress: bool = True) -> Dict[str, Any]:
        """
        è¿è¡Œå®Œæ•´çš„RAGæ•°æ®æµæ°´çº¿
        
        Args:
            batch_size: æ‰¹å¤„ç†å¤§å°
            max_jobs: æœ€å¤§å¤„ç†èŒä½æ•°é‡
            force_reprocess: æ˜¯å¦å¼ºåˆ¶é‡æ–°å¤„ç†
            save_progress: æ˜¯å¦ä¿å­˜è¿›åº¦
            
        Returns:
            å¤„ç†ç»“æœç»Ÿè®¡
        """
        logger.info("ğŸš€ å¼€å§‹è¿è¡ŒRAGæ•°æ®æµæ°´çº¿")
        
        try:
            # 1. åˆå§‹åŒ–ç³»ç»Ÿ
            await self._initialize_pipeline()
            
            # 2. é¢„å¤„ç†æ£€æŸ¥
            pre_check_result = await self._pre_processing_check()
            if not pre_check_result['can_proceed']:
                raise RAGSystemError(f"é¢„å¤„ç†æ£€æŸ¥å¤±è´¥: {pre_check_result['reason']}")
            
            # 3. æ‰§è¡Œæ•°æ®å¯¼å…¥
            import_result = await self._execute_data_import(
                batch_size=batch_size,
                max_jobs=max_jobs,
                force_reprocess=force_reprocess
            )
            
            # 4. åå¤„ç†éªŒè¯
            validation_result = await self._post_processing_validation()
            
            # 5. ä¿å­˜å¤„ç†ç»“æœ
            if save_progress:
                await self._save_pipeline_results(import_result, validation_result)
            
            # 6. ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
            final_report = self._generate_final_report(import_result, validation_result)
            
            logger.info("âœ… RAGæ•°æ®æµæ°´çº¿æ‰§è¡Œå®Œæˆ")
            return final_report
            
        except Exception as e:
            logger.error(f"âŒ RAGæ•°æ®æµæ°´çº¿æ‰§è¡Œå¤±è´¥: {e}")
            self.pipeline_stats['status'] = 'failed'
            self.pipeline_stats['error'] = str(e)
            raise RAGSystemError(f"æµæ°´çº¿æ‰§è¡Œå¤±è´¥: {e}")
        
        finally:
            # æ¸…ç†èµ„æº
            self._cleanup_pipeline()
    
    async def _initialize_pipeline(self):
        """åˆå§‹åŒ–æµæ°´çº¿"""
        logger.info("ğŸ”§ åˆå§‹åŒ–RAGæµæ°´çº¿...")
        
        self.pipeline_stats['start_time'] = datetime.now()
        self.pipeline_stats['status'] = 'initializing'
        
        # åˆå§‹åŒ–RAGç³»ç»Ÿ
        if not self.coordinator.initialize_system():
            raise RAGSystemError("RAGç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥")
        
        # è·å–åˆå§‹ç»Ÿè®¡
        initial_progress = self.coordinator.get_processing_progress()
        db_stats = initial_progress.get('database_stats', {})
        
        self.pipeline_stats.update({
            'total_jobs': db_stats.get('total', 0),
            'processed_jobs': db_stats.get('processed', 0),
            'status': 'initialized'
        })
        
        logger.info(f"âœ… æµæ°´çº¿åˆå§‹åŒ–å®Œæˆï¼Œæ€»èŒä½æ•°: {self.pipeline_stats['total_jobs']}")
        
        # è°ƒç”¨è¿›åº¦å›è°ƒ
        if self.progress_callback:
            self.progress_callback('initialized', self.pipeline_stats)
    
    async def _pre_processing_check(self) -> Dict[str, Any]:
        """é¢„å¤„ç†æ£€æŸ¥"""
        logger.info("ğŸ” æ‰§è¡Œé¢„å¤„ç†æ£€æŸ¥...")
        
        check_result = {
            'can_proceed': True,
            'reason': '',
            'warnings': [],
            'recommendations': []
        }
        
        try:
            # æ£€æŸ¥æ•°æ®åº“è¿æ¥
            db_stats = self.coordinator.db_reader.get_rag_processing_stats()
            if db_stats.get('total', 0) == 0:
                check_result['can_proceed'] = False
                check_result['reason'] = 'æ•°æ®åº“ä¸­æ²¡æœ‰èŒä½æ•°æ®'
                return check_result
            
            # æ£€æŸ¥å‘é‡æ•°æ®åº“çŠ¶æ€
            vector_stats = self.coordinator.vector_manager.get_collection_stats()
            if vector_stats.get('document_count', 0) > 10000:
                check_result['warnings'].append('å‘é‡æ•°æ®åº“æ–‡æ¡£æ•°é‡è¾ƒå¤šï¼Œå»ºè®®å®šæœŸæ¸…ç†')
            
            # æ£€æŸ¥æœªå¤„ç†èŒä½æ•°é‡
            unprocessed_count = db_stats.get('unprocessed', 0)
            if unprocessed_count == 0:
                check_result['warnings'].append('æ²¡æœ‰æœªå¤„ç†çš„èŒä½ï¼Œå¯èƒ½ä¸éœ€è¦è¿è¡Œæµæ°´çº¿')
            
            # æ£€æŸ¥ç³»ç»Ÿèµ„æº
            import psutil
            memory_percent = psutil.virtual_memory().percent
            if memory_percent > 80:
                check_result['warnings'].append(f'ç³»ç»Ÿå†…å­˜ä½¿ç”¨ç‡è¾ƒé«˜: {memory_percent}%')
                check_result['recommendations'].append('å»ºè®®å‡å°æ‰¹å¤„ç†å¤§å°')
            
            logger.info(f"âœ… é¢„å¤„ç†æ£€æŸ¥å®Œæˆï¼Œå¯ä»¥ç»§ç»­: {check_result['can_proceed']}")
            return check_result
            
        except Exception as e:
            logger.error(f"é¢„å¤„ç†æ£€æŸ¥å¤±è´¥: {e}")
            check_result['can_proceed'] = False
            check_result['reason'] = f'é¢„å¤„ç†æ£€æŸ¥å¼‚å¸¸: {e}'
            return check_result
    
    async def _execute_data_import(self, 
                                 batch_size: int,
                                 max_jobs: int = None,
                                 force_reprocess: bool = False) -> Dict[str, Any]:
        """æ‰§è¡Œæ•°æ®å¯¼å…¥"""
        logger.info("ğŸ“¥ å¼€å§‹æ‰§è¡Œæ•°æ®å¯¼å…¥...")
        
        self.pipeline_stats['status'] = 'processing'
        
        # ä½¿ç”¨åè°ƒå™¨æ‰§è¡Œå¯¼å…¥
        import_result = await self.coordinator.import_database_jobs(
            batch_size=batch_size,
            max_jobs=max_jobs,
            force_reprocess=force_reprocess
        )
        
        # æ›´æ–°æµæ°´çº¿ç»Ÿè®¡
        self.pipeline_stats.update({
            'processed_jobs': import_result.get('total_imported', 0),
            'failed_jobs': import_result.get('total_errors', 0),
            'skipped_jobs': import_result.get('total_skipped', 0),
            'processing_rate': import_result.get('processing_rate', 0.0)
        })
        
        logger.info(f"âœ… æ•°æ®å¯¼å…¥å®Œæˆï¼Œå¤„ç† {import_result.get('total_imported', 0)} ä¸ªèŒä½")
        
        # è°ƒç”¨è¿›åº¦å›è°ƒ
        if self.progress_callback:
            self.progress_callback('processing_complete', self.pipeline_stats)
        
        return import_result
    
    async def _post_processing_validation(self) -> Dict[str, Any]:
        """åå¤„ç†éªŒè¯"""
        logger.info("âœ… æ‰§è¡Œåå¤„ç†éªŒè¯...")
        
        validation_result = {
            'database_consistency': True,
            'vector_store_integrity': True,
            'data_quality_score': 0.0,
            'issues': [],
            'recommendations': []
        }
        
        try:
            # éªŒè¯æ•°æ®åº“ä¸€è‡´æ€§
            db_stats = self.coordinator.db_reader.get_rag_processing_stats()
            vector_stats = self.coordinator.vector_manager.get_collection_stats()
            
            # æ£€æŸ¥å¤„ç†çŠ¶æ€ä¸€è‡´æ€§
            processed_count = db_stats.get('processed', 0)
            vector_count = vector_stats.get('document_count', 0)
            
            if processed_count > 0 and vector_count == 0:
                validation_result['vector_store_integrity'] = False
                validation_result['issues'].append('æ•°æ®åº“æ˜¾ç¤ºå·²å¤„ç†èŒä½ï¼Œä½†å‘é‡æ•°æ®åº“ä¸ºç©º')
            
            # è®¡ç®—æ•°æ®è´¨é‡è¯„åˆ†
            total_jobs = db_stats.get('total', 1)
            success_rate = processed_count / total_jobs
            error_rate = self.pipeline_stats['failed_jobs'] / max(total_jobs, 1)
            
            validation_result['data_quality_score'] = max(0, success_rate - error_rate * 0.5)
            
            # ç”Ÿæˆå»ºè®®
            if validation_result['data_quality_score'] < 0.8:
                validation_result['recommendations'].append('æ•°æ®è´¨é‡è¾ƒä½ï¼Œå»ºè®®æ£€æŸ¥å¤„ç†é€»è¾‘')
            
            if error_rate > 0.1:
                validation_result['recommendations'].append('é”™è¯¯ç‡è¾ƒé«˜ï¼Œå»ºè®®æ£€æŸ¥æ•°æ®æºè´¨é‡')
            
            logger.info(f"âœ… åå¤„ç†éªŒè¯å®Œæˆï¼Œæ•°æ®è´¨é‡è¯„åˆ†: {validation_result['data_quality_score']:.2f}")
            return validation_result
            
        except Exception as e:
            logger.error(f"åå¤„ç†éªŒè¯å¤±è´¥: {e}")
            validation_result['issues'].append(f'éªŒè¯è¿‡ç¨‹å¼‚å¸¸: {e}')
            return validation_result
    
    async def _save_pipeline_results(self, import_result: Dict, validation_result: Dict):
        """ä¿å­˜æµæ°´çº¿ç»“æœ"""
        logger.info("ğŸ’¾ ä¿å­˜æµæ°´çº¿ç»“æœ...")
        
        try:
            # åˆ›å»ºç»“æœç›®å½•
            results_dir = Path('./pipeline_results')
            results_dir.mkdir(exist_ok=True)
            
            # ç”Ÿæˆç»“æœæ–‡ä»¶å
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            result_file = results_dir / f'rag_pipeline_result_{timestamp}.json'
            
            # å‡†å¤‡ç»“æœæ•°æ®
            pipeline_result = {
                'pipeline_info': {
                    'timestamp': timestamp,
                    'config': self.config.get('rag_system', {}),
                    'pipeline_stats': self.pipeline_stats
                },
                'import_result': import_result,
                'validation_result': validation_result,
                'system_status': self.coordinator.get_system_status()
            }
            
            # ä¿å­˜åˆ°æ–‡ä»¶
            with open(result_file, 'w', encoding='utf-8') as f:
                json.dump(pipeline_result, f, ensure_ascii=False, indent=2, default=str)
            
            logger.info(f"âœ… æµæ°´çº¿ç»“æœå·²ä¿å­˜åˆ°: {result_file}")
            
        except Exception as e:
            logger.error(f"ä¿å­˜æµæ°´çº¿ç»“æœå¤±è´¥: {e}")
            raise DataStorageError(f"ä¿å­˜ç»“æœå¤±è´¥: {e}")
    
    def _generate_final_report(self, import_result: Dict, validation_result: Dict) -> Dict[str, Any]:
        """ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š"""
        logger.info("ğŸ“Š ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š...")
        
        # è®¡ç®—æ‰§è¡Œæ—¶é—´
        self.pipeline_stats['end_time'] = datetime.now()
        execution_time = (self.pipeline_stats['end_time'] - self.pipeline_stats['start_time']).total_seconds()
        
        # ç”ŸæˆæŠ¥å‘Š
        final_report = {
            'execution_summary': {
                'status': 'completed',
                'execution_time': execution_time,
                'start_time': self.pipeline_stats['start_time'],
                'end_time': self.pipeline_stats['end_time']
            },
            'processing_statistics': {
                'total_jobs': self.pipeline_stats['total_jobs'],
                'processed_jobs': self.pipeline_stats['processed_jobs'],
                'failed_jobs': self.pipeline_stats['failed_jobs'],
                'skipped_jobs': self.pipeline_stats['skipped_jobs'],
                'success_rate': (self.pipeline_stats['processed_jobs'] / max(self.pipeline_stats['total_jobs'], 1)) * 100,
                'processing_rate': import_result.get('processing_rate', 0.0)
            },
            'data_quality': {
                'quality_score': validation_result.get('data_quality_score', 0.0),
                'database_consistency': validation_result.get('database_consistency', True),
                'vector_store_integrity': validation_result.get('vector_store_integrity', True)
            },
            'recommendations': validation_result.get('recommendations', []),
            'issues': validation_result.get('issues', []),
            'detailed_results': {
                'import_result': import_result,
                'validation_result': validation_result
            }
        }
        
        # æ›´æ–°æµæ°´çº¿çŠ¶æ€
        self.pipeline_stats['status'] = 'completed'
        
        logger.info(f"âœ… æœ€ç»ˆæŠ¥å‘Šç”Ÿæˆå®Œæˆï¼ŒæˆåŠŸç‡: {final_report['processing_statistics']['success_rate']:.1f}%")
        return final_report
    
    def _cleanup_pipeline(self):
        """æ¸…ç†æµæ°´çº¿èµ„æº"""
        try:
            self.coordinator.cleanup_system()
            logger.info("âœ… æµæ°´çº¿èµ„æºæ¸…ç†å®Œæˆ")
        except Exception as e:
            logger.error(f"æµæ°´çº¿èµ„æºæ¸…ç†å¤±è´¥: {e}")
    
    def get_pipeline_status(self) -> Dict[str, Any]:
        """è·å–æµæ°´çº¿çŠ¶æ€"""
        return {
            'pipeline_stats': self.pipeline_stats,
            'system_status': self.coordinator.get_system_status() if hasattr(self, 'coordinator') else {},
            'processing_progress': self.coordinator.get_processing_progress() if hasattr(self, 'coordinator') else {}
        }
    
    async def resume_pipeline(self, batch_size: int = 50) -> Dict[str, Any]:
        """æ¢å¤ä¸­æ–­çš„æµæ°´çº¿"""
        logger.info("ğŸ”„ æ¢å¤ä¸­æ–­çš„RAGæµæ°´çº¿...")
        
        return await self.run_full_pipeline(
            batch_size=batch_size,
            force_reprocess=False,
            save_progress=True
        )
    
    def estimate_processing_time(self, batch_size: int = 50) -> Dict[str, Any]:
        """ä¼°ç®—å¤„ç†æ—¶é—´"""
        try:
            # è·å–æœªå¤„ç†èŒä½æ•°é‡
            db_stats = self.coordinator.db_reader.get_rag_processing_stats()
            unprocessed_count = db_stats.get('unprocessed', 0)
            
            # åŸºäºå†å²å¤„ç†é€Ÿç‡ä¼°ç®—
            historical_rate = self.coordinator.processing_stats.get('processing_rate', 1.0)
            if historical_rate <= 0:
                historical_rate = 1.0  # é»˜è®¤æ¯ç§’1ä¸ªèŒä½
            
            estimated_seconds = unprocessed_count / historical_rate
            estimated_batches = (unprocessed_count + batch_size - 1) // batch_size
            
            return {
                'unprocessed_jobs': unprocessed_count,
                'estimated_time_seconds': estimated_seconds,
                'estimated_time_minutes': estimated_seconds / 60,
                'estimated_batches': estimated_batches,
                'batch_size': batch_size,
                'processing_rate': historical_rate
            }
            
        except Exception as e:
            logger.error(f"ä¼°ç®—å¤„ç†æ—¶é—´å¤±è´¥: {e}")
            return {'error': str(e)}


def create_progress_callback():
    """åˆ›å»ºè¿›åº¦å›è°ƒå‡½æ•°"""
    def progress_callback(stage: str, stats: Dict):
        timestamp = datetime.now().strftime('%H:%M:%S')
        if stage == 'initialized':
            print(f"[{timestamp}] ğŸ”§ æµæ°´çº¿åˆå§‹åŒ–å®Œæˆï¼Œæ€»èŒä½æ•°: {stats.get('total_jobs', 0)}")
        elif stage == 'processing_complete':
            processed = stats.get('processed_jobs', 0)
            failed = stats.get('failed_jobs', 0)
            rate = stats.get('processing_rate', 0)
            print(f"[{timestamp}] âœ… å¤„ç†å®Œæˆ: æˆåŠŸ {processed}, å¤±è´¥ {failed}, é€Ÿç‡ {rate:.2f}/s")
        else:
            print(f"[{timestamp}] ğŸ“Š {stage}: {stats}")
    
    return progress_callback