"""
ç»Ÿä¸€ä¸»æ§åˆ¶å™¨ (MasterController)
åè°ƒæ•´ä¸ªç«¯åˆ°ç«¯æµç¨‹çš„æ‰§è¡Œ
"""

import asyncio
import time
import logging
from datetime import datetime
from typing import Dict, Any, Optional, List
from dataclasses import dataclass

from ..extraction.content_extractor import ContentExtractor
from ..rag.rag_system_coordinator import RAGSystemCoordinator
from ..matcher.generic_resume_matcher import GenericResumeJobMatcher
from .data_bridge import DataBridge
from .job_scheduler import JobScheduler
from .decision_engine import DecisionEngine
from .submission_integration import SubmissionIntegration

logger = logging.getLogger(__name__)

@dataclass
class PipelineConfig:
    """æµæ°´çº¿é…ç½®"""
    search_keywords: List[str]
    search_locations: List[str] = None
    max_jobs_per_keyword: int = 40  # æ€»40ä¸ªèŒä½
    max_pages: int = 2              # æµ‹è¯•2é¡µ
    resume_profile: Dict[str, Any] = None
    decision_criteria: Dict[str, Any] = None
    submission_config: Dict[str, Any] = None

@dataclass
class ExecutionReport:
    """æ‰§è¡ŒæŠ¥å‘Š"""
    pipeline_id: str
    start_time: datetime
    end_time: datetime
    total_execution_time: float
    extraction_result: Dict[str, Any]
    rag_result: Dict[str, Any]
    matching_result: Dict[str, Any]
    decision_result: Dict[str, Any]
    submission_result: Dict[str, Any]
    success: bool
    error_message: Optional[str] = None

class PipelineError(Exception):
    """æµæ°´çº¿æ‰§è¡Œé”™è¯¯"""
    pass

class MasterController:
    """ç»Ÿä¸€ä¸»æ§åˆ¶å™¨ - åè°ƒæ•´ä¸ªç«¯åˆ°ç«¯æµç¨‹"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.pipeline_id = f"pipeline_{int(time.time())}"
        
        # åˆå§‹åŒ–æ‰€æœ‰å­æ¨¡å—
        self.job_extractor = ContentExtractor(config)
        self.rag_coordinator = RAGSystemCoordinator(config)
        
        # åˆå§‹åŒ–å‘é‡ç®¡ç†å™¨ï¼ˆä»RAGåè°ƒå™¨è·å–ï¼‰
        vector_manager = self.rag_coordinator.vector_manager
        
        # ä½¿ç”¨ä¸ batch_rematch_jobs.py ä¸€è‡´çš„åŒ¹é…å™¨é…ç½®
        matcher_config = self._get_consistent_matcher_config(config)
        self.resume_matcher = GenericResumeJobMatcher(vector_manager, matcher_config)
        
        self.decision_engine = DecisionEngine(config)
        self.submission_integration = SubmissionIntegration(config)
        self.data_bridge = DataBridge(config)
        self.job_scheduler = JobScheduler(config)
        
        # æ‰§è¡ŒçŠ¶æ€
        self.current_stage = "initialized"
        self.execution_stats = {
            'total_jobs_processed': 0,
            'successful_operations': 0,
            'failed_operations': 0,
            'stage_timings': {}
        }
    
    def _get_consistent_matcher_config(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """è·å–ä¸ batch_rematch_jobs.py ä¸€è‡´çš„åŒ¹é…å™¨é…ç½® - ç›´æ¥ä¼ é€’å®Œæ•´é…ç½®"""
        logger.info("ğŸ”„ MasterController ä½¿ç”¨ä¸ batch_rematch_jobs.py ç›¸åŒçš„é…ç½®æº")
        # ç›´æ¥ä¼ é€’å®Œæ•´çš„é…ç½®ï¼Œè®© GenericResumeJobMatcher è‡ªå·±è§£æ
        # è¿™æ ·ç¡®ä¿ä¸ batch_rematch_jobs.py ä½¿ç”¨å®Œå…¨ç›¸åŒçš„é…ç½®é€»è¾‘
        return config
    
    async def run_full_pipeline(self, pipeline_config: PipelineConfig) -> ExecutionReport:
        """æ‰§è¡Œå®Œæ•´çš„ç«¯åˆ°ç«¯æµæ°´çº¿"""
        start_time = datetime.now()
        logger.info(f"å¼€å§‹æ‰§è¡Œæµæ°´çº¿ {self.pipeline_id}")
        
        try:
            # é˜¶æ®µ1: èŒä½æå–
            logger.info("å¼€å§‹é˜¶æ®µ1: èŒä½æå–")
            self.current_stage = "job_extraction"
            extraction_result = await self._execute_job_extraction(pipeline_config)
            
            if not extraction_result['success']:
                raise PipelineError(f"èŒä½æå–å¤±è´¥: {extraction_result.get('error', 'Unknown error')}")
            
            # é˜¶æ®µ2: RAGå¤„ç†
            logger.info("å¼€å§‹é˜¶æ®µ2: RAGå¤„ç†")
            self.current_stage = "rag_processing"
            rag_result = await self._execute_rag_processing_from_database()
            
            if not rag_result['success']:
                raise PipelineError(f"RAGå¤„ç†å¤±è´¥: {rag_result.get('error', 'Unknown error')}")
            
            # é˜¶æ®µ3: ç®€å†åŒ¹é…å¹¶ä¿å­˜ç»“æœåˆ°æ•°æ®åº“
            logger.info("å¼€å§‹é˜¶æ®µ3: ç®€å†åŒ¹é…")
            self.current_stage = "resume_matching"
            matching_result = await self._execute_resume_matching_with_database_save(pipeline_config.resume_profile)
            
            if not matching_result['success']:
                raise PipelineError(f"ç®€å†åŒ¹é…å¤±è´¥: {matching_result.get('error', 'Unknown error')}")
            
            # é˜¶æ®µ4: ç®€å†æŠ•é€’ - æ³¨é‡Šæ‰ç”¨äºæµ‹è¯•
            logger.info("å¼€å§‹é˜¶æ®µ4: ç®€å†æŠ•é€’")
            self.current_stage = "resume_submission"
            submission_result = self._execute_resume_submission(pipeline_config.submission_config)
            
            # æ¨¡æ‹ŸæŠ•é€’ç»“æœ
            # submission_result = {
            #     'success': True,
            #     'total_processed': 0,
            #     'successful_submissions': 0,
            #     'failed_submissions': 0,
            #     'skipped_submissions': 0,
            #     'submission_details': [],
            #     'processing_time': 0
            # }
            
            # åˆ›å»ºå†³ç­–ç»“æœç”¨äºå…¼å®¹æ€§
            decision_result = {'success': True, 'recommended_submissions': 0}
            
            # ç”Ÿæˆæ‰§è¡ŒæŠ¥å‘Š
            end_time = datetime.now()
            execution_time = (end_time - start_time).total_seconds()
            
            # ä½¿ç”¨å®é™…çš„ç»“æœ
            
            report = ExecutionReport(
                pipeline_id=self.pipeline_id,
                start_time=start_time,
                end_time=end_time,
                total_execution_time=execution_time,
                extraction_result=extraction_result,
                rag_result=rag_result,
                matching_result=matching_result,
                decision_result=decision_result,
                submission_result=submission_result,
                success=True
            )
            
            logger.info(f"æµæ°´çº¿ {self.pipeline_id} æ‰§è¡Œå®Œæˆï¼Œè€—æ—¶ {execution_time:.2f} ç§’")
            return report
            
        except Exception as e:
            logger.error(f"æµæ°´çº¿æ‰§è¡Œå¤±è´¥: {e}")
            end_time = datetime.now()
            execution_time = (end_time - start_time).total_seconds()
            
            return ExecutionReport(
                pipeline_id=self.pipeline_id,
                start_time=start_time,
                end_time=end_time,
                total_execution_time=execution_time,
                extraction_result=getattr(self, '_last_extraction_result', {}),
                rag_result=getattr(self, '_last_rag_result', {}),
                matching_result=getattr(self, '_last_matching_result', {}),
                decision_result=getattr(self, '_last_decision_result', {}),
                submission_result=getattr(self, '_last_submission_result', {}),
                success=False,
                error_message=str(e)
            )
    
    async def _execute_job_extraction(self, pipeline_config: PipelineConfig) -> Dict[str, Any]:
        """æ‰§è¡ŒèŒä½æå–é˜¶æ®µï¼ˆå¼‚æ­¥æ¥å£ï¼Œå†…éƒ¨è°ƒç”¨åŒæ­¥æ–¹æ³•ï¼‰"""
        return self._execute_job_extraction_sync(pipeline_config)
    
    def _execute_job_extraction_sync(self, pipeline_config: PipelineConfig) -> Dict[str, Any]:
        """æ‰§è¡ŒèŒä½æå–é˜¶æ®µï¼ˆåŒæ­¥æ–¹æ³•ï¼‰"""
        stage_start = time.time()
        
        try:
            combined_results = []
            
            # é¡ºåºæ‰§è¡Œæ¯ä¸ªå…³é”®è¯çš„æå–ä»»åŠ¡ï¼ˆé¿å…Seleniumå¹¶å‘é—®é¢˜ï¼‰
            for i, keyword in enumerate(pipeline_config.search_keywords, 1):
                logger.info(f"å¼€å§‹å¤„ç†å…³é”®è¯ {i}/{len(pipeline_config.search_keywords)}: '{keyword}'")
                
                try:
                    # åŒæ­¥è°ƒç”¨ContentExtractor
                    keyword_results = self.job_extractor.extract_from_keyword(
                        keyword=keyword,
                        max_results=pipeline_config.max_jobs_per_keyword,
                        save_results=True,
                        extract_details=True,
                        max_pages=pipeline_config.max_pages
                    )
                    
                    logger.info(f"å…³é”®è¯ '{keyword}' æå–å®Œæˆ: {len(keyword_results)} ä¸ªèŒä½")
                    combined_results.extend(keyword_results)
                    
                    # å…³é”®è¯ä¹‹é—´æ·»åŠ çŸ­æš‚é—´éš”ï¼Œé¿å…è¿‡äºé¢‘ç¹çš„è¯·æ±‚
                    if i < len(pipeline_config.search_keywords):
                        logger.info(f"ç­‰å¾… 10 ç§’åå¤„ç†ä¸‹ä¸€ä¸ªå…³é”®è¯...")
                        time.sleep(10)
                        
                except Exception as e:
                    logger.error(f"å¤„ç†å…³é”®è¯ '{keyword}' å¤±è´¥: {e}")
                    # ç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ªå…³é”®è¯ï¼Œä¸ä¸­æ–­æ•´ä¸ªæµç¨‹
                    continue
            
            stage_time = time.time() - stage_start
            self.execution_stats['stage_timings']['extraction'] = stage_time
            
            result = {
                'success': True,
                'total_extracted': len(combined_results),
                'jobs': combined_results,
                'extraction_time': stage_time,
                'keywords_processed': len(pipeline_config.search_keywords)
            }
            
            self._last_extraction_result = result
            return result
            
        except Exception as e:
            logger.error(f"èŒä½æå–å¤±è´¥: {e}")
            result = {'success': False, 'error': str(e)}
            self._last_extraction_result = result
            return result
    
    def _extract_jobs_for_keyword_sync(self, keyword: str, max_results: int, max_pages: int) -> List[Dict]:
        """ä¸ºå•ä¸ªå…³é”®è¯æå–èŒä½ï¼ˆåŒæ­¥æ–¹æ³•ï¼‰"""
        logger.info(f"å¼€å§‹ä¸ºå…³é”®è¯ '{keyword}' æå–èŒä½ï¼Œæœ€å¤§ç»“æœæ•°: {max_results}, æœ€å¤§é¡µæ•°: {max_pages}")
        
        try:
            # è°ƒç”¨çœŸå®çš„ContentExtractorï¼ˆåŒæ­¥æ–¹æ³•ï¼‰
            extracted_jobs = self.job_extractor.extract_from_keyword(
                keyword=keyword,
                max_results=max_results,
                save_results=True,
                extract_details=True,  # æå–è¯¦æƒ…
                max_pages=max_pages
            )
            
            logger.info(f"ContentExtractor ä¸ºå…³é”®è¯ '{keyword}' æå–äº† {len(extracted_jobs)} ä¸ªèŒä½")
            
            # æ‰“å°å‰5æ¡æå–çš„èŒä½è¯¦æƒ…
            for i, job in enumerate(extracted_jobs[:5]):
                logger.info(f"æå–èŒä½ {i+1}: æ ‡é¢˜={job.get('title', 'N/A')}, å…¬å¸={job.get('company', 'N/A')}, åœ°ç‚¹={job.get('location', 'N/A')}")
            
            return extracted_jobs
            
        except Exception as e:
            logger.error(f"ContentExtractor æå–èŒä½å¤±è´¥: {e}")
            # å¦‚æœçœŸå®æå–å¤±è´¥ï¼Œè¿”å›ç©ºåˆ—è¡¨è€Œä¸æ˜¯æ¨¡æ‹Ÿæ•°æ®
            logger.warning("æå–å¤±è´¥ï¼Œè¿”å›ç©ºç»“æœ")
            return []
    
    def _merge_extraction_results(self, results: List) -> List[Dict]:
        """åˆå¹¶æå–ç»“æœ"""
        combined = []
        for result in results:
            if isinstance(result, Exception):
                logger.warning(f"æå–ä»»åŠ¡å¤±è´¥: {result}")
                continue
            if isinstance(result, list):
                combined.extend(result)
        return combined
    
    async def _execute_rag_processing_from_database(self) -> Dict[str, Any]:
        """æ‰§è¡ŒRAGå¤„ç†é˜¶æ®µ - ä»æ•°æ®åº“è¯»å–æœªå¤„ç†çš„èŒä½"""
        stage_start = time.time()
        
        try:
            # åˆå§‹åŒ–RAGç³»ç»Ÿï¼ˆå¦‚æœå°šæœªåˆå§‹åŒ–ï¼‰
            if not self.rag_coordinator.is_initialized:
                logger.info("åˆå§‹åŒ–RAGç³»ç»Ÿ...")
                init_success = self.rag_coordinator.initialize_system()
                if not init_success:
                    logger.warning("RAGç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥ï¼Œä½†ç»§ç»­å¤„ç†...")
            
            # ç›´æ¥ä»æ•°æ®åº“è¯»å–æœªå¤„ç†çš„èŒä½è¿›è¡ŒRAGå¤„ç†
            rag_result = await self.rag_coordinator.import_database_jobs(
                batch_size=50,
                force_reprocess=False
            )
            
            stage_time = time.time() - stage_start
            self.execution_stats['stage_timings']['rag_processing'] = stage_time
            
            result = {
                'success': True,
                'processed_count': rag_result.get('total_imported', 0),
                'processing_time': stage_time,
                'success_rate': rag_result.get('success_rate', 0),
                'vector_db_stats': rag_result.get('vector_db_stats', {})
            }
            
            self._last_rag_result = result
            return result
            
        except Exception as e:
            logger.error(f"RAGå¤„ç†å¤±è´¥: {e}")
            result = {'success': False, 'error': str(e)}
            self._last_rag_result = result
            return result
    
    async def _execute_rag_processing(self, extraction_result: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡ŒRAGå¤„ç†é˜¶æ®µï¼ˆä¿ç•™åŸæ–¹æ³•ç”¨äºå…¼å®¹æ€§ï¼‰"""
        stage_start = time.time()
        
        try:
            # åˆå§‹åŒ–RAGç³»ç»Ÿï¼ˆå¦‚æœå°šæœªåˆå§‹åŒ–ï¼‰
            if not self.rag_coordinator.is_initialized:
                logger.info("åˆå§‹åŒ–RAGç³»ç»Ÿ...")
                init_success = self.rag_coordinator.initialize_system()
                if not init_success:
                    logger.warning("RAGç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥ï¼Œä½†ç»§ç»­å¤„ç†...")
            
            # å°†æå–çš„èŒä½æ•°æ®è½¬æ¢ä¸ºRAGè¾“å…¥æ ¼å¼
            rag_input = self.data_bridge.transform_extraction_to_rag(extraction_result)
            
            # æ‰¹é‡å¤„ç†èŒä½æ•°æ®
            rag_result = await self.rag_coordinator.import_database_jobs(
                batch_size=50,
                force_reprocess=False
            )
            
            stage_time = time.time() - stage_start
            self.execution_stats['stage_timings']['rag_processing'] = stage_time
            
            result = {
                'success': True,
                'processed_count': rag_result.get('total_imported', 0),
                'processing_time': stage_time,
                'success_rate': rag_result.get('success_rate', 0),
                'vector_db_stats': rag_result.get('vector_db_stats', {})
            }
            
            self._last_rag_result = result
            return result
            
        except Exception as e:
            logger.error(f"RAGå¤„ç†å¤±è´¥: {e}")
            result = {'success': False, 'error': str(e)}
            self._last_rag_result = result
            return result
    
    async def _execute_resume_matching_with_database_save(self, resume_profile: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œç®€å†åŒ¹é…é˜¶æ®µå¹¶ä¿å­˜ç»“æœåˆ°æ•°æ®åº“"""
        stage_start = time.time()
        
        try:
            # å°†å­—å…¸è½¬æ¢ä¸ºGenericResumeProfileå¯¹è±¡
            from ..matcher.generic_resume_models import GenericResumeProfile
            if isinstance(resume_profile, dict):
                # è½¬æ¢ç®€åŒ–æ ¼å¼åˆ°å®Œæ•´æ ¼å¼
                resume_profile_obj = self._convert_simple_resume_profile(resume_profile)
            else:
                resume_profile_obj = resume_profile
            
            # æ‰§è¡Œç®€å†åŒ¹é… - ä½¿ç”¨æ›´å¤§çš„ top_k å€¼ä»¥åŒ¹é…æ›´å¤šèŒä½
            # ä¸ batch_rematch_jobs.py ä¿æŒä¸€è‡´ï¼Œå¤„ç†æ‰€æœ‰å¯èƒ½çš„åŒ¹é…
            matching_result = await self.resume_matcher.find_matching_jobs(
                resume_profile=resume_profile_obj,
                top_k=1000  # å¢åŠ åˆ°1000ï¼Œç¡®ä¿èƒ½å¤„ç†æ‰€æœ‰èŒä½
            )
            
            # ä¿å­˜åŒ¹é…ç»“æœåˆ°æ•°æ®åº“
            saved_count = await self._save_matching_results_to_database(matching_result, resume_profile_obj)
            
            stage_time = time.time() - stage_start
            self.execution_stats['stage_timings']['matching'] = stage_time
            
            result = {
                'success': True,
                'total_matches': matching_result.matching_summary.total_matches,
                'high_priority': matching_result.matching_summary.high_priority,
                'medium_priority': matching_result.matching_summary.medium_priority,
                'low_priority': matching_result.matching_summary.low_priority,
                'matches': matching_result.matches,
                'saved_to_database': saved_count,
                'processing_time': stage_time
            }
            
            self._last_matching_result = result
            return result
            
        except Exception as e:
            logger.error(f"ç®€å†åŒ¹é…å¤±è´¥: {e}")
            result = {'success': False, 'error': str(e)}
            self._last_matching_result = result
            return result
    
    async def _execute_resume_matching(self, rag_result: Dict[str, Any], resume_profile: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œç®€å†åŒ¹é…é˜¶æ®µï¼ˆä¿ç•™åŸæ–¹æ³•ç”¨äºå…¼å®¹æ€§ï¼‰"""
        stage_start = time.time()
        
        try:
            # å°†å­—å…¸è½¬æ¢ä¸ºGenericResumeProfileå¯¹è±¡
            from ..matcher.generic_resume_models import GenericResumeProfile
            if isinstance(resume_profile, dict):
                # è½¬æ¢ç®€åŒ–æ ¼å¼åˆ°å®Œæ•´æ ¼å¼
                resume_profile_obj = self._convert_simple_resume_profile(resume_profile)
            else:
                resume_profile_obj = resume_profile
            
            # æ‰§è¡Œç®€å†åŒ¹é…
            matching_result = await self.resume_matcher.find_matching_jobs(
                resume_profile=resume_profile_obj,
                top_k=50
            )
            
            stage_time = time.time() - stage_start
            self.execution_stats['stage_timings']['matching'] = stage_time
            
            result = {
                'success': True,
                'total_matches': matching_result.matching_summary.total_matches,
                'high_priority': matching_result.matching_summary.high_priority,
                'medium_priority': matching_result.matching_summary.medium_priority,
                'low_priority': matching_result.matching_summary.low_priority,
                'matches': matching_result.matches,
                'processing_time': stage_time
            }
            
            self._last_matching_result = result
            return result
            
        except Exception as e:
            logger.error(f"ç®€å†åŒ¹é…å¤±è´¥: {e}")
            result = {'success': False, 'error': str(e)}
            self._last_matching_result = result
            return result
    
    async def _save_matching_results_to_database(self, matching_result, resume_profile) -> int:
        """ä¿å­˜åŒ¹é…ç»“æœåˆ°æ•°æ®åº“"""
        try:
            from ..database.operations import DatabaseManager
            
            # è·å–æ•°æ®åº“ç®¡ç†å™¨
            db_manager = DatabaseManager(self.config.get('database_path', 'data/jobs.db'))
            
            # å‡†å¤‡åŒ¹é…ç»“æœæ•°æ®
            match_records = []
            resume_profile_id = getattr(resume_profile, 'profile_id', 'default')
            
            for match in matching_result.matches:
                # è·å–ä¼˜å…ˆçº§ï¼Œå¦‚æœæ²¡æœ‰åˆ™æ ¹æ®åˆ†æ•°è®¡ç®—
                priority_level = getattr(match, 'priority_level', None)
                if not priority_level:
                    if match.overall_score >= 0.8:
                        priority_level = 'high'
                    elif match.overall_score >= 0.6:
                        priority_level = 'medium'
                    else:
                        priority_level = 'low'
                
                match_data = {
                    'job_id': match.job_id,
                    'resume_profile_id': resume_profile_id,
                    'match_score': match.overall_score,
                    'priority_level': priority_level,
                    'semantic_score': match.dimension_scores.get('semantic_similarity', 0) if hasattr(match, 'dimension_scores') else None,
                    'skill_match_score': match.dimension_scores.get('skills_match', 0) if hasattr(match, 'dimension_scores') else None,
                    'experience_match_score': match.dimension_scores.get('experience_match', 0) if hasattr(match, 'dimension_scores') else None,
                    'location_match_score': match.dimension_scores.get('industry_match', 0) if hasattr(match, 'dimension_scores') else None,
                    'salary_match_score': match.dimension_scores.get('salary_match', 0) if hasattr(match, 'dimension_scores') else None,
                    'match_details': str(match.dimension_scores) if hasattr(match, 'dimension_scores') else '{}',
                    'match_reasons': f"MasterControlleråŒ¹é…: {match.job_title} at {match.company}" if hasattr(match, 'job_title') and hasattr(match, 'company') else f"èŒä½ID: {match.job_id}"
                }
                match_records.append(match_data)
            
            # æ‰¹é‡ä¿å­˜åˆ°æ•°æ®åº“
            saved_count = db_manager.batch_save_resume_matches(match_records)
            
            logger.info(f"ä¿å­˜åŒ¹é…ç»“æœåˆ°æ•°æ®åº“: {saved_count}/{len(match_records)} æˆåŠŸ")
            return saved_count
            
        except Exception as e:
            logger.error(f"ä¿å­˜åŒ¹é…ç»“æœåˆ°æ•°æ®åº“å¤±è´¥: {e}")
            return 0
    
    def _convert_simple_resume_profile(self, simple_profile: Dict[str, Any]):
        """å°†ç®€åŒ–çš„ç®€å†æ ¼å¼è½¬æ¢ä¸ºGenericResumeProfileå¯¹è±¡"""
        from ..matcher.generic_resume_models import GenericResumeProfile, SkillCategory
        
        # å¦‚æœå·²ç»æ˜¯å®Œæ•´æ ¼å¼ï¼Œç›´æ¥ä½¿ç”¨from_dict
        if 'skill_categories' in simple_profile:
            return GenericResumeProfile.from_dict(simple_profile)
        
        # åˆ›å»ºåŸºæœ¬çš„ç®€å†å¯¹è±¡ï¼ˆç®€åŒ–æ ¼å¼ï¼‰
        profile = GenericResumeProfile(
            name=simple_profile.get('name', ''),
            total_experience_years=simple_profile.get('experience_years', 0),
            current_position=simple_profile.get('current_position', '')
        )
        
        # å¤„ç†æŠ€èƒ½ - å¦‚æœæ˜¯ç®€å•åˆ—è¡¨ï¼Œè½¬æ¢ä¸ºæŠ€èƒ½åˆ†ç±»
        skills = simple_profile.get('skills', [])
        if skills:
            # åˆ›å»ºä¸€ä¸ªé€šç”¨æŠ€èƒ½åˆ†ç±»
            skill_category = SkillCategory(
                category_name='core_skills',
                skills=skills,
                proficiency_level='intermediate'
            )
            profile.skill_categories.append(skill_category)
        
        # å¤„ç†æ•™è‚²èƒŒæ™¯
        education = simple_profile.get('education', '')
        if education:
            from ..matcher.generic_resume_models import Education
            edu = Education(
                degree=education,
                major='',
                university='',
                graduation_year=''
            )
            profile.education.append(edu)
        
        return profile
    
    async def _execute_intelligent_decision(self, matching_result: Dict[str, Any], decision_criteria: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œæ™ºèƒ½å†³ç­–é˜¶æ®µ"""
        stage_start = time.time()
        
        try:
            # åˆ¶å®šæŠ•é€’å†³ç­–
            decision_result = await self.decision_engine.make_submission_decisions(
                matching_result=matching_result,
                decision_criteria=decision_criteria or {}
            )
            
            stage_time = time.time() - stage_start
            self.execution_stats['stage_timings']['decision'] = stage_time
            
            result = {
                'success': True,
                'total_evaluated': decision_result.get('total_evaluated', 0),
                'recommended_submissions': decision_result.get('recommended_submissions', 0),
                'decisions': decision_result.get('decisions', []),
                'processing_time': stage_time
            }
            
            self._last_decision_result = result
            return result
            
        except Exception as e:
            logger.error(f"æ™ºèƒ½å†³ç­–å¤±è´¥: {e}")
            result = {'success': False, 'error': str(e)}
            self._last_decision_result = result
            return result
    
    def _execute_resume_submission(self, submission_config: Dict[str, Any] = None) -> Dict[str, Any]:
        """æ‰§è¡Œç®€å†æŠ•é€’é˜¶æ®µï¼ˆåŒæ­¥æ–¹æ³•ï¼Œå› ä¸ºSeleniumé™åˆ¶ï¼‰"""
        stage_start = time.time()
        
        try:
            logger.info("å¼€å§‹æ‰§è¡Œç®€å†æŠ•é€’...")
            
            # ä½¿ç”¨æ–°çš„æŠ•é€’é›†æˆæ¨¡å—ï¼ˆåŒæ­¥è°ƒç”¨ï¼‰
            submission_result = self.submission_integration.execute_submission_pipeline_sync(
                config=submission_config or {}
            )
            
            stage_time = time.time() - stage_start
            self.execution_stats['stage_timings']['resume_submission'] = stage_time
            
            result = {
                'success': submission_result.get('success', False),
                'total_processed': submission_result.get('total_processed', 0),
                'successful_submissions': submission_result.get('successful_submissions', 0),
                'failed_submissions': submission_result.get('failed_submissions', 0),
                'skipped_submissions': submission_result.get('skipped_submissions', 0),
                'submission_details': submission_result.get('submission_details', []),
                'processing_time': stage_time,
                'error_message': submission_result.get('error_message')
            }
            
            self._last_submission_result = result
            logger.info(f"ç®€å†æŠ•é€’å®Œæˆ: æˆåŠŸ {result['successful_submissions']}, å¤±è´¥ {result['failed_submissions']}, è·³è¿‡ {result['skipped_submissions']}")
            return result
            
        except Exception as e:
            logger.error(f"ç®€å†æŠ•é€’å¤±è´¥: {e}")
            result = {'success': False, 'error': str(e)}
            self._last_submission_result = result
            return result

    async def _execute_auto_submission(self, decision_result: Dict[str, Any], submission_config: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œè‡ªåŠ¨æŠ•é€’é˜¶æ®µï¼ˆä¿ç•™ç”¨äºå…¼å®¹æ€§ï¼‰"""
        stage_start = time.time()
        
        try:
            # æ‰§è¡Œè‡ªåŠ¨æŠ•é€’
            submission_result = await self.auto_submitter.submit_applications(
                decision_result=decision_result,
                submission_config=submission_config or {}
            )
            
            stage_time = time.time() - stage_start
            self.execution_stats['stage_timings']['submission'] = stage_time
            
            result = {
                'success': True,
                'total_attempts': submission_result.get('total_attempts', 0),
                'successful_submissions': submission_result.get('successful_submissions', 0),
                'failed_submissions': submission_result.get('failed_submissions', 0),
                'submission_details': submission_result.get('submission_details', []),
                'processing_time': stage_time
            }
            
            self._last_submission_result = result
            return result
            
        except Exception as e:
            logger.error(f"è‡ªåŠ¨æŠ•é€’å¤±è´¥: {e}")
            result = {'success': False, 'error': str(e)}
            self._last_submission_result = result
            return result
    
    def get_current_status(self) -> Dict[str, Any]:
        """è·å–å½“å‰æ‰§è¡ŒçŠ¶æ€"""
        return {
            'pipeline_id': self.pipeline_id,
            'current_stage': self.current_stage,
            'execution_stats': self.execution_stats,
            'stage_timings': self.execution_stats.get('stage_timings', {})
        }
    
    def generate_execution_summary(self, report: ExecutionReport) -> Dict[str, Any]:
        """ç”Ÿæˆæ‰§è¡Œæ‘˜è¦"""
        return {
            'pipeline_id': report.pipeline_id,
            'execution_time': report.total_execution_time,
            'success': report.success,
            'error_message': report.error_message,
            'stage_results': {
                'extraction': {
                    'jobs_extracted': report.extraction_result.get('total_extracted', 0),
                    'success': report.extraction_result.get('success', False)
                },
                'rag_processing': {
                    'jobs_processed': report.rag_result.get('processed_count', 0),
                    'success': report.rag_result.get('success', False)
                },
                'matching': {
                    'matches_found': report.matching_result.get('total_matches', 0),
                    'success': report.matching_result.get('success', False)
                },
                'decision': {
                    'recommended_submissions': report.decision_result.get('recommended_submissions', 0),
                    'success': report.decision_result.get('success', False)
                },
                'submission': {
                    'successful_submissions': report.submission_result.get('successful_submissions', 0),
                    'success': report.submission_result.get('success', False)
                }
            },
            'recommendations': self._generate_optimization_recommendations(report)
        }
    
    def _generate_optimization_recommendations(self, report: ExecutionReport) -> List[str]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []
        
        # åŸºäºæ‰§è¡Œæ—¶é—´çš„å»ºè®®
        if report.total_execution_time > 1800:  # 30åˆ†é’Ÿ
            recommendations.append("è€ƒè™‘å¢åŠ å¹¶å‘å¤„ç†æ•°é‡ä»¥æé«˜æ•´ä½“æ‰§è¡Œæ•ˆç‡")
        
        # åŸºäºå„é˜¶æ®µæ€§èƒ½çš„å»ºè®®
        if report.rag_result.get('processing_time', 0) > 600:  # 10åˆ†é’Ÿ
            recommendations.append("RAGå¤„ç†æ—¶é—´è¾ƒé•¿ï¼Œå»ºè®®ä¼˜åŒ–æ‰¹å¤„ç†å¤§å°æˆ–å¢åŠ ç¼“å­˜")
        
        if report.matching_result.get('total_matches', 0) < 10:
            recommendations.append("åŒ¹é…ç»“æœè¾ƒå°‘ï¼Œå»ºè®®è°ƒæ•´åŒ¹é…é˜ˆå€¼æˆ–æ‰©å¤§æœç´¢èŒƒå›´")
        
        if report.submission_result.get('successful_submissions', 0) == 0:
            recommendations.append("æŠ•é€’æˆåŠŸç‡ä¸º0ï¼Œå»ºè®®æ£€æŸ¥æŠ•é€’é…ç½®å’Œç½‘ç»œè¿æ¥")
        
        return recommendations
    
    def reset_rag_processing_status(self, job_ids: List[str] = None) -> int:
        """
        é‡ç½®RAGå¤„ç†çŠ¶æ€ï¼Œæ”¯æŒé‡æ–°å¤„ç†
        
        Args:
            job_ids: è¦é‡ç½®çš„èŒä½IDåˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºé‡ç½®æ‰€æœ‰
            
        Returns:
            é‡ç½®çš„è®°å½•æ•°
        """
        try:
            from ..database.operations import DatabaseManager
            db_manager = DatabaseManager(self.config.get('database_path', 'data/jobs.db'))
            
            reset_count = db_manager.reset_rag_processing_status(job_ids)
            logger.info(f"é‡ç½®RAGå¤„ç†çŠ¶æ€: {reset_count} ä¸ªèŒä½")
            return reset_count
            
        except Exception as e:
            logger.error(f"é‡ç½®RAGå¤„ç†çŠ¶æ€å¤±è´¥: {e}")
            return 0
    
    def get_processing_statistics(self) -> Dict[str, Any]:
        """
        è·å–å¤„ç†ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        try:
            from ..database.operations import DatabaseManager
            db_manager = DatabaseManager(self.config.get('database_path', 'data/jobs.db'))
            
            # è·å–RAGå¤„ç†ç»Ÿè®¡
            rag_stats = db_manager.get_rag_processing_stats()
            
            # è·å–åŒ¹é…ç»“æœç»Ÿè®¡
            match_stats = db_manager.get_match_statistics()
            
            # è·å–åŸºæœ¬ç»Ÿè®¡
            basic_stats = db_manager.get_statistics()
            
            return {
                'basic_stats': basic_stats,
                'rag_processing': rag_stats,
                'matching_results': match_stats,
                'pipeline_id': self.pipeline_id,
                'current_stage': self.current_stage
            }
            
        except Exception as e:
            logger.error(f"è·å–å¤„ç†ç»Ÿè®¡å¤±è´¥: {e}")
            return {}
    
    async def run_stage_only(self, stage: str, pipeline_config: PipelineConfig) -> Dict[str, Any]:
        """
        åªè¿è¡ŒæŒ‡å®šé˜¶æ®µ
        
        Args:
            stage: é˜¶æ®µåç§° ('rag_processing', 'resume_matching', 'decision', 'submission')
            pipeline_config: æµæ°´çº¿é…ç½®
            
        Returns:
            é˜¶æ®µæ‰§è¡Œç»“æœ
        """
        logger.info(f"å¼€å§‹æ‰§è¡Œå•ç‹¬é˜¶æ®µ: {stage}")
        
        try:
            if stage == 'rag_processing':
                return await self._execute_rag_processing_from_database()
            elif stage == 'resume_matching':
                return await self._execute_resume_matching_with_database_save(pipeline_config.resume_profile)
            elif stage == 'resume_submission':
                return self._execute_resume_submission(pipeline_config.submission_config)
            elif stage == 'decision':
                # éœ€è¦å…ˆè·å–åŒ¹é…ç»“æœ
                logger.warning("å†³ç­–é˜¶æ®µéœ€è¦åŒ¹é…ç»“æœï¼Œå»ºè®®å…ˆè¿è¡ŒåŒ¹é…é˜¶æ®µ")
                return {'success': False, 'error': 'éœ€è¦å…ˆè¿è¡ŒåŒ¹é…é˜¶æ®µ'}
            elif stage == 'submission':
                # å…¼å®¹æ—§çš„æŠ•é€’é˜¶æ®µåç§°
                return self._execute_resume_submission(pipeline_config.submission_config)
            else:
                return {'success': False, 'error': f'æœªçŸ¥é˜¶æ®µ: {stage}'}
                
        except Exception as e:
            logger.error(f"æ‰§è¡Œé˜¶æ®µ {stage} å¤±è´¥: {e}")
            return {'success': False, 'error': str(e)}
    
    def validate_pipeline_readiness(self) -> Dict[str, Any]:
        """
        éªŒè¯æµæ°´çº¿å‡†å¤‡çŠ¶æ€
        
        Returns:
            éªŒè¯ç»“æœ
        """
        try:
            from ..database.operations import DatabaseManager
            db_manager = DatabaseManager(self.config.get('database_path', 'data/jobs.db'))
            
            # æ£€æŸ¥æ•°æ®åº“ä¸­çš„æ•°æ®çŠ¶æ€
            stats = db_manager.get_statistics()
            rag_stats = db_manager.get_rag_processing_stats()
            
            readiness = {
                'database_ready': stats.get('total', 0) > 0,
                'jobs_available': stats.get('total', 0),
                'rag_ready': rag_stats.get('processed', 0) > 0,
                'rag_processed_count': rag_stats.get('processed', 0),
                'unprocessed_count': rag_stats.get('unprocessed', 0),
                'recommendations': []
            }
            
            # ç”Ÿæˆå»ºè®®
            if not readiness['database_ready']:
                readiness['recommendations'].append("éœ€è¦å…ˆè¿è¡ŒèŒä½æå–é˜¶æ®µï¼Œæ•°æ®åº“ä¸­æ²¡æœ‰èŒä½æ•°æ®")
            
            if readiness['jobs_available'] > 0 and rag_stats.get('unprocessed', 0) > 0:
                readiness['recommendations'].append(f"æœ‰ {rag_stats.get('unprocessed', 0)} ä¸ªèŒä½éœ€è¦RAGå¤„ç†")
            
            if rag_stats.get('processed', 0) > 0:
                readiness['recommendations'].append(f"å¯ä»¥å¼€å§‹ç®€å†åŒ¹é…ï¼Œå·²æœ‰ {rag_stats.get('processed', 0)} ä¸ªèŒä½å®ŒæˆRAGå¤„ç†")
            
            return readiness
            
        except Exception as e:
            logger.error(f"éªŒè¯æµæ°´çº¿å‡†å¤‡çŠ¶æ€å¤±è´¥: {e}")
            return {'database_ready': False, 'error': str(e)}