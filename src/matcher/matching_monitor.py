#!/usr/bin/env python3
"""
åŒ¹é…ç›‘æ§å’Œå®šæœŸæ£€æŸ¥ç³»ç»Ÿ
ç”¨äºç›‘æ§åŒ¹é…è´¨é‡å’Œæ‰§è¡Œå®šæœŸåŒ¹é…æ£€æŸ¥
"""

import asyncio
import time
import schedule
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
import sqlite3
import json

from ..utils.logger import get_logger
from ..database.operations import DatabaseManager
from .generic_resume_matcher import GenericResumeJobMatcher
from .generic_resume_models import GenericResumeProfile
from ..rag.vector_manager import ChromaDBManager


@dataclass
class MatchingStats:
    """åŒ¹é…ç»Ÿè®¡ä¿¡æ¯"""
    total_jobs: int = 0
    total_matches: int = 0
    match_rate: float = 0.0
    avg_score: float = 0.0
    high_quality_matches: int = 0
    processing_time: float = 0.0
    timestamp: datetime = None


@dataclass
class MatchingAlert:
    """åŒ¹é…å‘Šè­¦ä¿¡æ¯"""
    alert_type: str
    severity: str  # low, medium, high, critical
    message: str
    details: Dict[str, Any]
    timestamp: datetime


class MatchingMonitor:
    """åŒ¹é…ç›‘æ§ç³»ç»Ÿ"""
    
    def __init__(self, 
                 db_manager: DatabaseManager,
                 vector_manager: ChromaDBManager,
                 config: Dict = None):
        self.db_manager = db_manager
        self.vector_manager = vector_manager
        self.config = config or {}
        self.logger = get_logger(__name__)
        
        # ç›‘æ§é…ç½®
        self.min_match_rate = config.get('min_match_rate', 0.15)  # æœ€ä½åŒ¹é…ç‡15%
        self.min_avg_score = config.get('min_avg_score', 0.5)    # æœ€ä½å¹³å‡åˆ†æ•°
        self.check_interval_hours = config.get('check_interval_hours', 6)  # æ£€æŸ¥é—´éš”6å°æ—¶
        
        # åˆå§‹åŒ–åŒ¹é…å™¨
        self.matcher = GenericResumeJobMatcher(vector_manager, config)
        
        # ç»Ÿè®¡å†å²
        self.stats_history: List[MatchingStats] = []
        self.alerts: List[MatchingAlert] = []
    
    async def run_matching_check(self) -> MatchingStats:
        """æ‰§è¡ŒåŒ¹é…æ£€æŸ¥"""
        start_time = time.time()
        
        try:
            self.logger.info("ğŸ” å¼€å§‹æ‰§è¡Œå®šæœŸåŒ¹é…æ£€æŸ¥")
            
            # 1. è·å–æ•°æ®åº“ç»Ÿè®¡
            stats = await self._collect_database_stats()
            
            # 2. æ£€æŸ¥åŒ¹é…è´¨é‡
            quality_issues = await self._check_matching_quality(stats)
            
            # 3. ç”Ÿæˆå‘Šè­¦
            if quality_issues:
                await self._generate_alerts(quality_issues)
            
            # 4. æ‰§è¡Œè‡ªåŠ¨ä¿®å¤ï¼ˆå¦‚æœé…ç½®äº†ï¼‰
            if self.config.get('auto_fix_enabled', False):
                await self._auto_fix_issues(quality_issues)
            
            # 5. è®°å½•ç»Ÿè®¡
            stats.processing_time = time.time() - start_time
            stats.timestamp = datetime.now()
            self.stats_history.append(stats)
            
            # ä¿æŒå†å²è®°å½•åœ¨åˆç†èŒƒå›´å†…
            if len(self.stats_history) > 100:
                self.stats_history = self.stats_history[-100:]
            
            self.logger.info(f"âœ… åŒ¹é…æ£€æŸ¥å®Œæˆï¼Œè€—æ—¶ {stats.processing_time:.2f}ç§’")
            self.logger.info(f"ğŸ“Š åŒ¹é…ç»Ÿè®¡: æ€»èŒä½{stats.total_jobs}, åŒ¹é…{stats.total_matches}, "
                           f"åŒ¹é…ç‡{stats.match_rate:.1%}, å¹³å‡åˆ†{stats.avg_score:.3f}")
            
            return stats
            
        except Exception as e:
            self.logger.error(f"ğŸ’¥ åŒ¹é…æ£€æŸ¥å¤±è´¥: {str(e)}")
            raise
    
    async def _collect_database_stats(self) -> MatchingStats:
        """æ”¶é›†æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯"""
        try:
            conn = sqlite3.connect(self.db_manager.db_path)
            cursor = conn.cursor()
            
            # æ€»èŒä½æ•°
            cursor.execute("SELECT COUNT(*) FROM jobs")
            total_jobs = cursor.fetchone()[0]
            
            # æ€»åŒ¹é…æ•°
            cursor.execute("SELECT COUNT(*) FROM resume_matches")
            total_matches = cursor.fetchone()[0]
            
            # å¹³å‡åŒ¹é…åˆ†æ•°
            cursor.execute("SELECT AVG(match_score) FROM resume_matches WHERE match_score > 0")
            avg_score_result = cursor.fetchone()[0]
            avg_score = avg_score_result if avg_score_result else 0.0
            
            # é«˜è´¨é‡åŒ¹é…æ•°ï¼ˆåˆ†æ•° >= 0.7ï¼‰
            cursor.execute("SELECT COUNT(*) FROM resume_matches WHERE match_score >= 0.7")
            high_quality_matches = cursor.fetchone()[0]
            
            # è®¡ç®—åŒ¹é…ç‡
            match_rate = total_matches / total_jobs if total_jobs > 0 else 0.0
            
            conn.close()
            
            return MatchingStats(
                total_jobs=total_jobs,
                total_matches=total_matches,
                match_rate=match_rate,
                avg_score=avg_score,
                high_quality_matches=high_quality_matches
            )
            
        except Exception as e:
            self.logger.error(f"æ”¶é›†æ•°æ®åº“ç»Ÿè®¡å¤±è´¥: {str(e)}")
            raise
    
    async def _check_matching_quality(self, stats: MatchingStats) -> List[Dict[str, Any]]:
        """æ£€æŸ¥åŒ¹é…è´¨é‡é—®é¢˜"""
        issues = []
        
        # 1. æ£€æŸ¥åŒ¹é…ç‡
        if stats.match_rate < self.min_match_rate:
            issues.append({
                'type': 'low_match_rate',
                'severity': 'high' if stats.match_rate < 0.1 else 'medium',
                'current_value': stats.match_rate,
                'threshold': self.min_match_rate,
                'description': f'åŒ¹é…ç‡è¿‡ä½: {stats.match_rate:.1%} < {self.min_match_rate:.1%}'
            })
        
        # 2. æ£€æŸ¥å¹³å‡åˆ†æ•°
        if stats.avg_score < self.min_avg_score:
            issues.append({
                'type': 'low_avg_score',
                'severity': 'medium',
                'current_value': stats.avg_score,
                'threshold': self.min_avg_score,
                'description': f'å¹³å‡åŒ¹é…åˆ†æ•°è¿‡ä½: {stats.avg_score:.3f} < {self.min_avg_score:.3f}'
            })
        
        # 3. æ£€æŸ¥é«˜è´¨é‡åŒ¹é…æ¯”ä¾‹
        if stats.total_matches > 0:
            high_quality_ratio = stats.high_quality_matches / stats.total_matches
            if high_quality_ratio < 0.3:  # é«˜è´¨é‡åŒ¹é…åº”è¯¥å 30%ä»¥ä¸Š
                issues.append({
                    'type': 'low_quality_ratio',
                    'severity': 'medium',
                    'current_value': high_quality_ratio,
                    'threshold': 0.3,
                    'description': f'é«˜è´¨é‡åŒ¹é…æ¯”ä¾‹è¿‡ä½: {high_quality_ratio:.1%} < 30%'
                })
        
        # 4. æ£€æŸ¥è¶‹åŠ¿ï¼ˆå¦‚æœæœ‰å†å²æ•°æ®ï¼‰
        if len(self.stats_history) >= 3:
            recent_stats = self.stats_history[-3:]
            match_rates = [s.match_rate for s in recent_stats]
            
            # æ£€æŸ¥åŒ¹é…ç‡æ˜¯å¦æŒç»­ä¸‹é™
            if all(match_rates[i] > match_rates[i+1] for i in range(len(match_rates)-1)):
                issues.append({
                    'type': 'declining_trend',
                    'severity': 'medium',
                    'current_value': match_rates,
                    'description': 'åŒ¹é…ç‡å‘ˆä¸‹é™è¶‹åŠ¿'
                })
        
        return issues
    
    async def _generate_alerts(self, issues: List[Dict[str, Any]]):
        """ç”Ÿæˆå‘Šè­¦"""
        for issue in issues:
            alert = MatchingAlert(
                alert_type=issue['type'],
                severity=issue['severity'],
                message=issue['description'],
                details=issue,
                timestamp=datetime.now()
            )
            
            self.alerts.append(alert)
            
            # è®°å½•å‘Šè­¦æ—¥å¿—
            severity_emoji = {
                'low': 'ğŸŸ¡',
                'medium': 'ğŸŸ ', 
                'high': 'ğŸ”´',
                'critical': 'ğŸ’¥'
            }
            
            emoji = severity_emoji.get(alert.severity, 'âš ï¸')
            self.logger.warning(f"{emoji} åŒ¹é…è´¨é‡å‘Šè­¦ [{alert.severity.upper()}]: {alert.message}")
        
        # ä¿æŒå‘Šè­¦å†å²åœ¨åˆç†èŒƒå›´å†…
        if len(self.alerts) > 50:
            self.alerts = self.alerts[-50:]
    
    async def _auto_fix_issues(self, issues: List[Dict[str, Any]]):
        """è‡ªåŠ¨ä¿®å¤é—®é¢˜"""
        for issue in issues:
            issue_type = issue['type']
            
            try:
                if issue_type == 'low_match_rate':
                    await self._fix_low_match_rate()
                elif issue_type == 'low_avg_score':
                    await self._fix_low_avg_score()
                
                self.logger.info(f"ğŸ”§ å·²å°è¯•è‡ªåŠ¨ä¿®å¤é—®é¢˜: {issue_type}")
                
            except Exception as e:
                self.logger.error(f"è‡ªåŠ¨ä¿®å¤ {issue_type} å¤±è´¥: {str(e)}")
    
    async def _fix_low_match_rate(self):
        """ä¿®å¤ä½åŒ¹é…ç‡é—®é¢˜"""
        try:
            self.logger.info("ğŸ”§ å¼€å§‹ä¿®å¤ä½åŒ¹é…ç‡é—®é¢˜")
            
            # 1. è·å–æœªåŒ¹é…çš„èŒä½
            conn = sqlite3.connect(self.db_manager.db_path)
            cursor = conn.cursor()
            
            cursor.execute("""
                SELECT job_id FROM jobs 
                WHERE job_id NOT IN (SELECT DISTINCT job_id FROM resume_matches)
                AND rag_processed = 1
                LIMIT 50
            """)
            
            unmatched_jobs = [row[0] for row in cursor.fetchall()]
            conn.close()
            
            if not unmatched_jobs:
                self.logger.info("æ²¡æœ‰æ‰¾åˆ°æœªåŒ¹é…çš„å·²å¤„ç†èŒä½")
                return
            
            self.logger.info(f"æ‰¾åˆ° {len(unmatched_jobs)} ä¸ªæœªåŒ¹é…èŒä½ï¼Œå¼€å§‹é‡æ–°åŒ¹é…")
            
            # 2. åˆ›å»ºé»˜è®¤ç®€å†æ¡£æ¡ˆï¼ˆè¿™é‡Œåº”è¯¥ä»å®é™…ç®€å†æ•°æ®åˆ›å»ºï¼‰
            resume_profile = await self._get_default_resume_profile()
            
            # 3. é‡æ–°è¿è¡ŒåŒ¹é…
            result = await self.matcher.find_matching_jobs(resume_profile, top_k=len(unmatched_jobs))
            
            # 4. ä¿å­˜åŒ¹é…ç»“æœ
            if result.matches:
                await self._save_match_results(result.matches)
                self.logger.info(f"âœ… é‡æ–°åŒ¹é…å®Œæˆï¼Œæ–°å¢ {len(result.matches)} ä¸ªåŒ¹é…")
            
        except Exception as e:
            self.logger.error(f"ä¿®å¤ä½åŒ¹é…ç‡å¤±è´¥: {str(e)}")
    
    async def _fix_low_avg_score(self):
        """ä¿®å¤ä½å¹³å‡åˆ†æ•°é—®é¢˜"""
        try:
            self.logger.info("ğŸ”§ å¼€å§‹ä¿®å¤ä½å¹³å‡åˆ†æ•°é—®é¢˜")
            
            # è¿™é‡Œå¯ä»¥å®ç°åˆ†æ•°é‡æ–°è®¡ç®—é€»è¾‘
            # ä¾‹å¦‚ï¼šæ›´æ–°åŒ¹é…æƒé‡ã€é‡æ–°è®¡ç®—ç°æœ‰åŒ¹é…çš„åˆ†æ•°ç­‰
            
            self.logger.info("ä½å¹³å‡åˆ†æ•°ä¿®å¤é€»è¾‘å¾…å®ç°")
            
        except Exception as e:
            self.logger.error(f"ä¿®å¤ä½å¹³å‡åˆ†æ•°å¤±è´¥: {str(e)}")
    
    async def _get_default_resume_profile(self) -> GenericResumeProfile:
        """è·å–é»˜è®¤ç®€å†æ¡£æ¡ˆ"""
        # è¿™é‡Œåº”è¯¥ä»å®é™…çš„ç®€å†æ•°æ®åˆ›å»º
        # æš‚æ—¶è¿”å›ä¸€ä¸ªç¤ºä¾‹æ¡£æ¡ˆ
        from .generic_resume_models import GenericResumeProfile, SkillCategory
        
        return GenericResumeProfile(
            name="Default User",
            total_experience_years=5,
            current_position="Software Engineer",
            skill_categories=[
                SkillCategory(
                    category_name="programming_languages",
                    skills=["Python", "Java", "JavaScript"],
                    proficiency_level="advanced"
                )
            ],
            expected_salary_range={"min": 200000, "max": 500000}
        )
    
    async def _save_match_results(self, matches: List):
        """ä¿å­˜åŒ¹é…ç»“æœåˆ°æ•°æ®åº“"""
        try:
            conn = sqlite3.connect(self.db_manager.db_path)
            cursor = conn.cursor()
            
            for match in matches:
                cursor.execute("""
                    INSERT OR REPLACE INTO resume_matches 
                    (job_id, resume_profile_id, match_score, priority_level, 
                     semantic_score, skill_match_score, experience_match_score, 
                     location_match_score, salary_match_score, match_details, 
                     match_reasons, created_at, processed)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, datetime('now'), 0)
                """, (
                    match.job_id,
                    'default',
                    match.overall_score,
                    match.match_level.value if hasattr(match.match_level, 'value') else str(match.match_level),
                    match.dimension_scores.get('semantic_similarity', 0),
                    match.dimension_scores.get('skills_match', 0),
                    match.dimension_scores.get('experience_match', 0),
                    match.dimension_scores.get('industry_match', 0),
                    match.dimension_scores.get('salary_match', 0),
                    json.dumps(match.dimension_scores),
                    f"è‡ªåŠ¨åŒ¹é…ä¿®å¤: {match.job_title} at {match.company}"
                ))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜åŒ¹é…ç»“æœå¤±è´¥: {str(e)}")
    
    def start_scheduled_monitoring(self):
        """å¯åŠ¨å®šæ—¶ç›‘æ§"""
        self.logger.info(f"ğŸ• å¯åŠ¨å®šæ—¶åŒ¹é…ç›‘æ§ï¼Œæ£€æŸ¥é—´éš”: {self.check_interval_hours}å°æ—¶")
        
        # è®¾ç½®å®šæ—¶ä»»åŠ¡
        schedule.every(self.check_interval_hours).hours.do(
            lambda: asyncio.create_task(self.run_matching_check())
        )
        
        # ç«‹å³æ‰§è¡Œä¸€æ¬¡æ£€æŸ¥
        asyncio.create_task(self.run_matching_check())
        
        # è¿è¡Œè°ƒåº¦å™¨
        while True:
            schedule.run_pending()
            time.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡è°ƒåº¦
    
    def get_monitoring_report(self) -> Dict[str, Any]:
        """è·å–ç›‘æ§æŠ¥å‘Š"""
        if not self.stats_history:
            return {"message": "æš‚æ— ç›‘æ§æ•°æ®"}
        
        latest_stats = self.stats_history[-1]
        recent_alerts = [a for a in self.alerts if a.timestamp > datetime.now() - timedelta(hours=24)]
        
        return {
            "latest_stats": {
                "total_jobs": latest_stats.total_jobs,
                "total_matches": latest_stats.total_matches,
                "match_rate": f"{latest_stats.match_rate:.1%}",
                "avg_score": f"{latest_stats.avg_score:.3f}",
                "high_quality_matches": latest_stats.high_quality_matches,
                "timestamp": latest_stats.timestamp.isoformat() if latest_stats.timestamp else None
            },
            "recent_alerts": [
                {
                    "type": alert.alert_type,
                    "severity": alert.severity,
                    "message": alert.message,
                    "timestamp": alert.timestamp.isoformat()
                }
                for alert in recent_alerts
            ],
            "trend_analysis": self._analyze_trends(),
            "recommendations": self._generate_recommendations()
        }
    
    def _analyze_trends(self) -> Dict[str, Any]:
        """åˆ†æè¶‹åŠ¿"""
        if len(self.stats_history) < 2:
            return {"message": "æ•°æ®ä¸è¶³ï¼Œæ— æ³•åˆ†æè¶‹åŠ¿"}
        
        recent_stats = self.stats_history[-5:]  # æœ€è¿‘5æ¬¡æ£€æŸ¥
        
        match_rates = [s.match_rate for s in recent_stats]
        avg_scores = [s.avg_score for s in recent_stats]
        
        return {
            "match_rate_trend": "ä¸Šå‡" if match_rates[-1] > match_rates[0] else "ä¸‹é™",
            "avg_score_trend": "ä¸Šå‡" if avg_scores[-1] > avg_scores[0] else "ä¸‹é™",
            "match_rate_change": f"{((match_rates[-1] - match_rates[0]) / match_rates[0] * 100):.1f}%" if match_rates[0] > 0 else "N/A",
            "avg_score_change": f"{((avg_scores[-1] - avg_scores[0]) / avg_scores[0] * 100):.1f}%" if avg_scores[0] > 0 else "N/A"
        }
    
    def _generate_recommendations(self) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        recommendations = []
        
        if not self.stats_history:
            return ["å»ºè®®å…ˆè¿è¡ŒåŒ¹é…æ£€æŸ¥ä»¥è·å–æ•°æ®"]
        
        latest_stats = self.stats_history[-1]
        
        if latest_stats.match_rate < 0.15:
            recommendations.append("åŒ¹é…ç‡è¿‡ä½ï¼Œå»ºè®®æ£€æŸ¥åŒ¹é…ç®—æ³•å‚æ•°æˆ–ç®€å†æ•°æ®è´¨é‡")
        
        if latest_stats.avg_score < 0.5:
            recommendations.append("å¹³å‡åŒ¹é…åˆ†æ•°åä½ï¼Œå»ºè®®ä¼˜åŒ–åŒ¹é…æƒé‡é…ç½®")
        
        if latest_stats.total_matches > 0:
            high_quality_ratio = latest_stats.high_quality_matches / latest_stats.total_matches
            if high_quality_ratio < 0.3:
                recommendations.append("é«˜è´¨é‡åŒ¹é…æ¯”ä¾‹åä½ï¼Œå»ºè®®æé«˜åŒ¹é…é˜ˆå€¼æˆ–ä¼˜åŒ–æŠ€èƒ½åŒ¹é…é€»è¾‘")
        
        if not recommendations:
            recommendations.append("åŒ¹é…è´¨é‡è‰¯å¥½ï¼Œç»§ç»­ä¿æŒå½“å‰é…ç½®")
        
        return recommendations


# ç‹¬ç«‹è¿è¡Œè„šæœ¬
async def main():
    """ä¸»å‡½æ•°"""
    import sys
    import os
    
    # æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
    sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))
    
    from src.database.operations import DatabaseManager
    from src.rag.vector_manager import ChromaDBManager
    
    # åˆå§‹åŒ–ç»„ä»¶
    db_manager = DatabaseManager('data/jobs.db')
    vector_manager = ChromaDBManager()
    
    # åˆ›å»ºç›‘æ§å™¨
    monitor = MatchingMonitor(db_manager, vector_manager)
    
    # è¿è¡Œæ£€æŸ¥
    stats = await monitor.run_matching_check()
    
    # æ‰“å°æŠ¥å‘Š
    report = monitor.get_monitoring_report()
    print(json.dumps(report, indent=2, ensure_ascii=False))


if __name__ == "__main__":
    asyncio.run(main())