#!/usr/bin/env python3
"""
é€šç”¨ç®€å†èŒä½åŒ¹é…å¼•æ“
æ”¯æŒä»»æ„ç”¨æˆ·çš„çµæ´»åŒ¹é…ç³»ç»Ÿ
"""

import asyncio
import time
from typing import List, Dict, Any, Optional, Tuple
from collections import defaultdict
from langchain.schema import Document

from ..rag.semantic_search import SemanticSearchEngine
from ..rag.vector_manager import ChromaDBManager
from ..utils.logger import get_logger
from .generic_resume_models import (
    GenericResumeProfile, DynamicSkillWeights, MatchLevel, RecommendationPriority,
    create_default_skill_weights, JobMatchResult, ResumeMatchingResult,
    MatchingSummary, CareerInsights, MatchAnalysis
)


class GenericResumeJobMatcher:
    """é€šç”¨ç®€å†èŒä½åŒ¹é…å¼•æ“"""
    
    def __init__(self, 
                 vector_manager: ChromaDBManager,
                 config: Dict = None):
        self.vector_manager = vector_manager
        self.config = config or {}
        self.logger = get_logger(__name__)
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.search_engine = SemanticSearchEngine(vector_manager, config)
        
        # åŒ¹é…å‚æ•°é…ç½® - æé«˜é˜ˆå€¼é™ä½åŒ¹é…ç‡ï¼Œç¡®ä¿è´¨é‡
        self.default_search_k = config.get('default_search_k', 80)  # å¢åŠ æœç´¢èŒƒå›´
        self.min_score_threshold = config.get('min_score_threshold', 0.45)  # æé«˜æœ€ä½é˜ˆå€¼åˆ°0.45ï¼Œç¡®ä¿åŒ¹é…è´¨é‡
        self.max_results = config.get('max_results', 30)  # å¢åŠ æœ€å¤§ç»“æœæ•°
        
        # åŠ¨æ€æƒé‡é…ç½® - ä»é…ç½®æ–‡ä»¶è¯»å–
        self.matching_weights = self._load_matching_weights(config)
        
        # åˆå§‹åŒ–æŠ€èƒ½æƒé‡ç³»ç»Ÿ
        self.skill_weights = create_default_skill_weights()
        if config:
            self.skill_weights.update_from_config(config)
        
        # æ€§èƒ½ç›‘æ§
        self.performance_stats = {
            'total_matches': 0,
            'average_processing_time': 0.0,
            'cache_hits': 0
        }
    
    def _load_matching_weights(self, config: Dict) -> Dict[str, float]:
        """ä»é…ç½®æ–‡ä»¶åŠ è½½åŒ¹é…æƒé‡ - æ”¯æŒé«˜çº§åŒ¹é…é…ç½®"""
        # ä¼˜åŒ–çš„é»˜è®¤æƒé‡ - é’ˆå¯¹20å¹´ç»éªŒèµ„æ·±äººå£«
        default_weights = {
            'semantic_similarity': 0.40,  # è¯­ä¹‰ç›¸ä¼¼åº¦æƒé‡
            'skills_match': 0.45,         # æŠ€èƒ½åŒ¹é…æƒé‡ï¼ˆæœ€é‡è¦ï¼‰
            'experience_match': 0.05,     # ç»éªŒæƒé‡ï¼ˆ20å¹´ç»éªŒåŸºæœ¬éƒ½æ»¡è¶³ï¼‰
            'industry_match': 0.02,       # è¡Œä¸šæƒé‡ï¼ˆITè·¨è¡Œä¸šï¼‰
            'salary_match': 0.08          # è–ªèµ„æƒé‡ï¼ˆé‡è¦ä½†ä¸æ˜¯å†³å®šæ€§ï¼‰
        }
        
        # ä¼˜å…ˆè¯»å–é«˜çº§åŒ¹é…é…ç½®
        if config and 'resume_matching_advanced' in config:
            advanced_config = config['resume_matching_advanced']
            
            # è¯»å–é«˜çº§æƒé‡é…ç½®
            if 'matching_weights' in advanced_config:
                advanced_weights = advanced_config['matching_weights']
                default_weights.update(advanced_weights)
                self.logger.info(f"ğŸš€ ä½¿ç”¨é«˜çº§åŒ¹é…æƒé‡é…ç½®: {default_weights}")
            
            # è¯»å–é«˜çº§é˜ˆå€¼é…ç½®
            if 'match_thresholds' in advanced_config:
                thresholds = advanced_config['match_thresholds']
                if 'poor' in thresholds:
                    self.min_score_threshold = thresholds['poor']
                    self.logger.info(f"ğŸ¯ ä½¿ç”¨é«˜çº§åŒ¹é…é˜ˆå€¼: {self.min_score_threshold}")
            
            # è¯»å–é«˜çº§æœç´¢èŒƒå›´é…ç½®
            if 'default_search_k' in advanced_config:
                self.default_search_k = advanced_config['default_search_k']
                self.logger.info(f"ğŸ” ä½¿ç”¨é«˜çº§æœç´¢èŒƒå›´: {self.default_search_k}")
            
            # è¯»å–é«˜çº§æœ€å¤§ç»“æœé…ç½®
            if 'max_results' in advanced_config:
                self.max_results = advanced_config['max_results']
                self.logger.info(f"ğŸ“Š ä½¿ç”¨é«˜çº§æœ€å¤§ç»“æœæ•°: {self.max_results}")
        
        # å›é€€åˆ°æ ‡å‡†é…ç½®è¯»å–
        elif config and 'modules' in config and 'resume_matching' in config['modules']:
            resume_matching_config = config['modules']['resume_matching']
            
            if 'algorithms' in resume_matching_config:
                algorithms = resume_matching_config['algorithms']
                
                # è§£æç®—æ³•æƒé‡é…ç½®
                for algo in algorithms:
                    if algo.get('enabled', False):
                        algo_name = algo.get('name', '')
                        weight = algo.get('weight', 0.0)
                        
                        # æ˜ å°„é…ç½®æ–‡ä»¶ä¸­çš„ç®—æ³•åç§°åˆ°å†…éƒ¨æƒé‡é”®
                        if algo_name == 'semantic_matching':
                            default_weights['semantic_similarity'] = weight
                        elif algo_name == 'skill_matching':
                            default_weights['skills_match'] = weight
                        elif algo_name == 'keyword_matching':
                            # keyword_matching å¯ä»¥æ˜ å°„åˆ° semantic_similarity æˆ–å•ç‹¬å¤„ç†
                            # è¿™é‡Œæˆ‘ä»¬å°†å…¶åˆå¹¶åˆ° semantic_similarity ä¸­
                            default_weights['semantic_similarity'] += weight * 0.5
                            default_weights['skills_match'] += weight * 0.5
                
                self.logger.info(f"ä»æ ‡å‡†é…ç½®æ–‡ä»¶åŠ è½½åŒ¹é…æƒé‡: {default_weights}")
        
        # ä¹Ÿæ”¯æŒç›´æ¥çš„ weights é…ç½®ï¼ˆå‘åå…¼å®¹ï¼‰
        if config and 'weights' in config:
            default_weights.update(config['weights'])
            self.logger.info(f"ä½¿ç”¨ç›´æ¥æƒé‡é…ç½®: {default_weights}")
        
        # ç¡®ä¿æƒé‡æ€»å’Œä¸º1.0
        total_weight = sum(default_weights.values())
        if total_weight > 0:
            normalized_weights = {k: v/total_weight for k, v in default_weights.items()}
            self.logger.info(f"æ ‡å‡†åŒ–åçš„åŒ¹é…æƒé‡: {normalized_weights}")
            return normalized_weights
        
        return default_weights
    
    async def find_matching_jobs(self,
                                resume_profile: GenericResumeProfile,
                                filters: Dict[str, Any] = None,
                                top_k: int = 20) -> ResumeMatchingResult:
        """ä¸ºä»»æ„ç”¨æˆ·æŸ¥æ‰¾åŒ¹é…çš„èŒä½"""
        start_time = time.time()
        
        try:
            self.logger.info(f"ğŸ” å¼€å§‹ä¸º {resume_profile.name} æŸ¥æ‰¾åŒ¹é…èŒä½ï¼Œç›®æ ‡æ•°é‡: {top_k}")
            self.logger.info(f"ğŸ“Š ç®€å†åŸºæœ¬ä¿¡æ¯: ç»éªŒ{resume_profile.total_experience_years}å¹´, æŠ€èƒ½æ•°é‡{len(resume_profile.get_all_skills())}")
            
            # 1. æ„å»ºä¸ªæ€§åŒ–æŸ¥è¯¢
            query = self._build_personalized_query(resume_profile)
            self.logger.debug(f"ğŸ”¤ æ„å»ºæŸ¥è¯¢: {query[:100]}...")
            
            # 2. æ‰§è¡Œè¯­ä¹‰æœç´¢
            search_k = min(self.default_search_k, top_k * 3)
            self.logger.info(f"ğŸ” æ‰§è¡Œè¯­ä¹‰æœç´¢ï¼Œæœç´¢èŒƒå›´: {search_k}")
            search_results = await self._execute_semantic_search(
                query, filters, k=search_k
            )
            
            self.logger.info(f"ğŸ“„ è¯­ä¹‰æœç´¢è¿”å› {len(search_results)} ä¸ªå€™é€‰æ–‡æ¡£")
            
            # 3. æŒ‰èŒä½IDåˆ†ç»„æ–‡æ¡£
            jobs_by_id = self._group_results_by_job(search_results)
            self.logger.info(f"ğŸ“‹ åˆ†ç»„åå¾—åˆ° {len(jobs_by_id)} ä¸ªå€™é€‰èŒä½")
            
            # 4. è®¡ç®—åŒ¹é…åˆ†æ•°
            matching_jobs = []
            failed_matches = 0
            below_threshold = 0
            
            for job_id, job_docs in jobs_by_id.items():
                try:
                    match_result = await self._calculate_match_score(
                        resume_profile, job_docs, job_id
                    )
                    
                    if match_result:
                        if match_result.overall_score >= self.min_score_threshold:
                            matching_jobs.append(match_result)
                            self.logger.debug(f"âœ… èŒä½ {job_id} åŒ¹é…æˆåŠŸï¼Œåˆ†æ•°: {match_result.overall_score:.3f}")
                        else:
                            below_threshold += 1
                            self.logger.debug(f"âš ï¸ èŒä½ {job_id} åˆ†æ•°è¿‡ä½: {match_result.overall_score:.3f} < {self.min_score_threshold}")
                    else:
                        failed_matches += 1
                        
                except Exception as e:
                    failed_matches += 1
                    self.logger.warning(f"âŒ è®¡ç®—èŒä½ {job_id} åŒ¹é…åº¦å¤±è´¥: {str(e)}")
                    continue
            
            # è®°å½•åŒ¹é…ç»Ÿè®¡
            self.logger.info(f"ğŸ“Š åŒ¹é…ç»Ÿè®¡: æˆåŠŸ{len(matching_jobs)}, ä½åˆ†{below_threshold}, å¤±è´¥{failed_matches}")
            
            # 5. æ’åºå’Œç­›é€‰ç»“æœ
            matching_jobs.sort(key=lambda x: x.overall_score, reverse=True)
            top_matches = matching_jobs[:top_k]
            
            if top_matches:
                best_score = top_matches[0].overall_score
                worst_score = top_matches[-1].overall_score
                self.logger.info(f"ğŸ† æœ€ç»ˆç»“æœåˆ†æ•°èŒƒå›´: {worst_score:.3f} - {best_score:.3f}")
            
            # 6. ç”ŸæˆåŒ¹é…æ‘˜è¦å’Œæ´å¯Ÿ
            processing_time = time.time() - start_time
            summary = self._generate_matching_summary(top_matches, processing_time)
            insights = self._generate_career_insights(top_matches, resume_profile)
            
            # 7. åˆ›å»ºå®Œæ•´ç»“æœ
            result = ResumeMatchingResult(
                matching_summary=summary,
                matches=top_matches,
                career_insights=insights,
                resume_profile=resume_profile,
                query_metadata={
                    'query': query,
                    'filters': filters,
                    'search_results_count': len(search_results),
                    'candidate_jobs_count': len(jobs_by_id),
                    'successful_matches': len(matching_jobs),
                    'failed_matches': failed_matches,
                    'below_threshold': below_threshold,
                    'processing_time': processing_time
                }
            )
            
            # æ›´æ–°æ€§èƒ½ç»Ÿè®¡
            self._update_performance_stats(processing_time, len(top_matches))
            
            self.logger.info(f"âœ… åŒ¹é…å®Œæˆï¼Œè¿”å› {len(top_matches)} ä¸ªèŒä½ï¼Œè€—æ—¶ {processing_time:.2f}ç§’")
            
            # è®°å½•åŒ¹é…ç‡è­¦å‘Š
            if len(jobs_by_id) > 0:
                match_rate = len(matching_jobs) / len(jobs_by_id)
                if match_rate < 0.2:
                    self.logger.warning(f"âš ï¸ åŒ¹é…ç‡è¿‡ä½: {match_rate:.1%} ({len(matching_jobs)}/{len(jobs_by_id)})")
                else:
                    self.logger.info(f"ğŸ“ˆ åŒ¹é…ç‡: {match_rate:.1%} ({len(matching_jobs)}/{len(jobs_by_id)})")
            
            return result
            
        except Exception as e:
            self.logger.error(f"ğŸ’¥ èŒä½åŒ¹é…å¤±è´¥: {str(e)}")
            raise
    
    def _build_personalized_query(self, resume_profile: GenericResumeProfile) -> str:
        """æ„å»ºä¸ªæ€§åŒ–æŸ¥è¯¢"""
        query_parts = []
        
        # 1. å½“å‰èŒä½å’Œç»éªŒ
        if resume_profile.current_position:
            query_parts.append(f"{resume_profile.current_position} {resume_profile.total_experience_years}å¹´ç»éªŒ")
        
        # 2. æ ¸å¿ƒæŠ€èƒ½ï¼ˆä»å„ä¸ªåˆ†ç±»ä¸­é€‰æ‹©ï¼‰
        all_skills = resume_profile.get_all_skills()
        if all_skills:
            # æ ¹æ®æƒé‡æ’åºæŠ€èƒ½ï¼Œé€‰æ‹©å‰8ä¸ª
            weighted_skills = []
            for skill in all_skills[:15]:  # é™åˆ¶æŠ€èƒ½æ•°é‡é¿å…æŸ¥è¯¢è¿‡é•¿
                weight = self.skill_weights.get_skill_weight(skill)
                weighted_skills.append((skill, weight))
            
            weighted_skills.sort(key=lambda x: x[1], reverse=True)
            top_skills = [skill for skill, _ in weighted_skills[:8]]
            query_parts.append("æ ¸å¿ƒæŠ€èƒ½: " + " ".join(top_skills))
        
        # 3. æœŸæœ›èŒä½
        if resume_profile.preferred_positions:
            preferred_text = " ".join(resume_profile.preferred_positions[:3])
            query_parts.append(f"ç›®æ ‡èŒä½: {preferred_text}")
        
        # 4. è¡Œä¸šç»éªŒ
        if resume_profile.industry_experience:
            industries = list(resume_profile.industry_experience.keys())[:2]
            query_parts.append(f"è¡Œä¸šç»éªŒ: {' '.join(industries)}")
        
        # 5. è½¯æŠ€èƒ½å’Œç®¡ç†ç»éªŒ
        if resume_profile.soft_skills:
            soft_skills_text = " ".join(resume_profile.soft_skills[:3])
            query_parts.append(f"è½¯æŠ€èƒ½: {soft_skills_text}")
        
        query = " ".join(query_parts)
        return query
    
    async def _execute_semantic_search(self,
                                     query: str,
                                     filters: Dict[str, Any] = None,
                                     k: int = 60) -> List[Tuple[Document, float]]:
        """æ‰§è¡Œè¯­ä¹‰æœç´¢ - æ”¯æŒæ—¶é—´æ„ŸçŸ¥æœç´¢"""
        try:
            # æ£€æŸ¥æ˜¯å¦å¯ç”¨æ—¶é—´æ„ŸçŸ¥æœç´¢
            time_aware_config = self.config.get('time_aware_search', {})
            enable_time_aware = time_aware_config.get('enable_time_boost', True)
            search_strategy = time_aware_config.get('search_strategy', 'hybrid')
            
            if enable_time_aware and hasattr(self.vector_manager, 'time_aware_similarity_search'):
                # ä½¿ç”¨æ—¶é—´æ„ŸçŸ¥æœç´¢
                self.logger.info(f"ğŸ•’ ä½¿ç”¨æ—¶é—´æ„ŸçŸ¥æœç´¢ï¼Œç­–ç•¥: {search_strategy}")
                search_results = self.vector_manager.time_aware_similarity_search(
                    query=query,
                    k=k,
                    filters=filters,
                    strategy=search_strategy
                )
            else:
                # ä½¿ç”¨ä¼ ç»Ÿæœç´¢
                self.logger.debug("ä½¿ç”¨ä¼ ç»Ÿå‘é‡æœç´¢")
                search_results = self.vector_manager.similarity_search_with_score(
                    query=query,
                    k=k,
                    filters=filters
                )
            
            return search_results
            
        except Exception as e:
            self.logger.error(f"è¯­ä¹‰æœç´¢å¤±è´¥: {str(e)}")
            return []
    
    def _group_results_by_job(self, search_results: List[Tuple[Document, float]]) -> Dict[str, List[Document]]:
        """æŒ‰èŒä½IDåˆ†ç»„æœç´¢ç»“æœï¼Œè¿‡æ»¤å·²åˆ é™¤èŒä½"""
        jobs_by_id = defaultdict(list)
        
        for doc, score in search_results:
            job_id = doc.metadata.get('job_id')
            if job_id:
                # æ£€æŸ¥èŒä½æ˜¯å¦å·²è¢«åˆ é™¤
                if self._is_job_available(job_id):
                    doc.metadata['search_score'] = score
                    jobs_by_id[job_id].append(doc)
                else:
                    self.logger.debug(f"è·³è¿‡å·²åˆ é™¤èŒä½: {job_id}")
        
        return dict(jobs_by_id)
    
    async def _calculate_match_score(self,
                                   resume_profile: GenericResumeProfile,
                                   job_docs: List[Document],
                                   job_id: str) -> Optional[JobMatchResult]:
        """è®¡ç®—åŒ¹é…åˆ†æ•°"""
        try:
            # æå–èŒä½å…ƒæ•°æ®
            job_metadata = self._extract_job_metadata(job_docs, job_id)
            job_title = job_metadata.get('job_title', 'Unknown Position')
            
            self.logger.debug(f"ğŸ”¢ å¼€å§‹è®¡ç®—èŒä½ {job_id} ({job_title}) çš„åŒ¹é…åˆ†æ•°")
            
            # è®¡ç®—å„ç»´åº¦åˆ†æ•°
            semantic_score = self._calculate_semantic_similarity(resume_profile, job_docs)
            skills_score = self._calculate_skills_match(resume_profile, job_docs, job_metadata)
            experience_score = self._calculate_experience_match(resume_profile, job_metadata)
            industry_score = self._calculate_industry_match(resume_profile, job_metadata)
            salary_score = self._calculate_salary_match(resume_profile, job_metadata)
            
            # è®°å½•å„ç»´åº¦åˆ†æ•°
            self.logger.debug(f"ğŸ“Š {job_id} å„ç»´åº¦åˆ†æ•°: è¯­ä¹‰{semantic_score:.3f}, æŠ€èƒ½{skills_score:.3f}, "
                            f"ç»éªŒ{experience_score:.3f}, è¡Œä¸š{industry_score:.3f}, è–ªèµ„{salary_score:.3f}")
            
            # è®¡ç®—åŠ æƒæ€»åˆ†
            dimension_scores = {
                'semantic_similarity': semantic_score,
                'skills_match': skills_score,
                'experience_match': experience_score,
                'industry_match': industry_score,
                'salary_match': salary_score
            }
            
            overall_score = (
                semantic_score * self.matching_weights['semantic_similarity'] +
                skills_score * self.matching_weights['skills_match'] +
                experience_score * self.matching_weights['experience_match'] +
                industry_score * self.matching_weights['industry_match'] +
                salary_score * self.matching_weights['salary_match']
            )
            
            self.logger.debug(f"ğŸ¯ {job_id} åŠ æƒæ€»åˆ†: {overall_score:.3f} "
                            f"(æƒé‡: è¯­ä¹‰{self.matching_weights['semantic_similarity']}, "
                            f"æŠ€èƒ½{self.matching_weights['skills_match']}, "
                            f"ç»éªŒ{self.matching_weights['experience_match']}, "
                            f"è¡Œä¸š{self.matching_weights['industry_match']}, "
                            f"è–ªèµ„{self.matching_weights['salary_match']})")
            
            # ç”ŸæˆåŒ¹é…åˆ†æ
            match_analysis = self._generate_match_analysis(
                resume_profile, job_docs, job_metadata, dimension_scores
            )
            
            # åˆ›å»ºåŒ¹é…ç»“æœ
            match_result = JobMatchResult(
                job_id=job_metadata.get('job_id', 'unknown'),
                job_title=job_title,
                company=job_metadata.get('company', 'Unknown Company'),
                location=job_metadata.get('location'),
                salary_range=job_metadata.get('salary_range'),
                overall_score=overall_score,
                dimension_scores=dimension_scores,
                match_level=self._get_match_level_from_score(overall_score),
                match_analysis=match_analysis,
                recommendation_priority=self._get_recommendation_priority_from_score(overall_score),
                confidence_level=self._calculate_confidence_level(dimension_scores),
                processing_time=time.time()
            )
            
            return match_result
            
        except Exception as e:
            self.logger.error(f"ğŸ’¥ è®¡ç®—èŒä½ {job_id} åŒ¹é…åº¦å¤±è´¥: {str(e)}")
            return None
    
    def _calculate_semantic_similarity(self,
                                     resume_profile: GenericResumeProfile,
                                     job_docs: List[Document]) -> float:
        """
        è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦ - ä¼˜åŒ–ç‰ˆæœ¬
        ä½¿ç”¨å‘é‡æœç´¢åˆ†æ•°æ›¿ä»£TF-IDFï¼Œæä¾›æ›´å‡†ç¡®çš„ä¸­æ–‡è¯­ä¹‰åŒ¹é…
        """
        try:
            # ä¼˜å…ˆä½¿ç”¨å‘é‡æœç´¢åˆ†æ•°
            vector_score = self._get_vector_similarity_score(job_docs)
            if vector_score > 0:
                self.logger.debug(f"ä½¿ç”¨å‘é‡æœç´¢åˆ†æ•°: {vector_score:.3f}")
                return vector_score
            
            # å›é€€ç­–ç•¥ï¼šåŸºäºæ–‡æ¡£è´¨é‡å’Œç±»å‹çš„è¯„åˆ†
            fallback_score = self._calculate_fallback_similarity(job_docs)
            self.logger.debug(f"ä½¿ç”¨å›é€€ç­–ç•¥åˆ†æ•°: {fallback_score:.3f}")
            return fallback_score
            
        except Exception as e:
            self.logger.error(f"è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦å¤±è´¥: {str(e)}")
            return 0.0
    
    def _get_vector_similarity_score(self, job_docs: List[Document]) -> float:
        """è·å–å‘é‡æœç´¢ç›¸ä¼¼åº¦åˆ†æ•°"""
        try:
            search_scores = []
            for doc in job_docs:
                if 'search_score' in doc.metadata:
                    score = doc.metadata['search_score']
                    # éªŒè¯åˆ†æ•°æœ‰æ•ˆæ€§
                    if isinstance(score, (int, float)) and 0 <= score <= 1:
                        search_scores.append(score)
            
            if not search_scores:
                return 0.0
            
            # å•ä¸ªæ–‡æ¡£ç›´æ¥è¿”å›åˆ†æ•°
            if len(search_scores) == 1:
                return search_scores[0]
            
            # å¤šä¸ªæ–‡æ¡£ä½¿ç”¨åŠ æƒå¹³å‡ï¼Œé«˜åˆ†è·å¾—æ›´å¤šæƒé‡
            weights = [score ** 1.2 for score in search_scores]
            total_weight = sum(weights)
            
            if total_weight > 0:
                weighted_avg = sum(s * w for s, w in zip(search_scores, weights)) / total_weight
                return min(1.0, max(0.0, weighted_avg))
            
            # ç®€å•å¹³å‡ä½œä¸ºå¤‡é€‰
            return sum(search_scores) / len(search_scores)
            
        except Exception as e:
            self.logger.error(f"è·å–å‘é‡ç›¸ä¼¼åº¦åˆ†æ•°å¤±è´¥: {str(e)}")
            return 0.0
    
    def _calculate_fallback_similarity(self, job_docs: List[Document]) -> float:
        """å›é€€ç›¸ä¼¼åº¦è®¡ç®—ç­–ç•¥"""
        try:
            # åŸºäºæ–‡æ¡£ç±»å‹å’Œå†…å®¹é•¿åº¦çš„å¯å‘å¼è¯„åˆ†
            type_scores = {
                'overview': 0.8,
                'skills': 0.85,
                'responsibility': 0.7,
                'requirement': 0.75,
                'basic_requirements': 0.6,
                'company_info': 0.4
            }
            
            total_weight = 0
            weighted_score = 0
            
            for doc in job_docs:
                doc_type = doc.metadata.get('type', 'unknown')
                base_score = type_scores.get(doc_type, 0.5)
                
                # æ ¹æ®å†…å®¹é•¿åº¦è°ƒæ•´åˆ†æ•°
                content_length = len(doc.page_content) if doc.page_content else 0
                if content_length > 500:
                    length_bonus = 0.1
                elif content_length > 200:
                    length_bonus = 0.05
                else:
                    length_bonus = 0.0
                
                final_score = min(1.0, base_score + length_bonus)
                weight = 1.0
                
                total_weight += weight
                weighted_score += final_score * weight
            
            if total_weight > 0:
                return weighted_score / total_weight
            
            return 0.5  # é»˜è®¤ä¸­ç­‰åˆ†æ•°
            
        except Exception as e:
            self.logger.error(f"å›é€€ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {str(e)}")
            return 0.5
    
    def _calculate_skills_match(self, 
                              resume_profile: GenericResumeProfile,
                              job_docs: List[Document],
                              job_metadata: Dict[str, Any]) -> float:
        """è®¡ç®—æŠ€èƒ½åŒ¹é…åº¦"""
        try:
            # è·å–ç®€å†æŠ€èƒ½
            resume_skills = [skill.lower() for skill in resume_profile.get_all_skills()]
            
            # è·å–èŒä½æŠ€èƒ½è¦æ±‚
            job_skills = self._extract_job_skills(job_docs, job_metadata)
            
            if not job_skills:
                return 0.5
            
            # è®¡ç®—åŠ æƒåŒ¹é…
            matched_skills = []
            total_job_skill_weight = 0
            matched_skill_weight = 0
            
            for job_skill in job_skills:
                skill_weight = self.skill_weights.get_skill_weight(job_skill)
                total_job_skill_weight += skill_weight
                
                if self._is_skill_matched(job_skill, resume_skills):
                    matched_skills.append(job_skill)
                    matched_skill_weight += skill_weight
            
            # è®¡ç®—åŒ¹é…ç‡
            if total_job_skill_weight > 0:
                match_rate = matched_skill_weight / total_job_skill_weight
            else:
                match_rate = 0.0
            
            # è€ƒè™‘é«˜ä»·å€¼æŠ€èƒ½åŠ åˆ†
            bonus_score = self._calculate_skill_bonus(resume_skills, job_skills)
            
            final_score = min(1.0, match_rate + bonus_score)
            
            return final_score
            
        except Exception as e:
            self.logger.error(f"è®¡ç®—æŠ€èƒ½åŒ¹é…åº¦å¤±è´¥: {str(e)}")
            return 0.0
    
    def _calculate_experience_match(self, 
                                  resume_profile: GenericResumeProfile,
                                  job_metadata: Dict[str, Any]) -> float:
        """è®¡ç®—ç»éªŒåŒ¹é…åº¦"""
        try:
            required_years = self._extract_required_experience(job_metadata)
            resume_years = resume_profile.total_experience_years
            
            if required_years is None:
                return 0.9
            
            if resume_years >= required_years:
                if resume_years <= required_years * 2:
                    return 1.0
                else:
                    # ç»éªŒè¿‡å¤šå¯èƒ½è¢«è®¤ä¸ºoverqualified
                    return 0.95
            else:
                return min(1.0, resume_years / required_years)
            
        except Exception as e:
            self.logger.error(f"è®¡ç®—ç»éªŒåŒ¹é…åº¦å¤±è´¥: {str(e)}")
            return 0.0
    
    def _calculate_industry_match(self, 
                                resume_profile: GenericResumeProfile,
                                job_metadata: Dict[str, Any]) -> float:
        """è®¡ç®—è¡Œä¸šåŒ¹é…åº¦"""
        try:
            job_industry = job_metadata.get('industry', '').lower()
            
            if not job_industry:
                return 0.7
            
            # æ£€æŸ¥ç›´æ¥è¡Œä¸šåŒ¹é…
            for resume_industry, weight in resume_profile.industry_experience.items():
                if resume_industry.lower() in job_industry or job_industry in resume_industry.lower():
                    return weight
            
            # æ£€æŸ¥ç›¸å…³è¡Œä¸šåŒ¹é…
            return self._calculate_industry_similarity(job_industry, list(resume_profile.industry_experience.keys()))
            
        except Exception as e:
            self.logger.error(f"è®¡ç®—è¡Œä¸šåŒ¹é…åº¦å¤±è´¥: {str(e)}")
            return 0.0
    
    def _calculate_salary_match(self,
                              resume_profile: GenericResumeProfile,
                              job_metadata: Dict[str, Any]) -> float:
        """è®¡ç®—è–ªèµ„åŒ¹é…åº¦ - ä¼˜åŒ–ç‰ˆæœ¬ï¼Œæ›´å®½æ¾çš„åŒ¹é…æ¡ä»¶"""
        try:
            job_salary_range = job_metadata.get('salary_range')
            
            # å¦‚æœä»»ä¸€æ–¹æ²¡æœ‰è–ªèµ„ä¿¡æ¯ï¼Œç»™äºˆä¸­ç­‰åˆ†æ•°
            if not job_salary_range or not resume_profile.expected_salary_range:
                self.logger.debug("è–ªèµ„ä¿¡æ¯ç¼ºå¤±ï¼Œè¿”å›é»˜è®¤åˆ†æ•° 0.8")
                return 0.8
            
            resume_min = resume_profile.expected_salary_range.get('min', 0)
            resume_max = resume_profile.expected_salary_range.get('max', 0)
            
            if resume_min == 0 and resume_max == 0:
                self.logger.debug("ç®€å†æœŸæœ›è–ªèµ„ä¸º0ï¼Œè¿”å›é»˜è®¤åˆ†æ•° 0.8")
                return 0.8
            
            job_min = job_salary_range.get('min', 0)
            job_max = job_salary_range.get('max', float('inf'))
            
            self.logger.debug(f"è–ªèµ„åŒ¹é…è®¡ç®—: ç®€å†æœŸæœ› {resume_min}-{resume_max}, èŒä½æä¾› {job_min}-{job_max}")
            
            # 1. æ£€æŸ¥æ˜¯å¦æœ‰é‡å 
            overlap_min = max(resume_min, job_min)
            overlap_max = min(resume_max, job_max)
            
            if overlap_max >= overlap_min:
                # æœ‰é‡å ï¼Œè®¡ç®—é‡å åº¦
                overlap_size = overlap_max - overlap_min
                resume_range_size = resume_max - resume_min
                job_range_size = job_max - job_min if job_max != float('inf') else resume_range_size
                
                if resume_range_size > 0 and job_range_size > 0:
                    overlap_ratio = overlap_size / min(resume_range_size, job_range_size)
                    score = min(1.0, overlap_ratio)
                    self.logger.debug(f"è–ªèµ„æœ‰é‡å ï¼Œé‡å åº¦: {overlap_ratio:.3f}, åˆ†æ•°: {score:.3f}")
                    return score
            
            # 2. æ²¡æœ‰é‡å ï¼Œä½†æ£€æŸ¥æ˜¯å¦åœ¨åˆç†èŒƒå›´å†…
            resume_mid = (resume_min + resume_max) / 2
            job_mid = (job_min + job_max) / 2 if job_max != float('inf') else job_min * 1.5
            
            # è®¡ç®—è–ªèµ„å·®è·æ¯”ä¾‹
            if job_mid > 0:
                gap_ratio = abs(resume_mid - job_mid) / job_mid
                
                # æ ¹æ®å·®è·ç»™åˆ†
                if gap_ratio <= 0.2:  # å·®è·åœ¨20%ä»¥å†…
                    score = 0.8
                elif gap_ratio <= 0.4:  # å·®è·åœ¨40%ä»¥å†…
                    score = 0.6
                elif gap_ratio <= 0.6:  # å·®è·åœ¨60%ä»¥å†…
                    score = 0.4
                else:  # å·®è·è¶…è¿‡60%
                    score = 0.2
                
                self.logger.debug(f"è–ªèµ„æ— é‡å ï¼Œå·®è·æ¯”ä¾‹: {gap_ratio:.3f}, åˆ†æ•°: {score:.3f}")
                return score
            
            # 3. ç‰¹æ®Šæƒ…å†µï¼šå¦‚æœç®€å†æœŸæœ›æ˜æ˜¾ä½äºèŒä½æä¾›ï¼Œç»™é«˜åˆ†
            if resume_max <= job_min * 1.2:  # ç®€å†æœ€é«˜æœŸæœ›ä¸è¶…è¿‡èŒä½æœ€ä½çš„120%
                self.logger.debug("ç®€å†æœŸæœ›è–ªèµ„åˆç†åä½ï¼Œç»™äºˆé«˜åˆ† 0.9")
                return 0.9
            
            # 4. é»˜è®¤æƒ…å†µ
            self.logger.debug("è–ªèµ„åŒ¹é…é»˜è®¤æƒ…å†µï¼Œè¿”å› 0.5")
            return 0.5
            
        except Exception as e:
            self.logger.error(f"è®¡ç®—è–ªèµ„åŒ¹é…åº¦å¤±è´¥: {str(e)}")
            return 0.0
    
    def _build_resume_text(self, resume_profile: GenericResumeProfile) -> str:
        """æ„å»ºç®€å†æ–‡æœ¬"""
        text_parts = []
        
        # åŸºæœ¬ä¿¡æ¯
        if resume_profile.current_position:
            text_parts.append(f"{resume_profile.current_position} {resume_profile.total_experience_years}å¹´ç»éªŒ")
        
        # æŠ€èƒ½
        all_skills = resume_profile.get_all_skills()
        if all_skills:
            text_parts.append(" ".join(all_skills))
        
        # å·¥ä½œç»å†
        for work_exp in resume_profile.work_history:
            text_parts.append(f"{work_exp.company} {work_exp.position} {work_exp.industry}")
            text_parts.extend(work_exp.responsibilities)
            text_parts.extend(work_exp.achievements)
            text_parts.extend(work_exp.technologies)
        
        # é¡¹ç›®ç»éªŒ
        for project in resume_profile.projects:
            text_parts.append(f"{project.name} {project.description}")
            text_parts.extend(project.technologies)
        
        # èŒä¸šç›®æ ‡
        text_parts.extend(resume_profile.career_objectives)
        text_parts.extend(resume_profile.preferred_positions)
        
        return " ".join(text_parts)
    
    def _extract_job_metadata(self, job_docs: List[Document], job_id: str) -> Dict[str, Any]:
        """æå–èŒä½å…ƒæ•°æ®"""
        metadata = {'job_id': job_id}
        
        for doc in job_docs:
            doc_metadata = doc.metadata
            
            for key in ['job_title', 'company', 'location', 'industry']:
                if key in doc_metadata:
                    metadata[key] = doc_metadata[key]
            
            if 'salary_min' in doc_metadata and 'salary_max' in doc_metadata:
                metadata['salary_range'] = {
                    'min': doc_metadata['salary_min'],
                    'max': doc_metadata['salary_max']
                }
            
            if 'skills' in doc_metadata:
                if 'skills' not in metadata:
                    metadata['skills'] = []
                metadata['skills'].extend(doc_metadata['skills'])
            
            if 'required_experience_years' in doc_metadata:
                metadata['required_experience_years'] = doc_metadata['required_experience_years']
        
        if 'skills' in metadata:
            metadata['skills'] = list(set(metadata['skills']))
        
        metadata['description'] = " ".join([doc.page_content for doc in job_docs])
        
        return metadata
    
    def _extract_job_skills(self, job_docs: List[Document], job_metadata: Dict[str, Any]) -> List[str]:
        """æå–èŒä½æŠ€èƒ½è¦æ±‚"""
        skills = []
        
        # ä»å…ƒæ•°æ®ä¸­è·å–æŠ€èƒ½
        if 'skills' in job_metadata and isinstance(job_metadata['skills'], list):
            skills.extend(job_metadata['skills'])
        
        # ä»æ–‡æ¡£å†…å®¹ä¸­æå–æŠ€èƒ½ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
        job_text = " ".join([doc.page_content for doc in job_docs]).lower()
        
        # æ‰©å±•çš„æŠ€èƒ½å…³é”®è¯ - åŒ…å«å å½¬ç®€å†çš„æ ¸å¿ƒæŠ€èƒ½
        common_skills = [
            # ç¼–ç¨‹è¯­è¨€
            'python', 'java', 'javascript', 'typescript', 'c#', 'c++', 'golang', 'go',
            
            # Webå‰ç«¯æŠ€æœ¯
            'react', 'vue', 'angular', 'node.js', 'html', 'css', 'bootstrap',
            'jquery', 'webpack', 'npm', 'yarn', 'sass', 'less',
            
            # åç«¯æ¡†æ¶
            'spring', 'django', 'flask', '.net', 'asp.net',
            
            # æ•°æ®åº“æŠ€æœ¯
            'mysql', 'postgresql', 'mongodb', 'redis', 'azure sql', 'cosmos db',
            'sql server', 'oracle', 'sqlite', 'cassandra', 'elasticsearch',
            
            # äº‘å¹³å°æŠ€æœ¯ (é‡ç‚¹æ‰©å±•Azure)
            'azure', 'microsoft azure', 'aws', 'gcp', 'google cloud',
            'azure data factory', 'azure functions', 'azure storage',
            'azure data lake storage', 'azure data lake storage gen2',
            'azure synapse', 'azure databricks', 'azure devops',
            'azure app service', 'azure kubernetes service', 'aks',
            
            # å¤§æ•°æ®å’Œæ•°æ®å·¥ç¨‹æŠ€èƒ½ (å å½¬çš„æ ¸å¿ƒé¢†åŸŸ)
            'databricks', 'delta lake', 'spark', 'pyspark', 'spark sql',
            'hadoop', 'hdfs', 'hive', 'kafka', 'airflow', 'nifi',
            'ssis', 'informatica', 'talend', 'pentaho',
            'etl', 'elt', 'data pipeline', 'data integration',
            'oltp', 'olap', 'data warehouse', 'data mart',
            'data lineage', 'data governance', 'data quality',
            'metadata management', 'data catalog',
            
            # AI/MLæŠ€èƒ½ (å å½¬çš„ä¸“é•¿)
            'machine learning', 'deep learning', 'ai', 'artificial intelligence',
            'tensorflow', 'pytorch', 'scikit-learn', 'keras', 'xgboost',
            'computer vision', 'opencv', 'yolo', 'resnet', 'cnn', 'rnn', 'lstm',
            'attention mechanism', 'transformer', 'bert', 'gpt',
            'numpy', 'pandas', 'matplotlib', 'seaborn', 'plotly',
            'jupyter', 'anaconda', 'mlflow', 'kubeflow',
            'langchain', 'llamaindex', 'openai api', 'azure openai',
            'rag', 'retrieval augmented generation', 'prompt engineering',
            
            # æ•°æ®ç§‘å­¦å’Œåˆ†æ
            'data science', 'data analysis', 'data visualization',
            'tableau', 'power bi', 'qlik', 'looker', 'grafana',
            'r', 'stata', 'spss', 'sas',
            
            # DevOpså’ŒåŸºç¡€è®¾æ–½
            'docker', 'kubernetes', 'jenkins', 'gitlab ci', 'github actions',
            'terraform', 'ansible', 'chef', 'puppet',
            'linux', 'ubuntu', 'centos', 'windows server',
            'nginx', 'apache', 'iis',
            
            # é¡¹ç›®ç®¡ç†å’Œæ–¹æ³•è®º
            'agile', 'scrum', 'kanban', 'waterfall', 'devops',
            'ci/cd', 'continuous integration', 'continuous deployment',
            'git', 'github', 'gitlab', 'bitbucket', 'svn',
            
            # æ¶æ„å’Œè®¾è®¡æ¨¡å¼
            'microservices', 'api', 'rest', 'graphql', 'soap',
            'event driven', 'message queue', 'rabbitmq', 'activemq',
            'design patterns', 'solid principles', 'clean architecture',
            
            # åˆ¶è¯å’ŒåŒ»ç–—è¡Œä¸šç‰¹å®šæŠ€èƒ½
            'pharmaceutical', 'clinical data', 'regulatory compliance',
            'gxp', 'fda', 'ich', 'clinical trials', 'pharmacovigilance',
            
            # ä¸­æ–‡æŠ€èƒ½å…³é”®è¯ (å å½¬ç®€å†ä¸­çš„ä¸­æ–‡æŠ€èƒ½)
            'æ•°æ®å·¥ç¨‹', 'æ•°æ®æ¶æ„', 'æ•°æ®æ²»ç†', 'æ•°æ®è´¨é‡', 'æ•°æ®è¡€ç¼˜',
            'å…ƒæ•°æ®ç®¡ç†', 'æœºå™¨å­¦ä¹ ', 'æ·±åº¦å­¦ä¹ ', 'è®¡ç®—æœºè§†è§‰',
            'äººå·¥æ™ºèƒ½', 'æ•°æ®ç§‘å­¦', 'å¤§æ•°æ®', 'äº‘è®¡ç®—',
            'æ•æ·å¼€å‘', 'é¡¹ç›®ç®¡ç†', 'æŠ€æœ¯ç®¡ç†', 'æ¶æ„è®¾è®¡',
            'æ¹–ä»“ä¸€ä½“', 'å®æ—¶å¤„ç†', 'æ‰¹å¤„ç†', 'æµå¤„ç†'
        ]
        
        # åªæ·»åŠ åœ¨èŒä½æ–‡æœ¬ä¸­æ‰¾åˆ°çš„æŠ€èƒ½å…³é”®è¯
        for skill in common_skills:
            if skill.lower() in job_text:
                skills.append(skill)
        
        # å»é‡å¹¶è¿‡æ»¤ç©ºå€¼
        unique_skills = []
        for skill in skills:
            if skill and skill.strip() and skill not in unique_skills:
                unique_skills.append(skill.lower().strip())
        
        return unique_skills
    
    def _is_skill_matched(self, job_skill: str, resume_skills: List[str]) -> bool:
        """åˆ¤æ–­æŠ€èƒ½æ˜¯å¦åŒ¹é… - å¢å¼ºç‰ˆæœ¬æ”¯æŒä¸­è‹±æ–‡æ˜ å°„å’Œæ™ºèƒ½åŒ¹é…"""
        job_skill_lower = job_skill.lower().strip()
        
        # 1. ç²¾ç¡®åŒ¹é…
        if job_skill_lower in resume_skills:
            return True
        
        # 2. ä¸­è‹±æ–‡æŠ€èƒ½æ˜ å°„
        skill_mappings = self._get_skill_mappings()
        
        # æ£€æŸ¥job_skillæ˜¯å¦æœ‰å¯¹åº”çš„ä¸­æ–‡æˆ–è‹±æ–‡æ˜ å°„
        for cn_skill, en_skills in skill_mappings.items():
            if job_skill_lower in [s.lower() for s in en_skills]:
                # job_skillæ˜¯è‹±æ–‡ï¼Œæ£€æŸ¥ç®€å†ä¸­æ˜¯å¦æœ‰å¯¹åº”ä¸­æ–‡
                if cn_skill in [s.lower() for s in resume_skills]:
                    return True
            elif job_skill_lower == cn_skill.lower():
                # job_skillæ˜¯ä¸­æ–‡ï¼Œæ£€æŸ¥ç®€å†ä¸­æ˜¯å¦æœ‰å¯¹åº”è‹±æ–‡
                for en_skill in en_skills:
                    if en_skill.lower() in resume_skills:
                        return True
        
        # 3. æŠ€èƒ½å˜ä½“åŒ¹é…
        skill_variants = self._get_skill_variants()
        for base_skill, variants in skill_variants.items():
            if job_skill_lower == base_skill.lower() or job_skill_lower in [v.lower() for v in variants]:
                # æ£€æŸ¥ç®€å†ä¸­æ˜¯å¦æœ‰ä»»ä½•å˜ä½“
                all_variants = [base_skill] + variants
                for variant in all_variants:
                    if variant.lower() in resume_skills:
                        return True
        
        # 4. éƒ¨åˆ†åŒ¹é… (æ›´æ™ºèƒ½çš„åŒ¹é…)
        for resume_skill in resume_skills:
            # é¿å…è¿‡çŸ­çš„åŒ¹é…
            if len(job_skill_lower) >= 3 and len(resume_skill) >= 3:
                if job_skill_lower in resume_skill or resume_skill in job_skill_lower:
                    return True
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯å¤åˆæŠ€èƒ½çš„ä¸€éƒ¨åˆ†
                if self._is_compound_skill_match(job_skill_lower, resume_skill):
                    return True
        
        return False
    
    def _get_skill_mappings(self) -> Dict[str, List[str]]:
        """è·å–ä¸­è‹±æ–‡æŠ€èƒ½æ˜ å°„"""
        return {
            # å å½¬ç®€å†ä¸­çš„æ ¸å¿ƒæŠ€èƒ½æ˜ å°„
            'æ•°æ®å·¥ç¨‹å¸ˆ': ['data engineer', 'data engineering', 'data architect', 'data architecture'],
            'ai/mlæ¶æ„å¸ˆ': ['ai architect', 'ml architect', 'ai/ml architect', 'machine learning architect', 'ai', 'artificial intelligence'],
            'æŠ€æœ¯ç®¡ç†è€…': ['technical lead', 'tech lead', 'technical manager', 'engineering manager'],
            'æ•°æ®å¹³å°æ¶æ„å¸ˆ': ['data platform architect', 'data architect', 'platform architect', 'data architecture'],
            'ç«¯åˆ°ç«¯aiç³»ç»Ÿå¼€å‘': ['end-to-end ai development', 'ai system development', 'ai pipeline', 'ai'],
            'ä¼ä¸šåˆ†å¸ƒå¼ç³»ç»Ÿå¼€å‘': ['distributed systems', 'enterprise systems', 'distributed computing'],
            'webäº’è”ç½‘æ¶æ„æ„å»º': ['web architecture', 'internet architecture', 'web system design'],
            'æŠ€æœ¯é¢†å¯¼': ['technical leadership', 'tech leadership', 'engineering leadership'],
            'æ•æ·å®è·µ': ['agile practices', 'agile methodology', 'agile development', 'agile'],
            'ä¼ä¸šçº§ç³»ç»Ÿæ¶æ„è®¾è®¡': ['enterprise architecture', 'system architecture', 'solution architecture', 'data architecture'],
            'é¡¹ç›®é£é™©ç®¡ç†': ['project risk management', 'risk management', 'project management'],
            'æŠ€æœ¯å€ºåŠ¡æ§åˆ¶': ['technical debt management', 'code quality', 'technical debt'],
            
            # æ•°æ®å·¥ç¨‹æŠ€èƒ½
            'æ•°æ®è¡€ç¼˜è¿½è¸ª': ['data lineage', 'data lineage tracking', 'lineage management'],
            'å…ƒæ•°æ®ç®¡ç†': ['metadata management', 'data catalog', 'metadata governance'],
            'æ•°æ®æ²»ç†': ['data governance', 'data management', 'data stewardship'],
            'æ•°æ®è´¨é‡æ ¡éªŒ': ['data quality', 'data validation', 'data quality assurance'],
            'ssis': ['ssis', 'sql server integration services', 'etl', 'data integration'],
            'informatica': ['informatica', 'etl', 'data integration'],
            'databricks': ['databricks', 'delta lake', 'lakehouse'],
            
            # AI/MLæŠ€èƒ½
            'æœºå™¨å­¦ä¹ ': ['machine learning', 'ml', 'ai'],
            'æ·±åº¦å­¦ä¹ ': ['deep learning', 'dl', 'ai'],
            'è®¡ç®—æœºè§†è§‰': ['computer vision', 'cv'],
            'äººä½“å…³é”®ç‚¹æ£€æµ‹': ['human pose estimation', 'keypoint detection', 'pose detection'],
            'æ³¨æ„åŠ›æœºåˆ¶': ['attention mechanism', 'attention', 'self-attention'],
            'æ¨ç†åŠ é€Ÿ': ['inference acceleration', 'model optimization', 'inference optimization'],
            'å†…å­˜ä¼˜åŒ–': ['memory optimization', 'memory management'],
            'æ‰¹é‡é¢„æµ‹': ['batch prediction', 'batch inference'],
            'æ¨¡å‹éƒ¨ç½²': ['model deployment', 'model serving', 'ml deployment'],
            'ragæ¶æ„': ['rag', 'retrieval augmented generation', 'rag architecture'],
            'æç¤ºå·¥ç¨‹': ['prompt engineering', 'prompt design', 'prompt optimization'],
            
            # ç®¡ç†æŠ€èƒ½
            'æŠ€æœ¯å›¢é˜Ÿç®¡ç†': ['technical team management', 'engineering team lead', 'team leadership'],
            'æ•æ·å¼€å‘å®è·µè€…': ['agile practitioner', 'agile coach', 'scrum practitioner', 'agile'],
            'æ¶æ„è®¾è®¡': ['architecture design', 'system design', 'solution design', 'data architecture'],
            'é£é™©æ§åˆ¶': ['risk management', 'risk control', 'risk mitigation'],
            'è‡ªåŠ¨åŒ–éƒ¨ç½²': ['automated deployment', 'deployment automation', 'ci/cd'],
            'ç‰ˆæœ¬ç®¡ç†': ['version control', 'source control', 'git management'],
            'å›¢é˜Ÿå»ºè®¾': ['team building', 'team development', 'team management'],
            'é¡¹ç›®ç®¡ç†': ['project management', 'program management'],
            'scrumæ–¹æ³•è®º': ['scrum', 'agile', 'scrum master'],
            
            # äº‘å¹³å°å’Œæ•°æ®å¹³å°
            'æ¹–ä»“ä¸€ä½“': ['lakehouse', 'data lakehouse', 'lakehouse architecture', 'delta lake'],
            'å®æ—¶å¤„ç†': ['real-time processing', 'stream processing', 'real-time analytics'],
            'æ‰¹å¤„ç†': ['batch processing', 'batch analytics'],
            'æµå¤„ç†': ['stream processing', 'streaming', 'real-time streaming'],
            
            # åˆ¶è¯è¡Œä¸šç»éªŒ
            'zoetis': ['pharmaceutical', 'pharma', 'healthcare'],
            'pwc': ['consulting', 'pharmaceutical']
        }
    
    def _get_skill_variants(self) -> Dict[str, List[str]]:
        """è·å–æŠ€èƒ½å˜ä½“æ˜ å°„"""
        return {
            'azure': ['microsoft azure', 'azure cloud', 'azure platform'],
            'databricks': ['databricks platform', 'databricks workspace', 'databricks runtime'],
            'azure data factory': ['adf', 'data factory', 'azure adf'],
            'azure functions': ['azure function', 'function app', 'serverless functions'],
            'machine learning': ['ml', 'ai/ml', 'artificial intelligence'],
            'deep learning': ['dl', 'neural networks', 'deep neural networks'],
            'computer vision': ['cv', 'image processing', 'visual recognition'],
            'data engineering': ['data engineer', 'big data engineering'],
            'data architecture': ['data architect', 'data platform architecture'],
            'etl': ['extract transform load', 'data integration', 'data pipeline'],
            'oltp': ['online transaction processing', 'transactional database'],
            'olap': ['online analytical processing', 'analytical database'],
            'ci/cd': ['continuous integration', 'continuous deployment', 'devops pipeline'],
            'scrum master': ['scrum', 'agile coach', 'agile facilitator'],
            'pytorch': ['torch', 'pytorch framework'],
            'tensorflow': ['tf', 'tensorflow framework'],
            'kubernetes': ['k8s', 'container orchestration'],
            'docker': ['containerization', 'container technology'],
            'rest api': ['rest', 'restful api', 'web api'],
            'microservices': ['micro services', 'service oriented architecture', 'soa']
        }
    
    def _is_compound_skill_match(self, job_skill: str, resume_skill: str) -> bool:
        """æ£€æŸ¥å¤åˆæŠ€èƒ½åŒ¹é…"""
        # å¤„ç†å¤åˆæŠ€èƒ½ï¼Œå¦‚ "azure data factory" åŒ¹é… "data factory"
        job_words = set(job_skill.split())
        resume_words = set(resume_skill.split())
        
        # å¦‚æœæœ‰è¶³å¤Ÿçš„è¯æ±‡é‡å ï¼Œè®¤ä¸ºåŒ¹é…
        if len(job_words) > 1 and len(resume_words) > 1:
            overlap = job_words.intersection(resume_words)
            # è‡³å°‘æœ‰2ä¸ªè¯åŒ¹é…ï¼Œä¸”åŒ¹é…ç‡è¶…è¿‡50%
            if len(overlap) >= 2 and len(overlap) / min(len(job_words), len(resume_words)) >= 0.5:
                return True
        
        return False
    
    def _calculate_skill_bonus(self, resume_skills: List[str], job_skills: List[str]) -> float:
        """è®¡ç®—æŠ€èƒ½åŠ åˆ† - æ‰©å±•å å½¬çš„é«˜ä»·å€¼æŠ€èƒ½"""
        bonus = 0.0
        
        # æ‰©å±•çš„é«˜ä»·å€¼æŠ€èƒ½åŠ åˆ† - é‡ç‚¹åŒ…å«å å½¬çš„æ ¸å¿ƒæŠ€èƒ½
        high_value_skills = [
            # AI/MLæ ¸å¿ƒæŠ€èƒ½
            'ai', 'machine learning', 'deep learning', 'computer vision',
            'pytorch', 'tensorflow', 'rag', 'langchain', 'llamaindex',
            'prompt engineering', 'attention mechanism',
            
            # æ•°æ®å·¥ç¨‹å’Œæ¶æ„ (å å½¬çš„æ ¸å¿ƒä¼˜åŠ¿)
            'databricks', 'delta lake', 'data architecture', 'data governance',
            'data lineage', 'metadata management', 'lakehouse',
            'azure data factory', 'pyspark', 'etl', 'data pipeline',
            
            # äº‘å¹³å°æŠ€èƒ½
            'azure', 'aws', 'kubernetes', 'azure databricks',
            'azure functions', 'azure synapse', 'azure data lake storage',
            
            # æ¶æ„å’Œç®¡ç†æŠ€èƒ½
            'solution architecture', 'enterprise architecture', 'scrum master',
            'technical leadership', 'data platform architect',
            
            # ä¸­æ–‡é«˜ä»·å€¼æŠ€èƒ½
            'æ•°æ®æ¶æ„', 'æ•°æ®æ²»ç†', 'æ¹–ä»“ä¸€ä½“', 'ai/mlæ¶æ„å¸ˆ',
            'æ•°æ®å¹³å°æ¶æ„å¸ˆ', 'æŠ€æœ¯ç®¡ç†è€…', 'æ•°æ®å·¥ç¨‹å¸ˆ'
        ]
        
        for skill in resume_skills:
            skill_lower = skill.lower()
            if skill_lower in high_value_skills and skill_lower not in job_skills:
                # æ ¹æ®æŠ€èƒ½é‡è¦æ€§ç»™ä¸åŒåŠ åˆ†
                if skill_lower in ['databricks', 'data architecture', 'æ•°æ®æ¶æ„', 'azure data factory']:
                    bonus += 0.08  # å å½¬çš„æœ€æ ¸å¿ƒæŠ€èƒ½ï¼Œæ›´é«˜åŠ åˆ†
                elif skill_lower in ['ai', 'machine learning', 'azure', 'rag']:
                    bonus += 0.06  # é‡è¦æŠ€èƒ½
                else:
                    bonus += 0.04  # ä¸€èˆ¬é«˜ä»·å€¼æŠ€èƒ½
        
        return min(0.25, bonus)  # æé«˜æœ€å¤§åŠ åˆ†åˆ°25%
    
    def _extract_required_experience(self, job_metadata: Dict[str, Any]) -> Optional[int]:
        """æå–èŒä½ç»éªŒè¦æ±‚"""
        if 'required_experience_years' in job_metadata:
            return job_metadata['required_experience_years']
        
        # ä»èŒä½æè¿°ä¸­æå–ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
        job_description = job_metadata.get('description', '').lower()
        
        import re
        patterns = [
            r'(\d+)\+?\s*years?\s*of?\s*experience',
            r'(\d+)\+?\s*å¹´.*ç»éªŒ',
            r'experience.*(\d+)\+?\s*years?',
            r'minimum.*(\d+)\s*years?'
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, job_description)
            if matches:
                return int(matches[0])
        
        return None
    
    def _calculate_industry_similarity(self, job_industry: str, resume_industries: List[str]) -> float:
        """è®¡ç®—è¡Œä¸šç›¸ä¼¼åº¦"""
        # ç®€åŒ–çš„è¡Œä¸šç›¸å…³æ€§
        industry_relations = {
            'ç§‘æŠ€': ['äº’è”ç½‘', 'ai', 'è½¯ä»¶', 'æŠ€æœ¯'],
            'åˆ¶è¯': ['åŒ»ç–—', 'ç”Ÿç‰©', 'å¥åº·'],
            'é‡‘è': ['é“¶è¡Œ', 'ä¿é™©', 'æŠ•èµ„'],
            'å’¨è¯¢': ['ç®¡ç†', 'æˆ˜ç•¥', 'é¡¾é—®']
        }
        
        max_similarity = 0.0
        
        for resume_industry in resume_industries:
            if job_industry.lower() in resume_industry.lower() or resume_industry.lower() in job_industry.lower():
                max_similarity = max(max_similarity, 0.8)
            
            for category, related_terms in industry_relations.items():
                if any(term in job_industry.lower() for term in related_terms):
                    if any(term in resume_industry.lower() for term in related_terms):
                        max_similarity = max(max_similarity, 0.6)
        
        return max_similarity
    
    def _calculate_confidence_level(self, dimension_scores: Dict[str, float]) -> float:
        """è®¡ç®—ç½®ä¿¡åº¦"""
        import numpy as np
        scores = list(dimension_scores.values())
        mean_score = np.mean(scores)
        variance = np.var(scores)
        
        confidence = max(0.5, 1.0 - variance)
        return confidence
    
    def _get_match_level_from_score(self, score: float) -> MatchLevel:
        """æ ¹æ®åˆ†æ•°è·å–åŒ¹é…ç­‰çº§"""
        if score >= 0.85:
            return MatchLevel.EXCELLENT
        elif score >= 0.70:
            return MatchLevel.GOOD
        elif score >= 0.50:
            return MatchLevel.FAIR
        else:
            return MatchLevel.POOR
    
    def _get_recommendation_priority_from_score(self, score: float) -> RecommendationPriority:
        """æ ¹æ®åˆ†æ•°è·å–æ¨èä¼˜å…ˆçº§"""
        if score >= 0.85:
            return RecommendationPriority.HIGH
        elif score >= 0.70:
            return RecommendationPriority.MEDIUM
        elif score >= 0.50:
            return RecommendationPriority.LOW
        else:
            return RecommendationPriority.NOT_RECOMMENDED
    
    def _generate_match_analysis(self, 
                               resume_profile: GenericResumeProfile,
                               job_docs: List[Document],
                               job_metadata: Dict[str, Any],
                               dimension_scores: Dict[str, float]) -> MatchAnalysis:
        """ç”ŸæˆåŒ¹é…åˆ†æ"""
        analysis = MatchAnalysis()
        
        # åˆ†æä¼˜åŠ¿
        if dimension_scores['skills_match'] >= 0.8:
            analysis.strengths.append("æŠ€èƒ½åŒ¹é…åº¦æé«˜")
        elif dimension_scores['skills_match'] >= 0.6:
            analysis.strengths.append("æŠ€èƒ½åŒ¹é…åº¦è‰¯å¥½")
        
        if dimension_scores['experience_match'] >= 0.9:
            analysis.strengths.append("ç»éªŒå®Œå…¨ç¬¦åˆè¦æ±‚")
        elif dimension_scores['experience_match'] >= 0.7:
            analysis.strengths.append("ç»éªŒåŸºæœ¬ç¬¦åˆè¦æ±‚")
        
        if dimension_scores['salary_match'] >= 0.8:
            analysis.strengths.append("è–ªèµ„æœŸæœ›åˆç†")
        
        # åˆ†æåŠ£åŠ¿
        if dimension_scores['skills_match'] < 0.5:
            analysis.weaknesses.append("éƒ¨åˆ†å…³é”®æŠ€èƒ½éœ€è¦æå‡")
        
        if dimension_scores['industry_match'] < 0.6:
            analysis.weaknesses.append("è¡Œä¸šè½¬æ¢éœ€è¦é€‚åº”")
        
        if dimension_scores['semantic_similarity'] < 0.5:
            analysis.weaknesses.append("èŒä½æè¿°åŒ¹é…åº¦æœ‰å¾…æé«˜")
        
        # ç”Ÿæˆå»ºè®®
        if dimension_scores['skills_match'] < 0.7:
            analysis.recommendations.append("é‡ç‚¹çªå‡ºç›¸å…³æŠ€èƒ½å’Œé¡¹ç›®ç»éªŒ")
        
        if dimension_scores['industry_match'] < 0.6:
            analysis.recommendations.append("å‡†å¤‡è¡Œä¸šè½¬æ¢è¯´æ˜å’Œå­¦ä¹ è®¡åˆ’")
        
        # æå–åŒ¹é…å’Œç¼ºå¤±çš„æŠ€èƒ½
        resume_skills = [skill.lower().strip() for skill in resume_profile.get_all_skills() if skill and skill.strip()]
        job_skills = self._extract_job_skills(job_docs, job_metadata)
        
        # ç¡®ä¿job_skillsæ˜¯æœ‰æ•ˆçš„æŠ€èƒ½åˆ—è¡¨ï¼Œè¿‡æ»¤æ‰å•ä¸ªå­—ç¬¦å’Œæ— æ•ˆæŠ€èƒ½
        valid_job_skills = [skill for skill in job_skills if skill and len(skill.strip()) > 1 and skill.strip().isalpha()]
        
        for job_skill in valid_job_skills:
            if self._is_skill_matched(job_skill, resume_skills):
                analysis.matched_skills.append(job_skill)
            else:
                analysis.missing_skills.append(job_skill)
        
        # è®¡ç®—å„é¡¹æŒ‡æ ‡
        analysis.skill_gap_score = 1.0 - dimension_scores['skills_match']
        analysis.experience_alignment = dimension_scores['experience_match']
        analysis.industry_fit = dimension_scores['industry_match']
        
        return analysis
    
    def _generate_matching_summary(self, 
                                 matches: List[JobMatchResult],
                                 processing_time: float) -> MatchingSummary:
        """ç”ŸæˆåŒ¹é…ç»“æœæ‘˜è¦"""
        summary = MatchingSummary()
        
        summary.total_matches = len(matches)
        summary.processing_time = processing_time
        
        if matches:
            summary.average_score = sum(match.overall_score for match in matches) / len(matches)
            
            for match in matches:
                if match.recommendation_priority == RecommendationPriority.HIGH:
                    summary.high_priority += 1
                elif match.recommendation_priority == RecommendationPriority.MEDIUM:
                    summary.medium_priority += 1
                elif match.recommendation_priority == RecommendationPriority.LOW:
                    summary.low_priority += 1
        
        return summary
    
    def _generate_career_insights(self, 
                                matches: List[JobMatchResult],
                                resume_profile: GenericResumeProfile) -> CareerInsights:
        """ç”ŸæˆèŒä¸šæ´å¯Ÿåˆ†æ"""
        insights = CareerInsights()
        
        if not matches:
            return insights
        
        # åˆ†æçƒ­é—¨èŒä½
        position_counts = defaultdict(int)
        for match in matches:
            position_counts[match.job_title] += 1
        
        insights.top_matching_positions = [
            pos for pos, count in sorted(position_counts.items(),
                                       key=lambda x: x[1], reverse=True)[:5]
        ]
        
        # æŠ€èƒ½å·®è·åˆ†æ
        all_missing_skills = []
        high_demand_skills = defaultdict(int)
        
        for match in matches:
            all_missing_skills.extend(match.match_analysis.missing_skills)
            
            if match.overall_score >= 0.7:
                for skill in match.match_analysis.missing_skills:
                    high_demand_skills[skill] += 1
        
        insights.skill_gap_analysis = {
            'high_demand_missing': [
                skill for skill, count in sorted(high_demand_skills.items(),
                                                key=lambda x: x[1], reverse=True)[:5]
            ],
            'emerging_skills': ['AI/ML', 'Cloud Native', 'DevOps', 'Microservices', 'Data Science']
        }
        
        # è–ªèµ„åˆ†æ
        salary_ranges = []
        for match in matches:
            if match.salary_range:
                salary_ranges.append(match.salary_range)
        
        if salary_ranges:
            min_salaries = [sr['min'] for sr in salary_ranges if 'min' in sr]
            max_salaries = [sr['max'] for sr in salary_ranges if 'max' in sr]
            
            if min_salaries and max_salaries:
                market_min = min(min_salaries)
                market_max = max(max_salaries)
                
                insights.salary_analysis = {
                    'market_range': {'min': market_min, 'max': market_max},
                    'your_range': resume_profile.expected_salary_range,
                    'market_position': self._analyze_salary_position(
                        resume_profile.expected_salary_range, market_min, market_max
                    )
                }
        
        # å¸‚åœºè¶‹åŠ¿
        insights.market_trends = [
            "AI/MLæŠ€èƒ½éœ€æ±‚æŒç»­å¢é•¿",
            "äº‘å¹³å°æŠ€èƒ½æˆä¸ºæ ‡é…",
            "å…¨æ ˆå¼€å‘èƒ½åŠ›å—æ¬¢è¿",
            "è½¯æŠ€èƒ½è¶Šæ¥è¶Šé‡è¦"
        ]
        
        # èŒä¸šå»ºè®®
        insights.career_recommendations = self._generate_career_recommendations(
            matches, resume_profile
        )
        
        return insights
    
    def _analyze_salary_position(self,
                               expected_range: Dict[str, int],
                               market_min: int,
                               market_max: int) -> str:
        """åˆ†æè–ªèµ„å¸‚åœºå®šä½"""
        if not expected_range or expected_range.get('min', 0) == 0:
            return "æœªè®¾å®šæœŸæœ›è–ªèµ„"
        
        expected_mid = (expected_range['min'] + expected_range['max']) / 2
        market_mid = (market_min + market_max) / 2
        
        if expected_mid >= market_mid * 1.2:
            return "é«˜äºå¸‚åœºæ°´å¹³"
        elif expected_mid >= market_mid * 0.8:
            return "ç¬¦åˆå¸‚åœºæ°´å¹³"
        else:
            return "ä½äºå¸‚åœºæ°´å¹³"
    
    def _generate_career_recommendations(self,
                                       matches: List[JobMatchResult],
                                       resume_profile: GenericResumeProfile) -> List[str]:
        """ç”ŸæˆèŒä¸šå»ºè®®"""
        recommendations = []
        
        high_score_matches = [m for m in matches if m.overall_score >= 0.8]
        
        if len(high_score_matches) >= 5:
            recommendations.append("æ‚¨çš„èƒŒæ™¯éå¸¸ç¬¦åˆå¸‚åœºéœ€æ±‚ï¼Œå¯ä»¥ç§¯æç”³è¯·é«˜åŒ¹é…åº¦èŒä½")
        elif len(high_score_matches) >= 2:
            recommendations.append("æœ‰å‡ ä¸ªé«˜åŒ¹é…åº¦èŒä½ï¼Œå»ºè®®é‡ç‚¹å…³æ³¨å’Œå‡†å¤‡")
        else:
            recommendations.append("å»ºè®®æå‡å…³é”®æŠ€èƒ½ä»¥å¢åŠ åŒ¹é…åº¦")
        
        # åŸºäºæŠ€èƒ½å·®è·çš„å»ºè®®
        common_missing_skills = defaultdict(int)
        for match in matches[:10]:
            for skill in match.match_analysis.missing_skills:
                common_missing_skills[skill] += 1
        
        if common_missing_skills:
            top_missing = max(common_missing_skills.items(), key=lambda x: x[1])
            recommendations.append(f"å»ºè®®é‡ç‚¹å­¦ä¹  {top_missing[0]} æŠ€èƒ½")
        
        # åŸºäºè¡Œä¸šç»éªŒçš„å»ºè®®
        avg_industry_score = sum(m.dimension_scores.get('industry_match', 0) for m in matches) / len(matches)
        if avg_industry_score < 0.6:
            recommendations.append("è€ƒè™‘å‡†å¤‡è¡Œä¸šè½¬æ¢çš„è¯´æ˜å’Œå­¦ä¹ è®¡åˆ’")
        
        return recommendations
    
    def _update_performance_stats(self, processing_time: float, results_count: int):
        """æ›´æ–°æ€§èƒ½ç»Ÿè®¡"""
        self.performance_stats['total_matches'] += results_count
        
        current_avg = self.performance_stats['average_processing_time']
        total_operations = self.performance_stats.get('total_operations', 0) + 1
        
        new_avg = (current_avg * (total_operations - 1) + processing_time) / total_operations
        self.performance_stats['average_processing_time'] = new_avg
        self.performance_stats['total_operations'] = total_operations
    
    def get_performance_stats(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯"""
        return self.performance_stats.copy()
    
    def reset_performance_stats(self):
        """é‡ç½®æ€§èƒ½ç»Ÿè®¡"""
        self.performance_stats = {
            'total_matches': 0,
            'average_processing_time': 0.0,
            'cache_hits': 0,
            'total_operations': 0
        }
    
    def _is_job_available(self, job_id: str) -> bool:
        """æ£€æŸ¥èŒä½æ˜¯å¦å¯ç”¨ï¼ˆæœªåˆ é™¤ï¼‰"""
        try:
            from ..database.operations import DatabaseManager
            
            # ä»é…ç½®ä¸­è·å–æ•°æ®åº“è·¯å¾„
            db_path = self.config.get('database_path', 'data/jobs.db')
            db_manager = DatabaseManager(db_path)
            
            with db_manager.get_connection() as conn:
                cursor = conn.cursor()
                cursor.execute("""
                    SELECT 1 FROM jobs
                    WHERE job_id = ? AND (is_deleted = 0 OR is_deleted IS NULL)
                """, (job_id,))
                return cursor.fetchone() is not None
        except Exception as e:
            self.logger.warning(f"æ£€æŸ¥èŒä½å¯ç”¨æ€§å¤±è´¥: {e}")
            return True  # å‡ºé”™æ—¶é»˜è®¤å¯ç”¨ï¼Œé¿å…è¯¯åˆ 