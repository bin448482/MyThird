#!/usr/bin/env python3
"""
RAGç³»ç»Ÿå®Œæ•´åŠŸèƒ½æµ‹è¯•

ç»¼åˆæµ‹è¯•RAGç³»ç»Ÿçš„æ‰€æœ‰æ ¸å¿ƒåŠŸèƒ½ï¼ŒåŒ…æ‹¬æ•°æ®å¤„ç†ã€å‘é‡å­˜å‚¨ã€ç®€å†ä¼˜åŒ–ç­‰
"""

import sys
import asyncio
import logging
import json
from pathlib import Path
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(str(Path(__file__).parent))

from src.rag.rag_system_coordinator import RAGSystemCoordinator
from src.rag.data_pipeline import RAGDataPipeline
from src.rag.resume_optimizer import ResumeOptimizer

class RAGSystemTester:
    """RAGç³»ç»Ÿæµ‹è¯•å™¨"""
    
    def __init__(self):
        self.test_results = {
            'total_tests': 0,
            'passed_tests': 0,
            'failed_tests': 0,
            'test_details': []
        }
        
        # æµ‹è¯•é…ç½®
        self.config = {
            'rag_system': {
                'database': {
                    'path': './data/jobs.db',
                    'batch_size': 10
                },
                'llm': {
                    'provider': 'zhipu',
                    'model': 'glm-4-flash',
                    'api_key': 'test-key',
                    'temperature': 0.1,
                    'max_tokens': 1500
                },
                'vector_db': {
                    'persist_directory': './data/test_chroma_db_complete',
                    'collection_name': 'test_complete_jobs',
                    'embeddings': {
                        'model_name': 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',
                        'device': 'cpu',
                        'normalize_embeddings': True
                    }
                },
                'documents': {
                    'types': ['overview', 'responsibility', 'requirement', 'skills', 'basic_requirements']
                },
                'processing': {
                    'skip_processed': True,
                    'batch_size': 5
                }
            }
        }
        
        # æµ‹è¯•ç®€å†æ•°æ®
        self.test_resume = {
            'name': 'æå››',
            'current_position': 'Pythonåç«¯å¼€å‘å·¥ç¨‹å¸ˆ',
            'years_of_experience': 5,
            'education': 'ç¡•å£« - è½¯ä»¶å·¥ç¨‹',
            'skills': ['Python', 'Django', 'Flask', 'PostgreSQL', 'Redis', 'Docker', 'AWS'],
            'summary': 'å…·æœ‰5å¹´Pythonåç«¯å¼€å‘ç»éªŒï¼Œç†Ÿæ‚‰å¾®æœåŠ¡æ¶æ„ï¼Œæœ‰ä¸°å¯Œçš„äº‘å¹³å°éƒ¨ç½²ç»éªŒã€‚',
            'experience': [
                {
                    'position': 'Pythonåç«¯å¼€å‘å·¥ç¨‹å¸ˆ',
                    'company': 'TechCorpç§‘æŠ€å…¬å¸',
                    'duration': '2020-2024',
                    'description': 'è´Ÿè´£å¾®æœåŠ¡æ¶æ„è®¾è®¡ï¼ŒAPIå¼€å‘ï¼Œæ•°æ®åº“ä¼˜åŒ–ï¼Œäº‘å¹³å°éƒ¨ç½²'
                }
            ]
        }
    
    def log_test_result(self, test_name: str, passed: bool, details: str = ""):
        """è®°å½•æµ‹è¯•ç»“æœ"""
        self.test_results['total_tests'] += 1
        if passed:
            self.test_results['passed_tests'] += 1
            status = "âœ… PASS"
        else:
            self.test_results['failed_tests'] += 1
            status = "âŒ FAIL"
        
        result = {
            'test_name': test_name,
            'status': status,
            'passed': passed,
            'details': details,
            'timestamp': datetime.now().isoformat()
        }
        
        self.test_results['test_details'].append(result)
        print(f"{status} - {test_name}")
        if details:
            print(f"      {details}")
    
    async def test_system_initialization(self):
        """æµ‹è¯•ç³»ç»Ÿåˆå§‹åŒ–"""
        print("\nğŸ”§ æµ‹è¯•1: ç³»ç»Ÿåˆå§‹åŒ–")
        
        try:
            coordinator = RAGSystemCoordinator(self.config)
            init_success = coordinator.initialize_system()
            
            if init_success:
                # æ£€æŸ¥ç»„ä»¶çŠ¶æ€
                system_status = coordinator.get_system_status()
                components = system_status.get('components', {})
                
                all_components_ok = all(components.values())
                
                self.log_test_result(
                    "ç³»ç»Ÿåˆå§‹åŒ–",
                    all_components_ok,
                    f"ç»„ä»¶çŠ¶æ€: {components}"
                )
                
                return coordinator
            else:
                self.log_test_result("ç³»ç»Ÿåˆå§‹åŒ–", False, "åˆå§‹åŒ–å¤±è´¥")
                return None
                
        except Exception as e:
            self.log_test_result("ç³»ç»Ÿåˆå§‹åŒ–", False, f"å¼‚å¸¸: {e}")
            return None
    
    async def test_database_operations(self, coordinator):
        """æµ‹è¯•æ•°æ®åº“æ“ä½œ"""
        print("\nğŸ“Š æµ‹è¯•2: æ•°æ®åº“æ“ä½œ")
        
        try:
            # æµ‹è¯•æ•°æ®è¯»å–
            db_reader = coordinator.db_reader
            
            # è·å–ç»Ÿè®¡ä¿¡æ¯
            stats = db_reader.get_rag_processing_stats()
            stats_ok = stats.get('total', 0) > 0
            
            self.log_test_result(
                "æ•°æ®åº“ç»Ÿè®¡æŸ¥è¯¢",
                stats_ok,
                f"æ€»èŒä½æ•°: {stats.get('total', 0)}"
            )
            
            # æµ‹è¯•èŒä½è¯»å–
            jobs = db_reader.get_jobs_for_rag_processing(limit=2)
            jobs_ok = len(jobs) > 0
            
            self.log_test_result(
                "èŒä½æ•°æ®è¯»å–",
                jobs_ok,
                f"è¯»å–åˆ° {len(jobs)} ä¸ªèŒä½"
            )
            
            # æµ‹è¯•æ•°æ®è´¨é‡æŠ¥å‘Š
            quality_report = db_reader.get_data_quality_report()
            quality_ok = quality_report.get('total_jobs', 0) > 0
            
            self.log_test_result(
                "æ•°æ®è´¨é‡æŠ¥å‘Š",
                quality_ok,
                f"æ•°æ®è´¨é‡è¯„åˆ†: {quality_report.get('data_quality_score', 0)}"
            )
            
            return jobs
            
        except Exception as e:
            self.log_test_result("æ•°æ®åº“æ“ä½œ", False, f"å¼‚å¸¸: {e}")
            return []
    
    async def test_job_processing(self, coordinator, test_jobs):
        """æµ‹è¯•èŒä½å¤„ç†"""
        print("\nğŸ”„ æµ‹è¯•3: èŒä½å¤„ç†")
        
        if not test_jobs:
            self.log_test_result("èŒä½å¤„ç†", False, "æ²¡æœ‰æµ‹è¯•æ•°æ®")
            return
        
        try:
            job_processor = coordinator.job_processor
            test_job = test_jobs[0]
            
            # æµ‹è¯•å¤‡ç”¨æå–ï¼ˆä¸è°ƒç”¨LLMï¼‰
            job_structure = job_processor._fallback_extraction_from_db(test_job)
            
            structure_ok = (
                job_structure.job_title and 
                job_structure.company
            )
            
            self.log_test_result(
                "èŒä½ç»“æ„åŒ–å¤„ç†",
                structure_ok,
                f"èŒä½: {job_structure.job_title} - {job_structure.company}"
            )
            
            # æµ‹è¯•æ–‡æ¡£åˆ›å»º
            documents = job_processor.create_documents(
                job_structure,
                job_id=test_job.get('job_id'),
                job_url=test_job.get('url')
            )
            
            docs_ok = len(documents) > 0
            
            self.log_test_result(
                "æ–‡æ¡£åˆ›å»º",
                docs_ok,
                f"åˆ›å»ºäº† {len(documents)} ä¸ªæ–‡æ¡£"
            )
            
            # æµ‹è¯•ç»“æ„éªŒè¯
            is_valid = job_processor.validate_job_structure(job_structure)
            
            self.log_test_result(
                "ç»“æ„éªŒè¯",
                True,  # å¤‡ç”¨æ–¹æ¡ˆæ€»æ˜¯é€šè¿‡åŸºæœ¬éªŒè¯
                f"ç»“æ„æœ‰æ•ˆæ€§: {is_valid}"
            )
            
            return job_structure, documents
            
        except Exception as e:
            self.log_test_result("èŒä½å¤„ç†", False, f"å¼‚å¸¸: {e}")
            return None, []
    
    async def test_vector_operations(self, coordinator, documents):
        """æµ‹è¯•å‘é‡æ“ä½œ"""
        print("\nğŸ” æµ‹è¯•4: å‘é‡æ“ä½œ")
        
        if not documents:
            self.log_test_result("å‘é‡æ“ä½œ", False, "æ²¡æœ‰æ–‡æ¡£æ•°æ®")
            return
        
        try:
            vector_manager = coordinator.vector_manager
            
            # æµ‹è¯•æ–‡æ¡£æ·»åŠ 
            doc_ids = vector_manager.add_job_documents(documents, job_id="test_job_001")
            
            add_ok = len(doc_ids) == len(documents)
            
            self.log_test_result(
                "å‘é‡æ–‡æ¡£æ·»åŠ ",
                add_ok,
                f"æ·»åŠ äº† {len(doc_ids)} ä¸ªæ–‡æ¡£"
            )
            
            # æµ‹è¯•é›†åˆç»Ÿè®¡
            stats = vector_manager.get_collection_stats()
            stats_ok = stats.get('document_count', 0) > 0
            
            self.log_test_result(
                "å‘é‡é›†åˆç»Ÿè®¡",
                stats_ok,
                f"æ–‡æ¡£æ•°é‡: {stats.get('document_count', 0)}"
            )
            
            # æµ‹è¯•ç›¸ä¼¼æ€§æœç´¢
            query = "Pythonå¼€å‘å·¥ç¨‹å¸ˆ"
            similar_docs = vector_manager.search_similar_jobs(query, k=3)
            
            search_ok = len(similar_docs) > 0
            
            self.log_test_result(
                "å‘é‡ç›¸ä¼¼æ€§æœç´¢",
                search_ok,
                f"æœç´¢åˆ° {len(similar_docs)} ä¸ªç›¸ä¼¼æ–‡æ¡£"
            )
            
            return True
            
        except Exception as e:
            self.log_test_result("å‘é‡æ“ä½œ", False, f"å¼‚å¸¸: {e}")
            return False
    
    async def test_resume_optimizer(self, coordinator, test_jobs):
        """æµ‹è¯•ç®€å†ä¼˜åŒ–å™¨"""
        print("\nğŸ“ æµ‹è¯•5: ç®€å†ä¼˜åŒ–å™¨")
        
        if not test_jobs:
            self.log_test_result("ç®€å†ä¼˜åŒ–å™¨", False, "æ²¡æœ‰æµ‹è¯•èŒä½")
            return
        
        try:
            optimizer = ResumeOptimizer(coordinator, self.config['rag_system']['llm'])
            
            # æµ‹è¯•æ•°æ®æ ¼å¼åŒ–
            formatted_resume = optimizer._format_resume_info(self.test_resume)
            format_ok = len(formatted_resume) > 0
            
            self.log_test_result(
                "ç®€å†æ ¼å¼åŒ–",
                format_ok,
                f"æ ¼å¼åŒ–é•¿åº¦: {len(formatted_resume)} å­—ç¬¦"
            )
            
            # æµ‹è¯•èŒä½æ ¼å¼åŒ–
            formatted_job = optimizer._format_job_info(test_jobs[0])
            job_format_ok = len(formatted_job) > 0
            
            self.log_test_result(
                "èŒä½æ ¼å¼åŒ–",
                job_format_ok,
                f"æ ¼å¼åŒ–é•¿åº¦: {len(formatted_job)} å­—ç¬¦"
            )
            
            # æµ‹è¯•JSONè§£æ
            test_json = '{"test": "value", "skills": ["Python", "Java"]}'
            parsed = optimizer._parse_json_result(test_json)
            parse_ok = 'test' in parsed and 'skills' in parsed
            
            self.log_test_result(
                "JSONè§£æ",
                parse_ok,
                f"è§£æç»“æœ: {list(parsed.keys())}"
            )
            
            # æµ‹è¯•ç›¸å…³èŒä½æŸ¥æ‰¾
            relevant_jobs = await optimizer._find_relevant_jobs(self.test_resume)
            relevant_ok = True  # å³ä½¿æ²¡æ‰¾åˆ°ä¹Ÿç®—æ­£å¸¸
            
            self.log_test_result(
                "ç›¸å…³èŒä½æŸ¥æ‰¾",
                relevant_ok,
                f"æ‰¾åˆ° {len(relevant_jobs)} ä¸ªç›¸å…³èŒä½"
            )
            
            return True
            
        except Exception as e:
            self.log_test_result("ç®€å†ä¼˜åŒ–å™¨", False, f"å¼‚å¸¸: {e}")
            return False
    
    async def test_data_pipeline(self, coordinator):
        """æµ‹è¯•æ•°æ®æµæ°´çº¿"""
        print("\nğŸš€ æµ‹è¯•6: æ•°æ®æµæ°´çº¿")
        
        try:
            pipeline = RAGDataPipeline(self.config)
            
            # æµ‹è¯•æµæ°´çº¿åˆå§‹åŒ–
            await pipeline._initialize_pipeline()
            
            init_ok = pipeline.pipeline_stats['status'] == 'initialized'
            
            self.log_test_result(
                "æµæ°´çº¿åˆå§‹åŒ–",
                init_ok,
                f"çŠ¶æ€: {pipeline.pipeline_stats['status']}"
            )
            
            # æµ‹è¯•é¢„å¤„ç†æ£€æŸ¥
            pre_check = await pipeline._pre_processing_check()
            check_ok = 'can_proceed' in pre_check
            
            self.log_test_result(
                "é¢„å¤„ç†æ£€æŸ¥",
                check_ok,
                f"å¯ä»¥ç»§ç»­: {pre_check.get('can_proceed', False)}"
            )
            
            # æµ‹è¯•å¤„ç†æ—¶é—´ä¼°ç®—
            estimate = pipeline.estimate_processing_time(batch_size=5)
            estimate_ok = 'unprocessed_jobs' in estimate
            
            self.log_test_result(
                "å¤„ç†æ—¶é—´ä¼°ç®—",
                estimate_ok,
                f"æœªå¤„ç†èŒä½: {estimate.get('unprocessed_jobs', 0)}"
            )
            
            # æµ‹è¯•æµæ°´çº¿çŠ¶æ€
            status = pipeline.get_pipeline_status()
            status_ok = 'pipeline_stats' in status
            
            self.log_test_result(
                "æµæ°´çº¿çŠ¶æ€æŸ¥è¯¢",
                status_ok,
                f"çŠ¶æ€å­—æ®µ: {list(status.keys())}"
            )
            
            return True
            
        except Exception as e:
            self.log_test_result("æ•°æ®æµæ°´çº¿", False, f"å¼‚å¸¸: {e}")
            return False
    
    async def test_integration_workflow(self, coordinator):
        """æµ‹è¯•é›†æˆå·¥ä½œæµ"""
        print("\nğŸ”— æµ‹è¯•7: é›†æˆå·¥ä½œæµ")
        
        try:
            # æ¨¡æ‹Ÿå®Œæ•´çš„RAGå·¥ä½œæµç¨‹
            
            # 1. è·å–èŒä½æ•°æ®
            jobs = coordinator.db_reader.get_jobs_for_rag_processing(limit=1)
            step1_ok = len(jobs) > 0
            
            self.log_test_result(
                "å·¥ä½œæµ-æ•°æ®è·å–",
                step1_ok,
                f"è·å– {len(jobs)} ä¸ªèŒä½"
            )
            
            if not jobs:
                return False
            
            # 2. å¤„ç†èŒä½æ•°æ®
            job_structure = coordinator.job_processor._fallback_extraction_from_db(jobs[0])
            step2_ok = job_structure.job_title is not None
            
            self.log_test_result(
                "å·¥ä½œæµ-æ•°æ®å¤„ç†",
                step2_ok,
                f"å¤„ç†èŒä½: {job_structure.job_title}"
            )
            
            # 3. åˆ›å»ºæ–‡æ¡£
            documents = coordinator.document_creator.create_job_documents(
                job_structure,
                job_id=jobs[0]['job_id']
            )
            step3_ok = len(documents) > 0
            
            self.log_test_result(
                "å·¥ä½œæµ-æ–‡æ¡£åˆ›å»º",
                step3_ok,
                f"åˆ›å»º {len(documents)} ä¸ªæ–‡æ¡£"
            )
            
            # 4. å‘é‡å­˜å‚¨
            doc_ids = coordinator.vector_manager.add_job_documents(documents, jobs[0]['job_id'])
            step4_ok = len(doc_ids) > 0
            
            self.log_test_result(
                "å·¥ä½œæµ-å‘é‡å­˜å‚¨",
                step4_ok,
                f"å­˜å‚¨ {len(doc_ids)} ä¸ªå‘é‡"
            )
            
            # 5. æœç´¢éªŒè¯
            search_results = coordinator.vector_manager.search_similar_jobs("Pythonå¼€å‘", k=2)
            step5_ok = len(search_results) > 0
            
            self.log_test_result(
                "å·¥ä½œæµ-æœç´¢éªŒè¯",
                step5_ok,
                f"æœç´¢åˆ° {len(search_results)} ä¸ªç»“æœ"
            )
            
            # 6. ç®€å†ä¼˜åŒ–æµ‹è¯•
            optimizer = ResumeOptimizer(coordinator, self.config['rag_system']['llm'])
            formatted_resume = optimizer._format_resume_info(self.test_resume)
            step6_ok = len(formatted_resume) > 0
            
            self.log_test_result(
                "å·¥ä½œæµ-ç®€å†ä¼˜åŒ–",
                step6_ok,
                f"ç®€å†æ ¼å¼åŒ–: {len(formatted_resume)} å­—ç¬¦"
            )
            
            return all([step1_ok, step2_ok, step3_ok, step4_ok, step5_ok, step6_ok])
            
        except Exception as e:
            self.log_test_result("é›†æˆå·¥ä½œæµ", False, f"å¼‚å¸¸: {e}")
            return False
    
    def generate_test_report(self):
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        print("\n" + "="*60)
        print("ğŸ“‹ RAGç³»ç»Ÿæµ‹è¯•æŠ¥å‘Š")
        print("="*60)
        
        # æ€»ä½“ç»Ÿè®¡
        total = self.test_results['total_tests']
        passed = self.test_results['passed_tests']
        failed = self.test_results['failed_tests']
        success_rate = (passed / total * 100) if total > 0 else 0
        
        print(f"æ€»æµ‹è¯•æ•°: {total}")
        print(f"é€šè¿‡æ•°: {passed}")
        print(f"å¤±è´¥æ•°: {failed}")
        print(f"æˆåŠŸç‡: {success_rate:.1f}%")
        
        # è¯¦ç»†ç»“æœ
        print(f"\nğŸ“ è¯¦ç»†æµ‹è¯•ç»“æœ:")
        for result in self.test_results['test_details']:
            print(f"  {result['status']} {result['test_name']}")
            if result['details']:
                print(f"      {result['details']}")
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = f"./test_reports/rag_system_test_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        Path(report_file).parent.mkdir(parents=True, exist_ok=True)
        
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(self.test_results, f, ensure_ascii=False, indent=2, default=str)
        
        print(f"\nğŸ’¾ æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
        
        return success_rate >= 80  # 80%ä»¥ä¸Šé€šè¿‡ç‡ç®—æˆåŠŸ
    
    async def run_all_tests(self):
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        print("ğŸ§ª å¼€å§‹RAGç³»ç»Ÿå®Œæ•´åŠŸèƒ½æµ‹è¯•")
        print("="*60)
        
        try:
            # 1. ç³»ç»Ÿåˆå§‹åŒ–æµ‹è¯•
            coordinator = await self.test_system_initialization()
            if not coordinator:
                print("âŒ ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥ï¼Œç»ˆæ­¢æµ‹è¯•")
                return False
            
            # 2. æ•°æ®åº“æ“ä½œæµ‹è¯•
            test_jobs = await self.test_database_operations(coordinator)
            
            # 3. èŒä½å¤„ç†æµ‹è¯•
            job_structure, documents = await self.test_job_processing(coordinator, test_jobs)
            
            # 4. å‘é‡æ“ä½œæµ‹è¯•
            await self.test_vector_operations(coordinator, documents)
            
            # 5. ç®€å†ä¼˜åŒ–å™¨æµ‹è¯•
            await self.test_resume_optimizer(coordinator, test_jobs)
            
            # 6. æ•°æ®æµæ°´çº¿æµ‹è¯•
            await self.test_data_pipeline(coordinator)
            
            # 7. é›†æˆå·¥ä½œæµæµ‹è¯•
            await self.test_integration_workflow(coordinator)
            
            # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
            success = self.generate_test_report()
            
            # æ¸…ç†èµ„æº
            coordinator.cleanup_system()
            
            return success
            
        except Exception as e:
            print(f"âŒ æµ‹è¯•æ‰§è¡Œå¼‚å¸¸: {e}")
            import traceback
            traceback.print_exc()
            return False

async def main():
    """ä¸»å‡½æ•°"""
    # é…ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.WARNING,  # å‡å°‘æ—¥å¿—è¾“å‡º
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # åˆ›å»ºæµ‹è¯•å™¨å¹¶è¿è¡Œæµ‹è¯•
    tester = RAGSystemTester()
    success = await tester.run_all_tests()
    
    return success

if __name__ == "__main__":
    success = asyncio.run(main())
    sys.exit(0 if success else 1)