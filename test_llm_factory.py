#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LLMå·¥å‚æ¨¡å¼æµ‹è¯•è„šæœ¬

å±•ç¤ºå¦‚ä½•ä½¿ç”¨ç»Ÿä¸€çš„LLMå·¥å‚æ¥åˆ‡æ¢ä¸åŒçš„è¯­è¨€æ¨¡å‹
"""

import os
import sys
import json
import logging
import asyncio
from pathlib import Path

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent / "src"))

from src.rag.llm_factory import (
    LLMFactory, 
    create_llm, 
    create_llm_from_config,
    LLM_CONFIGS
)
from src.rag.vector_manager import ChromaDBManager
from src.rag.job_processor import LangChainJobProcessor
from src.rag.document_creator import DocumentCreator
from src.rag.rag_chain import JobRAGSystem

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# APIå¯†é’¥é…ç½®
API_KEYS = {
    "zhipu": "0175134f27a040709d7541e14b4db353.V3KP9u8rZ0oQj9s9",
    "openai": "your_openai_api_key",  # æ›¿æ¢ä¸ºä½ çš„OpenAI APIå¯†é’¥
    "claude": "your_claude_api_key",  # æ›¿æ¢ä¸ºä½ çš„Claude APIå¯†é’¥
}

class LLMFactoryTest:
    """LLMå·¥å‚æ¨¡å¼æµ‹è¯•ç±»"""
    
    def __init__(self):
        self.test_prompt = "è¯·ç®€è¦ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½çš„å‘å±•å†å²ï¼Œä¸è¶…è¿‡100å­—ã€‚"
    
    def test_basic_llm_creation(self):
        """æµ‹è¯•åŸºç¡€LLMåˆ›å»º"""
        print("ğŸ”§ æµ‹è¯•åŸºç¡€LLMåˆ›å»º")
        print("-" * 50)
        
        # æµ‹è¯•æ”¯æŒçš„æä¾›å•†
        providers = LLMFactory.get_supported_providers()
        print(f"æ”¯æŒçš„LLMæä¾›å•†: {providers}")
        
        # æµ‹è¯•æ™ºè°±GLM
        if API_KEYS["zhipu"]:
            try:
                zhipu_llm = create_llm(
                    provider="zhipu",
                    api_key=API_KEYS["zhipu"],
                    model="glm-4-flash",
                    temperature=0.1
                )
                print(f"âœ… æ™ºè°±GLMåˆ›å»ºæˆåŠŸ: {zhipu_llm._llm_type}")
                
                # æµ‹è¯•è°ƒç”¨
                response = zhipu_llm(self.test_prompt)
                print(f"ğŸ“ æ™ºè°±GLMå›å¤: {response[:100]}...")
                
            except Exception as e:
                print(f"âŒ æ™ºè°±GLMæµ‹è¯•å¤±è´¥: {e}")
        
        # æµ‹è¯•Ollamaï¼ˆå¦‚æœæœ¬åœ°è¿è¡Œï¼‰
        try:
            ollama_llm = create_llm(
                provider="ollama",
                model="llama2",
                base_url="http://localhost:11434",
                temperature=0.1
            )
            print(f"âœ… Ollamaåˆ›å»ºæˆåŠŸ: {ollama_llm._llm_type}")
            
            # æµ‹è¯•è°ƒç”¨ï¼ˆå¯èƒ½ä¼šå¤±è´¥å¦‚æœOllamaæœªè¿è¡Œï¼‰
            try:
                response = ollama_llm(self.test_prompt)
                print(f"ğŸ“ Ollamaå›å¤: {response[:100]}...")
            except Exception as e:
                print(f"âš ï¸ Ollamaè°ƒç”¨å¤±è´¥ï¼ˆå¯èƒ½æœªè¿è¡Œï¼‰: {e}")
                
        except Exception as e:
            print(f"âŒ Ollamaåˆ›å»ºå¤±è´¥: {e}")
        
        print()
    
    def test_config_based_creation(self):
        """æµ‹è¯•åŸºäºé…ç½®çš„LLMåˆ›å»º"""
        print("âš™ï¸ æµ‹è¯•åŸºäºé…ç½®çš„LLMåˆ›å»º")
        print("-" * 50)
        
        # æ˜¾ç¤ºå¯ç”¨é…ç½®
        print(f"å¯ç”¨é…ç½®: {list(LLM_CONFIGS.keys())}")
        
        # æµ‹è¯•æ™ºè°±GLMé…ç½®
        try:
            zhipu_llm = create_llm_from_config(
                "zhipu_default",
                api_key=API_KEYS["zhipu"]
            )
            print(f"âœ… ä»é…ç½®åˆ›å»ºæ™ºè°±GLMæˆåŠŸ: {zhipu_llm._identifying_params}")
            
        except Exception as e:
            print(f"âŒ é…ç½®åˆ›å»ºå¤±è´¥: {e}")
        
        # æµ‹è¯•Ollamaé…ç½®
        try:
            ollama_llm = create_llm_from_config("ollama_llama2")
            print(f"âœ… ä»é…ç½®åˆ›å»ºOllamaæˆåŠŸ: {ollama_llm._identifying_params}")
            
        except Exception as e:
            print(f"âŒ Ollamaé…ç½®åˆ›å»ºå¤±è´¥: {e}")
        
        print()
    
    async def test_rag_with_different_llms(self):
        """æµ‹è¯•RAGç³»ç»Ÿä½¿ç”¨ä¸åŒçš„LLM"""
        print("ğŸ¤– æµ‹è¯•RAGç³»ç»Ÿä½¿ç”¨ä¸åŒLLM")
        print("-" * 50)
        
        # åŠ è½½æµ‹è¯•æ•°æ®
        json_file = "fixed_job_detail_result.json"
        if not os.path.exists(json_file):
            print(f"âŒ æ‰¾ä¸åˆ°æµ‹è¯•æ•°æ®æ–‡ä»¶: {json_file}")
            return
        
        with open(json_file, 'r', encoding='utf-8') as f:
            job_data = json.load(f)
        
        # æµ‹è¯•ä¸åŒLLMé…ç½®
        llm_configs = [
            {
                "name": "æ™ºè°±GLM",
                "config": {
                    "provider": "zhipu",
                    "api_key": API_KEYS["zhipu"],
                    "model": "glm-4-flash",
                    "temperature": 0.1,
                    "max_tokens": 1000
                }
            },
            {
                "name": "Ollama Llama2",
                "config": {
                    "provider": "ollama",
                    "model": "llama2",
                    "base_url": "http://localhost:11434",
                    "temperature": 0.1,
                    "max_tokens": 1000
                }
            }
        ]
        
        for llm_config in llm_configs:
            print(f"\nğŸ” æµ‹è¯• {llm_config['name']}")
            print("-" * 30)
            
            try:
                # åˆ›å»ºRAGç³»ç»Ÿé…ç½®
                rag_config = {
                    'vectorstore': {
                        'persist_directory': f'./test_chroma_db_{llm_config["name"].lower().replace(" ", "_")}',
                        'collection_name': 'test_job_positions'
                    },
                    'llm': llm_config['config']
                }
                
                # åˆå§‹åŒ–ç»„ä»¶
                vector_manager = ChromaDBManager(rag_config['vectorstore'])
                job_processor = LangChainJobProcessor(
                    llm_config=llm_config['config']
                )
                document_creator = DocumentCreator()
                rag_system = JobRAGSystem(
                    vectorstore_manager=vector_manager,
                    config=rag_config
                )
                
                # å¤„ç†èŒä½æ•°æ®
                job_structure = await job_processor.process_job_data(job_data)
                print(f"  âœ… èŒä½ç»“æ„åŒ–æå–å®Œæˆ: {job_structure.job_title}")
                
                # åˆ›å»ºæ–‡æ¡£å¹¶å­˜å‚¨
                documents = document_creator.create_job_documents(job_structure)
                doc_ids = vector_manager.add_job_documents(documents)
                print(f"  âœ… ä¿å­˜äº† {len(doc_ids)} ä¸ªæ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“")
                
                # æµ‹è¯•é—®ç­”
                question = "è¿™ä¸ªèŒä½éœ€è¦ä»€ä¹ˆæŠ€èƒ½ï¼Ÿ"
                answer_result = await rag_system.ask_question(question)
                print(f"  ğŸ’¬ é—®é¢˜: {question}")
                print(f"  ğŸ’¡ å›ç­”: {answer_result['answer'][:150]}...")
                
                # æ¸…ç†
                vector_manager.close()
                
            except Exception as e:
                print(f"  âŒ {llm_config['name']} æµ‹è¯•å¤±è´¥: {e}")
        
        print()
    
    def test_custom_adapter_registration(self):
        """æµ‹è¯•è‡ªå®šä¹‰é€‚é…å™¨æ³¨å†Œ"""
        print("ğŸ”Œ æµ‹è¯•è‡ªå®šä¹‰é€‚é…å™¨æ³¨å†Œ")
        print("-" * 50)
        
        # åˆ›å»ºä¸€ä¸ªç®€å•çš„è‡ªå®šä¹‰é€‚é…å™¨
        from src.rag.llm_factory import BaseLLMAdapter
        from typing import Optional, List, Any
        from langchain.callbacks.manager import CallbackManagerForLLMRun
        
        class MockLLMAdapter(BaseLLMAdapter):
            """æ¨¡æ‹ŸLLMé€‚é…å™¨"""
            
            def __init__(self, **kwargs):
                super().__init__(**kwargs)
            
            @property
            def _llm_type(self) -> str:
                return "mock_llm"
            
            def _call(
                self,
                prompt: str,
                stop: Optional[List[str]] = None,
                run_manager: Optional[CallbackManagerForLLMRun] = None,
                **kwargs: Any,
            ) -> str:
                return f"è¿™æ˜¯æ¨¡æ‹Ÿå›å¤ï¼š{prompt[:50]}..."
            
            @property
            def _identifying_params(self):
                return {"type": "mock"}
        
        # æ³¨å†Œè‡ªå®šä¹‰é€‚é…å™¨
        try:
            LLMFactory.register_adapter("mock", MockLLMAdapter)
            print("âœ… è‡ªå®šä¹‰é€‚é…å™¨æ³¨å†ŒæˆåŠŸ")
            
            # æµ‹è¯•ä½¿ç”¨è‡ªå®šä¹‰é€‚é…å™¨
            mock_llm = create_llm(provider="mock")
            response = mock_llm("æµ‹è¯•æç¤º")
            print(f"ğŸ“ è‡ªå®šä¹‰é€‚é…å™¨å›å¤: {response}")
            
            # æ£€æŸ¥æä¾›å•†åˆ—è¡¨
            providers = LLMFactory.get_supported_providers()
            print(f"æ›´æ–°åçš„æä¾›å•†åˆ—è¡¨: {providers}")
            
        except Exception as e:
            print(f"âŒ è‡ªå®šä¹‰é€‚é…å™¨æµ‹è¯•å¤±è´¥: {e}")
        
        print()

async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ LLMå·¥å‚æ¨¡å¼æµ‹è¯•")
    print("=" * 60)
    
    test = LLMFactoryTest()
    
    # 1. æµ‹è¯•åŸºç¡€LLMåˆ›å»º
    test.test_basic_llm_creation()
    
    # 2. æµ‹è¯•åŸºäºé…ç½®çš„åˆ›å»º
    test.test_config_based_creation()
    
    # 3. æµ‹è¯•RAGç³»ç»Ÿä½¿ç”¨ä¸åŒLLM
    await test.test_rag_with_different_llms()
    
    # 4. æµ‹è¯•è‡ªå®šä¹‰é€‚é…å™¨æ³¨å†Œ
    test.test_custom_adapter_registration()
    
    print("=" * 60)
    print("ğŸ‰ LLMå·¥å‚æ¨¡å¼æµ‹è¯•å®Œæˆï¼")
    print("\nğŸ’¡ ä½¿ç”¨è¯´æ˜:")
    print("1. é€šè¿‡é…ç½®æ–‡ä»¶è½»æ¾åˆ‡æ¢ä¸åŒçš„LLMæä¾›å•†")
    print("2. æ”¯æŒæ™ºè°±GLMã€Ollamaã€OpenAIã€Claudeç­‰å¤šç§æ¨¡å‹")
    print("3. å¯ä»¥æ³¨å†Œè‡ªå®šä¹‰çš„LLMé€‚é…å™¨")
    print("4. ç»Ÿä¸€çš„æ¥å£ï¼Œä¾¿äºç»´æŠ¤å’Œæ‰©å±•")
    
    print("\nğŸ”§ é…ç½®ç¤ºä¾‹:")
    print("""
# ä½¿ç”¨æ™ºè°±GLM
config = {
    'llm': {
        'provider': 'zhipu',
        'api_key': 'your_api_key',
        'model': 'glm-4-flash'
    }
}

# ä½¿ç”¨Ollama
config = {
    'llm': {
        'provider': 'ollama',
        'model': 'llama2',
        'base_url': 'http://localhost:11434'
    }
}

# ä½¿ç”¨OpenAI
config = {
    'llm': {
        'provider': 'openai',
        'api_key': 'your_api_key',
        'model': 'gpt-3.5-turbo'
    }
}
""")

if __name__ == "__main__":
    asyncio.run(main())