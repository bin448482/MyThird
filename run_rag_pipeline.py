#!/usr/bin/env python3
"""
RAGæ•°æ®æµæ°´çº¿è¿è¡Œè„šæœ¬

æä¾›å‘½ä»¤è¡Œæ¥å£æ¥è¿è¡ŒRAGæ•°æ®å¤„ç†æµæ°´çº¿
"""

import sys
import asyncio
import argparse
import logging
from pathlib import Path
import json

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(str(Path(__file__).parent))

from src.rag.data_pipeline import RAGDataPipeline, create_progress_callback

def setup_logging(log_level: str = 'INFO'):
    """è®¾ç½®æ—¥å¿—é…ç½®"""
    logging.basicConfig(
        level=getattr(logging, log_level.upper()),
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(),
            logging.FileHandler('./logs/rag_pipeline.log', encoding='utf-8')
        ]
    )

def load_config(config_file: str = None) -> dict:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    if config_file and Path(config_file).exists():
        with open(config_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    # é»˜è®¤é…ç½®
    return {
        'rag_system': {
            'database': {
                'path': './data/jobs.db',
                'batch_size': 50
            },
            'llm': {
                'provider': 'zhipu',
                'model': 'glm-4-flash',
                'api_key': 'your-api-key-here',
                'temperature': 0.1,
                'max_tokens': 1500
            },
            'vector_db': {
                'persist_directory': './chroma_db',
                'collection_name': 'job_positions',
                'embeddings': {
                    'model_name': 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',
                    'device': 'cpu',
                    'normalize_embeddings': True
                }
            },
            'documents': {
                'types': ['overview', 'responsibility', 'requirement', 'skills', 'basic_requirements'],
                'create_comprehensive_doc': False
            },
            'processing': {
                'skip_processed': True,
                'force_reprocess': False,
                'batch_size': 50,
                'max_retry_attempts': 3
            }
        }
    }

async def run_pipeline_command(args):
    """è¿è¡Œæµæ°´çº¿å‘½ä»¤"""
    print("ğŸš€ å¯åŠ¨RAGæ•°æ®æµæ°´çº¿")
    print("=" * 50)
    
    try:
        # åŠ è½½é…ç½®
        config = load_config(args.config)
        
        # è¦†ç›–å‘½ä»¤è¡Œå‚æ•°
        if args.batch_size:
            config['rag_system']['processing']['batch_size'] = args.batch_size
        if args.force_reprocess:
            config['rag_system']['processing']['force_reprocess'] = True
        
        # åˆ›å»ºè¿›åº¦å›è°ƒ
        progress_callback = create_progress_callback() if args.show_progress else None
        
        # åˆå§‹åŒ–æµæ°´çº¿
        pipeline = RAGDataPipeline(config, progress_callback)
        
        # è¿è¡Œæµæ°´çº¿
        result = await pipeline.run_full_pipeline(
            batch_size=args.batch_size or 50,
            max_jobs=args.max_jobs,
            force_reprocess=args.force_reprocess,
            save_progress=not args.no_save
        )
        
        # æ˜¾ç¤ºç»“æœ
        print("\nğŸ“Š æµæ°´çº¿æ‰§è¡Œç»“æœ:")
        print("=" * 30)
        
        exec_summary = result.get('execution_summary', {})
        proc_stats = result.get('processing_statistics', {})
        data_quality = result.get('data_quality', {})
        
        print(f"æ‰§è¡ŒçŠ¶æ€: {exec_summary.get('status', 'unknown')}")
        print(f"æ‰§è¡Œæ—¶é—´: {exec_summary.get('execution_time', 0):.1f} ç§’")
        print(f"å¤„ç†èŒä½: {proc_stats.get('processed_jobs', 0)} / {proc_stats.get('total_jobs', 0)}")
        print(f"æˆåŠŸç‡: {proc_stats.get('success_rate', 0):.1f}%")
        print(f"å¤„ç†é€Ÿç‡: {proc_stats.get('processing_rate', 0):.2f} èŒä½/ç§’")
        print(f"æ•°æ®è´¨é‡è¯„åˆ†: {data_quality.get('quality_score', 0):.2f}")
        
        # æ˜¾ç¤ºå»ºè®®
        recommendations = result.get('recommendations', [])
        if recommendations:
            print(f"\nğŸ’¡ å»ºè®®:")
            for i, rec in enumerate(recommendations, 1):
                print(f"   {i}. {rec}")
        
        # æ˜¾ç¤ºé—®é¢˜
        issues = result.get('issues', [])
        if issues:
            print(f"\nâš ï¸ é—®é¢˜:")
            for i, issue in enumerate(issues, 1):
                print(f"   {i}. {issue}")
        
        print(f"\nâœ… æµæ°´çº¿æ‰§è¡Œå®Œæˆ!")
        return True
        
    except Exception as e:
        print(f"âŒ æµæ°´çº¿æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

async def status_command(args):
    """çŠ¶æ€æŸ¥è¯¢å‘½ä»¤"""
    print("ğŸ“Š RAGç³»ç»ŸçŠ¶æ€æŸ¥è¯¢")
    print("=" * 30)
    
    try:
        config = load_config(args.config)
        pipeline = RAGDataPipeline(config)
        
        # åˆå§‹åŒ–ç³»ç»Ÿï¼ˆä»…ç”¨äºçŠ¶æ€æŸ¥è¯¢ï¼‰
        await pipeline._initialize_pipeline()
        
        # è·å–çŠ¶æ€
        status = pipeline.get_pipeline_status()
        
        # æ˜¾ç¤ºç³»ç»ŸçŠ¶æ€
        system_status = status.get('system_status', {})
        print(f"ç³»ç»Ÿåˆå§‹åŒ–: {'âœ…' if system_status.get('is_initialized') else 'âŒ'}")
        
        components = system_status.get('components', {})
        print("ç»„ä»¶çŠ¶æ€:")
        for comp_name, comp_status in components.items():
            status_icon = "âœ…" if comp_status else "âŒ"
            print(f"  {comp_name}: {status_icon}")
        
        # æ˜¾ç¤ºå¤„ç†è¿›åº¦
        progress = status.get('processing_progress', {})
        db_stats = progress.get('database_stats', {})
        
        print(f"\næ•°æ®åº“ç»Ÿè®¡:")
        print(f"  æ€»èŒä½æ•°: {db_stats.get('total', 0)}")
        print(f"  å·²å¤„ç†: {db_stats.get('processed', 0)}")
        print(f"  æœªå¤„ç†: {db_stats.get('unprocessed', 0)}")
        print(f"  å¤„ç†ç‡: {db_stats.get('processing_rate', 0):.1f}%")
        
        vector_stats = progress.get('vector_stats', {})
        print(f"\nå‘é‡æ•°æ®åº“ç»Ÿè®¡:")
        print(f"  æ–‡æ¡£æ•°é‡: {vector_stats.get('document_count', 0)}")
        print(f"  é›†åˆåç§°: {vector_stats.get('collection_name', 'N/A')}")
        
        # ä¼°ç®—å¤„ç†æ—¶é—´
        if args.estimate_time:
            estimate = pipeline.estimate_processing_time(args.batch_size or 50)
            if 'error' not in estimate:
                print(f"\nâ±ï¸ å¤„ç†æ—¶é—´ä¼°ç®—:")
                print(f"  æœªå¤„ç†èŒä½: {estimate.get('unprocessed_jobs', 0)}")
                print(f"  é¢„è®¡æ—¶é—´: {estimate.get('estimated_time_minutes', 0):.1f} åˆ†é’Ÿ")
                print(f"  é¢„è®¡æ‰¹æ¬¡: {estimate.get('estimated_batches', 0)}")
        
        return True
        
    except Exception as e:
        print(f"âŒ çŠ¶æ€æŸ¥è¯¢å¤±è´¥: {e}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='RAGæ•°æ®æµæ°´çº¿ç®¡ç†å·¥å…·')
    
    # å…¨å±€å‚æ•°
    parser.add_argument('--config', '-c', help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--log-level', default='INFO', choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'], help='æ—¥å¿—çº§åˆ«')
    
    # å­å‘½ä»¤
    subparsers = parser.add_subparsers(dest='command', help='å¯ç”¨å‘½ä»¤')
    
    # è¿è¡Œæµæ°´çº¿å‘½ä»¤
    run_parser = subparsers.add_parser('run', help='è¿è¡ŒRAGæ•°æ®æµæ°´çº¿')
    run_parser.add_argument('--batch-size', '-b', type=int, help='æ‰¹å¤„ç†å¤§å°')
    run_parser.add_argument('--max-jobs', '-m', type=int, help='æœ€å¤§å¤„ç†èŒä½æ•°é‡')
    run_parser.add_argument('--force-reprocess', '-f', action='store_true', help='å¼ºåˆ¶é‡æ–°å¤„ç†æ‰€æœ‰èŒä½')
    run_parser.add_argument('--no-save', action='store_true', help='ä¸ä¿å­˜å¤„ç†ç»“æœ')
    run_parser.add_argument('--show-progress', '-p', action='store_true', help='æ˜¾ç¤ºå¤„ç†è¿›åº¦')
    
    # çŠ¶æ€æŸ¥è¯¢å‘½ä»¤
    status_parser = subparsers.add_parser('status', help='æŸ¥è¯¢ç³»ç»ŸçŠ¶æ€')
    status_parser.add_argument('--estimate-time', '-e', action='store_true', help='ä¼°ç®—å¤„ç†æ—¶é—´')
    status_parser.add_argument('--batch-size', '-b', type=int, default=50, help='ç”¨äºæ—¶é—´ä¼°ç®—çš„æ‰¹å¤„ç†å¤§å°')
    
    # æ¢å¤æµæ°´çº¿å‘½ä»¤
    resume_parser = subparsers.add_parser('resume', help='æ¢å¤ä¸­æ–­çš„æµæ°´çº¿')
    resume_parser.add_argument('--batch-size', '-b', type=int, default=50, help='æ‰¹å¤„ç†å¤§å°')
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—
    setup_logging(args.log_level)
    
    # ç¡®ä¿logsç›®å½•å­˜åœ¨
    Path('./logs').mkdir(exist_ok=True)
    
    # æ‰§è¡Œå‘½ä»¤
    if args.command == 'run':
        success = asyncio.run(run_pipeline_command(args))
    elif args.command == 'status':
        success = asyncio.run(status_command(args))
    elif args.command == 'resume':
        # æ¢å¤å‘½ä»¤ä½¿ç”¨runå‘½ä»¤çš„é€»è¾‘ï¼Œä½†ä¸å¼ºåˆ¶é‡å¤„ç†
        args.force_reprocess = False
        args.show_progress = True
        success = asyncio.run(run_pipeline_command(args))
    else:
        parser.print_help()
        success = False
    
    sys.exit(0 if success else 1)

if __name__ == "__main__":
    main()