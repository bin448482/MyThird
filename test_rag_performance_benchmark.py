#!/usr/bin/env python3
"""
RAGç³»ç»Ÿæ€§èƒ½åŸºå‡†æµ‹è¯•

æµ‹è¯•RAGç³»ç»Ÿåœ¨ä¸åŒè´Ÿè½½ä¸‹çš„æ€§èƒ½è¡¨ç°ï¼ŒåŒ…æ‹¬å¤„ç†é€Ÿåº¦ã€å†…å­˜ä½¿ç”¨ã€å¹¶å‘èƒ½åŠ›ç­‰
"""

import sys
import asyncio
import time
import psutil
import json
import logging
from pathlib import Path
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor
import threading

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(str(Path(__file__).parent))

from src.rag.rag_system_coordinator import RAGSystemCoordinator
from src.rag.data_pipeline import RAGDataPipeline

class PerformanceBenchmark:
    """æ€§èƒ½åŸºå‡†æµ‹è¯•å™¨"""
    
    def __init__(self):
        self.benchmark_results = {
            'test_start_time': datetime.now().isoformat(),
            'system_info': self._get_system_info(),
            'benchmarks': []
        }
        
        # æµ‹è¯•é…ç½®
        self.config = {
            'rag_system': {
                'database': {
                    'path': './data/jobs.db',
                    'batch_size': 20
                },
                'llm': {
                    'provider': 'zhipu',
                    'model': 'glm-4-flash',
                    'api_key': 'test-key',
                    'temperature': 0.1,
                    'max_tokens': 1500
                },
                'vector_db': {
                    'persist_directory': './data/test_chroma_db_perf',
                    'collection_name': 'perf_test_jobs',
                    'embeddings': {
                        'model_name': 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',
                        'device': 'cpu',
                        'normalize_embeddings': True
                    }
                },
                'documents': {
                    'types': ['overview', 'responsibility', 'requirement', 'skills', 'basic_requirements']
                },
                'processing': {
                    'skip_processed': False,  # æ€§èƒ½æµ‹è¯•æ—¶ä¸è·³è¿‡
                    'batch_size': 10
                }
            }
        }
    
    def _get_system_info(self):
        """è·å–ç³»ç»Ÿä¿¡æ¯"""
        return {
            'cpu_count': psutil.cpu_count(),
            'memory_total_gb': round(psutil.virtual_memory().total / (1024**3), 2),
            'python_version': sys.version,
            'platform': sys.platform
        }
    
    def _monitor_resources(self, duration_seconds=60):
        """ç›‘æ§ç³»ç»Ÿèµ„æºä½¿ç”¨"""
        start_time = time.time()
        measurements = []
        
        while time.time() - start_time < duration_seconds:
            cpu_percent = psutil.cpu_percent(interval=1)
            memory = psutil.virtual_memory()
            
            measurements.append({
                'timestamp': time.time() - start_time,
                'cpu_percent': cpu_percent,
                'memory_percent': memory.percent,
                'memory_used_gb': round(memory.used / (1024**3), 2)
            })
        
        return {
            'avg_cpu_percent': sum(m['cpu_percent'] for m in measurements) / len(measurements),
            'max_cpu_percent': max(m['cpu_percent'] for m in measurements),
            'avg_memory_percent': sum(m['memory_percent'] for m in measurements) / len(measurements),
            'max_memory_percent': max(m['memory_percent'] for m in measurements),
            'peak_memory_gb': max(m['memory_used_gb'] for m in measurements),
            'measurements': measurements
        }
    
    async def benchmark_system_initialization(self):
        """åŸºå‡†æµ‹è¯•ï¼šç³»ç»Ÿåˆå§‹åŒ–æ€§èƒ½"""
        print("ğŸš€ åŸºå‡†æµ‹è¯•1: ç³»ç»Ÿåˆå§‹åŒ–æ€§èƒ½")
        
        results = []
        
        for i in range(5):  # æµ‹è¯•5æ¬¡å–å¹³å‡å€¼
            start_time = time.time()
            start_memory = psutil.virtual_memory().used / (1024**3)
            
            try:
                coordinator = RAGSystemCoordinator(self.config)
                init_success = coordinator.initialize_system()
                
                end_time = time.time()
                end_memory = psutil.virtual_memory().used / (1024**3)
                
                if init_success:
                    results.append({
                        'run': i + 1,
                        'init_time_seconds': round(end_time - start_time, 3),
                        'memory_increase_mb': round((end_memory - start_memory) * 1024, 2),
                        'success': True
                    })
                    
                    # æ¸…ç†èµ„æº
                    coordinator.cleanup_system()
                else:
                    results.append({
                        'run': i + 1,
                        'success': False,
                        'error': 'Initialization failed'
                    })
                    
            except Exception as e:
                results.append({
                    'run': i + 1,
                    'success': False,
                    'error': str(e)
                })
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        successful_runs = [r for r in results if r.get('success', False)]
        
        if successful_runs:
            avg_init_time = sum(r['init_time_seconds'] for r in successful_runs) / len(successful_runs)
            avg_memory_increase = sum(r['memory_increase_mb'] for r in successful_runs) / len(successful_runs)
            
            benchmark = {
                'test_name': 'ç³»ç»Ÿåˆå§‹åŒ–æ€§èƒ½',
                'runs': len(results),
                'successful_runs': len(successful_runs),
                'avg_init_time_seconds': round(avg_init_time, 3),
                'avg_memory_increase_mb': round(avg_memory_increase, 2),
                'min_init_time_seconds': min(r['init_time_seconds'] for r in successful_runs),
                'max_init_time_seconds': max(r['init_time_seconds'] for r in successful_runs),
                'detailed_results': results
            }
        else:
            benchmark = {
                'test_name': 'ç³»ç»Ÿåˆå§‹åŒ–æ€§èƒ½',
                'runs': len(results),
                'successful_runs': 0,
                'error': 'All initialization attempts failed',
                'detailed_results': results
            }
        
        self.benchmark_results['benchmarks'].append(benchmark)
        
        print(f"   å¹³å‡åˆå§‹åŒ–æ—¶é—´: {benchmark.get('avg_init_time_seconds', 'N/A')} ç§’")
        print(f"   å¹³å‡å†…å­˜å¢é•¿: {benchmark.get('avg_memory_increase_mb', 'N/A')} MB")
        print(f"   æˆåŠŸç‡: {len(successful_runs)}/{len(results)}")
    
    async def benchmark_batch_processing(self):
        """åŸºå‡†æµ‹è¯•ï¼šæ‰¹é‡å¤„ç†æ€§èƒ½"""
        print("\nğŸ“Š åŸºå‡†æµ‹è¯•2: æ‰¹é‡å¤„ç†æ€§èƒ½")
        
        try:
            coordinator = RAGSystemCoordinator(self.config)
            coordinator.initialize_system()
            
            # è·å–æµ‹è¯•æ•°æ®
            jobs = coordinator.db_reader.get_jobs_for_rag_processing(limit=50)
            
            if not jobs:
                print("   âš ï¸ æ²¡æœ‰å¯ç”¨çš„æµ‹è¯•æ•°æ®")
                return
            
            # æµ‹è¯•ä¸åŒæ‰¹æ¬¡å¤§å°çš„æ€§èƒ½
            batch_sizes = [1, 5, 10, 20]
            batch_results = []
            
            for batch_size in batch_sizes:
                print(f"   æµ‹è¯•æ‰¹æ¬¡å¤§å°: {batch_size}")
                
                start_time = time.time()
                start_memory = psutil.virtual_memory().used / (1024**3)
                
                processed_count = 0
                error_count = 0
                
                # åˆ†æ‰¹å¤„ç†
                for i in range(0, min(len(jobs), 20), batch_size):  # æœ€å¤šå¤„ç†20ä¸ªèŒä½
                    batch = jobs[i:i+batch_size]
                    
                    for job in batch:
                        try:
                            # ä½¿ç”¨å¤‡ç”¨å¤„ç†æ–¹æ³•ï¼ˆä¸è°ƒç”¨LLMï¼‰
                            job_structure = coordinator.job_processor._fallback_extraction_from_db(job)
                            documents = coordinator.job_processor.create_documents(
                                job_structure,
                                job_id=job.get('job_id'),
                                job_url=job.get('url')
                            )
                            
                            # æ·»åŠ åˆ°å‘é‡æ•°æ®åº“
                            coordinator.vector_manager.add_job_documents(documents, job.get('job_id'))
                            processed_count += 1
                            
                        except Exception as e:
                            error_count += 1
                
                end_time = time.time()
                end_memory = psutil.virtual_memory().used / (1024**3)
                
                batch_result = {
                    'batch_size': batch_size,
                    'processed_jobs': processed_count,
                    'error_count': error_count,
                    'total_time_seconds': round(end_time - start_time, 3),
                    'jobs_per_second': round(processed_count / (end_time - start_time), 2) if end_time > start_time else 0,
                    'memory_increase_mb': round((end_memory - start_memory) * 1024, 2)
                }
                
                batch_results.append(batch_result)
                
                print(f"     å¤„ç†é€Ÿåº¦: {batch_result['jobs_per_second']} èŒä½/ç§’")
                print(f"     å†…å­˜å¢é•¿: {batch_result['memory_increase_mb']} MB")
            
            benchmark = {
                'test_name': 'æ‰¹é‡å¤„ç†æ€§èƒ½',
                'total_jobs_available': len(jobs),
                'batch_results': batch_results,
                'optimal_batch_size': max(batch_results, key=lambda x: x['jobs_per_second'])['batch_size']
            }
            
            self.benchmark_results['benchmarks'].append(benchmark)
            
            # æ¸…ç†èµ„æº
            coordinator.cleanup_system()
            
        except Exception as e:
            print(f"   âŒ æ‰¹é‡å¤„ç†æµ‹è¯•å¤±è´¥: {e}")
    
    async def benchmark_vector_search_performance(self):
        """åŸºå‡†æµ‹è¯•ï¼šå‘é‡æœç´¢æ€§èƒ½"""
        print("\nğŸ” åŸºå‡†æµ‹è¯•3: å‘é‡æœç´¢æ€§èƒ½")
        
        try:
            coordinator = RAGSystemCoordinator(self.config)
            coordinator.initialize_system()
            
            # å…ˆæ·»åŠ ä¸€äº›æµ‹è¯•æ•°æ®
            jobs = coordinator.db_reader.get_jobs_for_rag_processing(limit=10)
            
            if not jobs:
                print("   âš ï¸ æ²¡æœ‰å¯ç”¨çš„æµ‹è¯•æ•°æ®")
                return
            
            # æ·»åŠ æµ‹è¯•æ–‡æ¡£
            for job in jobs[:5]:  # åªå¤„ç†å‰5ä¸ª
                try:
                    job_structure = coordinator.job_processor._fallback_extraction_from_db(job)
                    documents = coordinator.job_processor.create_documents(
                        job_structure,
                        job_id=job.get('job_id'),
                        job_url=job.get('url')
                    )
                    coordinator.vector_manager.add_job_documents(documents, job.get('job_id'))
                except:
                    continue
            
            # æµ‹è¯•æœç´¢æ€§èƒ½
            search_queries = [
                "Pythonå¼€å‘å·¥ç¨‹å¸ˆ",
                "Javaåç«¯å¼€å‘",
                "å‰ç«¯Reactå¼€å‘",
                "æ•°æ®åˆ†æå¸ˆ",
                "æœºå™¨å­¦ä¹ å·¥ç¨‹å¸ˆ"
            ]
            
            search_results = []
            
            for query in search_queries:
                # æµ‹è¯•ä¸åŒçš„kå€¼
                for k in [1, 3, 5, 10]:
                    start_time = time.time()
                    
                    try:
                        results = coordinator.vector_manager.search_similar_jobs(query, k=k)
                        
                        end_time = time.time()
                        
                        search_results.append({
                            'query': query,
                            'k': k,
                            'search_time_ms': round((end_time - start_time) * 1000, 2),
                            'results_count': len(results),
                            'success': True
                        })
                        
                    except Exception as e:
                        search_results.append({
                            'query': query,
                            'k': k,
                            'success': False,
                            'error': str(e)
                        })
            
            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            successful_searches = [r for r in search_results if r.get('success', False)]
            
            if successful_searches:
                avg_search_time = sum(r['search_time_ms'] for r in successful_searches) / len(successful_searches)
                
                benchmark = {
                    'test_name': 'å‘é‡æœç´¢æ€§èƒ½',
                    'total_searches': len(search_results),
                    'successful_searches': len(successful_searches),
                    'avg_search_time_ms': round(avg_search_time, 2),
                    'min_search_time_ms': min(r['search_time_ms'] for r in successful_searches),
                    'max_search_time_ms': max(r['search_time_ms'] for r in successful_searches),
                    'search_results': search_results
                }
            else:
                benchmark = {
                    'test_name': 'å‘é‡æœç´¢æ€§èƒ½',
                    'total_searches': len(search_results),
                    'successful_searches': 0,
                    'error': 'All searches failed',
                    'search_results': search_results
                }
            
            self.benchmark_results['benchmarks'].append(benchmark)
            
            print(f"   å¹³å‡æœç´¢æ—¶é—´: {benchmark.get('avg_search_time_ms', 'N/A')} ms")
            print(f"   æœç´¢æˆåŠŸç‡: {len(successful_searches)}/{len(search_results)}")
            
            # æ¸…ç†èµ„æº
            coordinator.cleanup_system()
            
        except Exception as e:
            print(f"   âŒ å‘é‡æœç´¢æµ‹è¯•å¤±è´¥: {e}")
    
    async def benchmark_concurrent_processing(self):
        """åŸºå‡†æµ‹è¯•ï¼šå¹¶å‘å¤„ç†æ€§èƒ½"""
        print("\nğŸ”„ åŸºå‡†æµ‹è¯•4: å¹¶å‘å¤„ç†æ€§èƒ½")
        
        try:
            coordinator = RAGSystemCoordinator(self.config)
            coordinator.initialize_system()
            
            jobs = coordinator.db_reader.get_jobs_for_rag_processing(limit=20)
            
            if not jobs:
                print("   âš ï¸ æ²¡æœ‰å¯ç”¨çš„æµ‹è¯•æ•°æ®")
                return
            
            # æµ‹è¯•ä¸åŒå¹¶å‘çº§åˆ«
            concurrency_levels = [1, 2, 4, 8]
            concurrent_results = []
            
            for concurrency in concurrency_levels:
                print(f"   æµ‹è¯•å¹¶å‘çº§åˆ«: {concurrency}")
                
                start_time = time.time()
                processed_count = 0
                error_count = 0
                
                # ä½¿ç”¨çº¿ç¨‹æ± è¿›è¡Œå¹¶å‘å¤„ç†
                with ThreadPoolExecutor(max_workers=concurrency) as executor:
                    futures = []
                    
                    for job in jobs[:10]:  # å¤„ç†å‰10ä¸ªèŒä½
                        future = executor.submit(self._process_single_job, coordinator, job)
                        futures.append(future)
                    
                    # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
                    for future in futures:
                        try:
                            result = future.result(timeout=30)  # 30ç§’è¶…æ—¶
                            if result:
                                processed_count += 1
                            else:
                                error_count += 1
                        except Exception:
                            error_count += 1
                
                end_time = time.time()
                
                concurrent_result = {
                    'concurrency_level': concurrency,
                    'processed_jobs': processed_count,
                    'error_count': error_count,
                    'total_time_seconds': round(end_time - start_time, 3),
                    'jobs_per_second': round(processed_count / (end_time - start_time), 2) if end_time > start_time else 0
                }
                
                concurrent_results.append(concurrent_result)
                
                print(f"     å¤„ç†é€Ÿåº¦: {concurrent_result['jobs_per_second']} èŒä½/ç§’")
                print(f"     æˆåŠŸç‡: {processed_count}/{processed_count + error_count}")
            
            benchmark = {
                'test_name': 'å¹¶å‘å¤„ç†æ€§èƒ½',
                'concurrent_results': concurrent_results,
                'optimal_concurrency': max(concurrent_results, key=lambda x: x['jobs_per_second'])['concurrency_level']
            }
            
            self.benchmark_results['benchmarks'].append(benchmark)
            
            # æ¸…ç†èµ„æº
            coordinator.cleanup_system()
            
        except Exception as e:
            print(f"   âŒ å¹¶å‘å¤„ç†æµ‹è¯•å¤±è´¥: {e}")
    
    def _process_single_job(self, coordinator, job):
        """å¤„ç†å•ä¸ªèŒä½ï¼ˆç”¨äºå¹¶å‘æµ‹è¯•ï¼‰"""
        try:
            job_structure = coordinator.job_processor._fallback_extraction_from_db(job)
            documents = coordinator.job_processor.create_documents(
                job_structure,
                job_id=job.get('job_id'),
                job_url=job.get('url')
            )
            coordinator.vector_manager.add_job_documents(documents, job.get('job_id'))
            return True
        except Exception:
            return False
    
    async def benchmark_memory_usage(self):
        """åŸºå‡†æµ‹è¯•ï¼šå†…å­˜ä½¿ç”¨æƒ…å†µ"""
        print("\nğŸ’¾ åŸºå‡†æµ‹è¯•5: å†…å­˜ä½¿ç”¨æƒ…å†µ")
        
        try:
            # ç›‘æ§èµ„æºä½¿ç”¨
            resource_monitor = threading.Thread(
                target=lambda: self._monitor_resources(60),
                daemon=True
            )
            
            start_memory = psutil.virtual_memory().used / (1024**3)
            
            coordinator = RAGSystemCoordinator(self.config)
            coordinator.initialize_system()
            
            jobs = coordinator.db_reader.get_jobs_for_rag_processing(limit=30)
            
            memory_checkpoints = []
            
            # å¤„ç†ä¸åŒæ•°é‡çš„èŒä½ï¼Œè®°å½•å†…å­˜ä½¿ç”¨
            for i, checkpoint in enumerate([5, 10, 15, 20, 25, 30]):
                if checkpoint > len(jobs):
                    break
                
                # å¤„ç†åˆ°å½“å‰æ£€æŸ¥ç‚¹
                for job in jobs[i*5:checkpoint]:
                    try:
                        job_structure = coordinator.job_processor._fallback_extraction_from_db(job)
                        documents = coordinator.job_processor.create_documents(
                            job_structure,
                            job_id=job.get('job_id'),
                            job_url=job.get('url')
                        )
                        coordinator.vector_manager.add_job_documents(documents, job.get('job_id'))
                    except:
                        continue
                
                current_memory = psutil.virtual_memory().used / (1024**3)
                
                memory_checkpoints.append({
                    'processed_jobs': checkpoint,
                    'memory_used_gb': round(current_memory, 2),
                    'memory_increase_gb': round(current_memory - start_memory, 2)
                })
                
                print(f"   å¤„ç† {checkpoint} ä¸ªèŒä½åå†…å­˜ä½¿ç”¨: {round(current_memory, 2)} GB")
            
            benchmark = {
                'test_name': 'å†…å­˜ä½¿ç”¨æƒ…å†µ',
                'start_memory_gb': round(start_memory, 2),
                'memory_checkpoints': memory_checkpoints,
                'peak_memory_increase_gb': max(cp['memory_increase_gb'] for cp in memory_checkpoints) if memory_checkpoints else 0
            }
            
            self.benchmark_results['benchmarks'].append(benchmark)
            
            # æ¸…ç†èµ„æº
            coordinator.cleanup_system()
            
        except Exception as e:
            print(f"   âŒ å†…å­˜ä½¿ç”¨æµ‹è¯•å¤±è´¥: {e}")
    
    def generate_benchmark_report(self):
        """ç”Ÿæˆæ€§èƒ½åŸºå‡†æŠ¥å‘Š"""
        print("\n" + "="*60)
        print("ğŸ“Š RAGç³»ç»Ÿæ€§èƒ½åŸºå‡†æŠ¥å‘Š")
        print("="*60)
        
        # ç³»ç»Ÿä¿¡æ¯
        sys_info = self.benchmark_results['system_info']
        print(f"ç³»ç»Ÿä¿¡æ¯:")
        print(f"  CPUæ ¸å¿ƒæ•°: {sys_info['cpu_count']}")
        print(f"  æ€»å†…å­˜: {sys_info['memory_total_gb']} GB")
        print(f"  Pythonç‰ˆæœ¬: {sys_info['python_version']}")
        print(f"  å¹³å°: {sys_info['platform']}")
        
        # åŸºå‡†æµ‹è¯•ç»“æœ
        print(f"\nåŸºå‡†æµ‹è¯•ç»“æœ:")
        for benchmark in self.benchmark_results['benchmarks']:
            print(f"\nğŸ“‹ {benchmark['test_name']}")
            
            if benchmark['test_name'] == 'ç³»ç»Ÿåˆå§‹åŒ–æ€§èƒ½':
                if benchmark.get('avg_init_time_seconds'):
                    print(f"  å¹³å‡åˆå§‹åŒ–æ—¶é—´: {benchmark['avg_init_time_seconds']} ç§’")
                    print(f"  å¹³å‡å†…å­˜å¢é•¿: {benchmark['avg_memory_increase_mb']} MB")
                    print(f"  æˆåŠŸç‡: {benchmark['successful_runs']}/{benchmark['runs']}")
            
            elif benchmark['test_name'] == 'æ‰¹é‡å¤„ç†æ€§èƒ½':
                print(f"  æœ€ä¼˜æ‰¹æ¬¡å¤§å°: {benchmark.get('optimal_batch_size', 'N/A')}")
                for result in benchmark.get('batch_results', []):
                    print(f"    æ‰¹æ¬¡å¤§å° {result['batch_size']}: {result['jobs_per_second']} èŒä½/ç§’")
            
            elif benchmark['test_name'] == 'å‘é‡æœç´¢æ€§èƒ½':
                if benchmark.get('avg_search_time_ms'):
                    print(f"  å¹³å‡æœç´¢æ—¶é—´: {benchmark['avg_search_time_ms']} ms")
                    print(f"  æœç´¢æˆåŠŸç‡: {benchmark['successful_searches']}/{benchmark['total_searches']}")
            
            elif benchmark['test_name'] == 'å¹¶å‘å¤„ç†æ€§èƒ½':
                print(f"  æœ€ä¼˜å¹¶å‘çº§åˆ«: {benchmark.get('optimal_concurrency', 'N/A')}")
                for result in benchmark.get('concurrent_results', []):
                    print(f"    å¹¶å‘çº§åˆ« {result['concurrency_level']}: {result['jobs_per_second']} èŒä½/ç§’")
            
            elif benchmark['test_name'] == 'å†…å­˜ä½¿ç”¨æƒ…å†µ':
                print(f"  å³°å€¼å†…å­˜å¢é•¿: {benchmark.get('peak_memory_increase_gb', 'N/A')} GB")
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = f"./test_reports/rag_performance_benchmark_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        Path(report_file).parent.mkdir(parents=True, exist_ok=True)
        
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(self.benchmark_results, f, ensure_ascii=False, indent=2, default=str)
        
        print(f"\nğŸ’¾ æ€§èƒ½åŸºå‡†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
    
    async def run_all_benchmarks(self):
        """è¿è¡Œæ‰€æœ‰æ€§èƒ½åŸºå‡†æµ‹è¯•"""
        print("ğŸƒâ€â™‚ï¸ å¼€å§‹RAGç³»ç»Ÿæ€§èƒ½åŸºå‡†æµ‹è¯•")
        print("="*60)
        
        try:
            await self.benchmark_system_initialization()
            await self.benchmark_batch_processing()
            await self.benchmark_vector_search_performance()
            await self.benchmark_concurrent_processing()
            await self.benchmark_memory_usage()
            
            self.generate_benchmark_report()
            
            return True
            
        except Exception as e:
            print(f"âŒ æ€§èƒ½åŸºå‡†æµ‹è¯•å¼‚å¸¸: {e}")
            import traceback
            traceback.print_exc()
            return False

async def main():
    """ä¸»å‡½æ•°"""
    # é…ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.ERROR,  # åªæ˜¾ç¤ºé”™è¯¯æ—¥å¿—
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # åˆ›å»ºåŸºå‡†æµ‹è¯•å™¨å¹¶è¿è¡Œ
    benchmark = PerformanceBenchmark()
    success = await benchmark.run_all_benchmarks()
    
    return success

if __name__ == "__main__":
    success = asyncio.run(main())
    sys.exit(0 if success else 1)