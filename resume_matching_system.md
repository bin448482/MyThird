# åŸºäºLLMçš„æ™ºèƒ½ç®€å†åˆ†æå’ŒåŒ¹é…åº¦è®¡ç®—ç³»ç»Ÿ

## ğŸ¯ ç³»ç»Ÿæ¦‚è¿°

åŸºäºä½ å³å°†å­¦ä¹ çš„LangChainã€LlamaIndexå’Œå‘é‡æ•°æ®åº“æŠ€æœ¯ï¼Œè®¾è®¡ä¸€ä¸ªæ™ºèƒ½ç®€å†åˆ†æå’ŒèŒä½åŒ¹é…ç³»ç»Ÿï¼Œç»“åˆä½ ç°æœ‰çš„æ•°æ®å·¥ç¨‹å’ŒAI/MLèƒŒæ™¯ï¼Œæ‰“é€ ä¸€ä¸ªä¼ä¸šçº§çš„äººæ‰åŒ¹é…è§£å†³æ–¹æ¡ˆã€‚

## ğŸ—ï¸ ç³»ç»Ÿæ¶æ„è®¾è®¡

### æ ¸å¿ƒæŠ€æœ¯æ ˆ
```
å‰ç«¯å±‚: React + TypeScript + Ant Design
APIå±‚: FastAPI + Python
LLMå±‚: LangChain + OpenAI/Azure OpenAI
æ•°æ®å±‚: LlamaIndex + å‘é‡æ•°æ®åº“ (Pinecone/Weaviate)
å­˜å‚¨å±‚: PostgreSQL + Redis
éƒ¨ç½²å±‚: Azure Container Apps + Azure Functions
```

### ç³»ç»Ÿæ¶æ„å›¾
```mermaid
graph TB
    A[ç”¨æˆ·ç•Œé¢] --> B[API Gateway]
    B --> C[ç®€å†è§£ææœåŠ¡]
    B --> D[èŒä½åŒ¹é…æœåŠ¡]
    B --> E[åˆ†ææŠ¥å‘ŠæœåŠ¡]
    
    C --> F[LangChainå¤„ç†é“¾]
    D --> G[å‘é‡ç›¸ä¼¼åº¦è®¡ç®—]
    E --> H[LLMåˆ†æå¼•æ“]
    
    F --> I[æ–‡æ¡£è§£æ]
    F --> J[å®ä½“æŠ½å–]
    F --> K[æŠ€èƒ½æ ‡å‡†åŒ–]
    
    G --> L[å‘é‡æ•°æ®åº“]
    G --> M[è¯­ä¹‰æœç´¢]
    
    H --> N[åŒ¹é…åº¦è¯„åˆ†]
    H --> O[æ”¹è¿›å»ºè®®]
    
    L --> P[Pinecone/Weaviate]
    I --> Q[PostgreSQL]
    K --> R[Redisç¼“å­˜]
```

## ğŸ“‹ åŠŸèƒ½æ¨¡å—è®¾è®¡

### 1. ç®€å†è§£ææ¨¡å—

#### æŠ€æœ¯å®ç°
```python
from langchain.document_loaders import PyPDFLoader, Docx2txtLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate

class ResumeParser:
    def __init__(self):
        self.llm = AzureOpenAI(temperature=0.1)
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200
        )
        
    def parse_resume(self, file_path: str) -> dict:
        """è§£æç®€å†æ–‡ä»¶ï¼Œæå–ç»“æ„åŒ–ä¿¡æ¯"""
        
        # 1. æ–‡æ¡£åŠ è½½
        if file_path.endswith('.pdf'):
            loader = PyPDFLoader(file_path)
        elif file_path.endswith('.docx'):
            loader = Docx2txtLoader(file_path)
        
        documents = loader.load()
        
        # 2. ä¿¡æ¯æŠ½å–
        extraction_prompt = PromptTemplate(
            input_variables=["resume_text"],
            template="""
            ä»ä»¥ä¸‹ç®€å†ä¸­æå–ç»“æ„åŒ–ä¿¡æ¯ï¼Œä»¥JSONæ ¼å¼è¿”å›ï¼š
            
            ç®€å†å†…å®¹ï¼š
            {resume_text}
            
            è¯·æå–ä»¥ä¸‹ä¿¡æ¯ï¼š
            {{
                "personal_info": {{
                    "name": "å§“å",
                    "phone": "ç”µè¯",
                    "email": "é‚®ç®±",
                    "location": "åœ°å€"
                }},
                "education": [
                    {{
                        "school": "å­¦æ ¡åç§°",
                        "degree": "å­¦ä½",
                        "major": "ä¸“ä¸š",
                        "graduation_year": "æ¯•ä¸šå¹´ä»½"
                    }}
                ],
                "work_experience": [
                    {{
                        "company": "å…¬å¸åç§°",
                        "position": "èŒä½",
                        "start_date": "å¼€å§‹æ—¶é—´",
                        "end_date": "ç»“æŸæ—¶é—´",
                        "responsibilities": ["èŒè´£æè¿°"],
                        "achievements": ["æˆå°±æè¿°"]
                    }}
                ],
                "skills": {{
                    "technical_skills": ["æŠ€æœ¯æŠ€èƒ½"],
                    "programming_languages": ["ç¼–ç¨‹è¯­è¨€"],
                    "frameworks": ["æ¡†æ¶å·¥å…·"],
                    "databases": ["æ•°æ®åº“"],
                    "cloud_platforms": ["äº‘å¹³å°"]
                }},
                "projects": [
                    {{
                        "name": "é¡¹ç›®åç§°",
                        "description": "é¡¹ç›®æè¿°",
                        "technologies": ["ä½¿ç”¨æŠ€æœ¯"],
                        "achievements": ["é¡¹ç›®æˆæœ"]
                    }}
                ],
                "certifications": ["è®¤è¯è¯ä¹¦"],
                "languages": ["è¯­è¨€èƒ½åŠ›"]
            }}
            """
        )
        
        extraction_chain = LLMChain(llm=self.llm, prompt=extraction_prompt)
        
        # 3. æ‰§è¡ŒæŠ½å–
        resume_text = "\n".join([doc.page_content for doc in documents])
        result = extraction_chain.run(resume_text=resume_text)
        
        return json.loads(result)
```

#### æŠ€èƒ½æ ‡å‡†åŒ–å¤„ç†
```python
class SkillNormalizer:
    def __init__(self):
        self.skill_mapping = {
            # ç¼–ç¨‹è¯­è¨€æ ‡å‡†åŒ–
            "python": ["python", "py", "python3"],
            "javascript": ["javascript", "js", "node.js", "nodejs"],
            "java": ["java", "jvm"],
            # æ¡†æ¶æ ‡å‡†åŒ–
            "react": ["react", "reactjs", "react.js"],
            "vue": ["vue", "vuejs", "vue.js"],
            # æ•°æ®åº“æ ‡å‡†åŒ–
            "mysql": ["mysql", "my sql"],
            "postgresql": ["postgresql", "postgres", "pg"],
            # äº‘å¹³å°æ ‡å‡†åŒ–
            "azure": ["azure", "microsoft azure", "ms azure"],
            "aws": ["aws", "amazon web services"]
        }
        
    def normalize_skills(self, skills: List[str]) -> List[str]:
        """æ ‡å‡†åŒ–æŠ€èƒ½åç§°"""
        normalized = []
        for skill in skills:
            skill_lower = skill.lower().strip()
            
            # æŸ¥æ‰¾æ ‡å‡†åŒ–æ˜ å°„
            for standard_name, variants in self.skill_mapping.items():
                if skill_lower in variants:
                    if standard_name not in normalized:
                        normalized.append(standard_name)
                    break
            else:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ˜ å°„ï¼Œä¿ç•™åŸå§‹æŠ€èƒ½
                normalized.append(skill)
                
        return normalized
```

### 2. å‘é‡åŒ–å’Œç´¢å¼•æ¨¡å—

#### LlamaIndexå®ç°
```python
from llama_index import VectorStoreIndex, ServiceContext
from llama_index.vector_stores import PineconeVectorStore
from llama_index.embeddings import AzureOpenAIEmbedding
import pinecone

class ResumeVectorStore:
    def __init__(self):
        # åˆå§‹åŒ–Pinecone
        pinecone.init(
            api_key=os.getenv("PINECONE_API_KEY"),
            environment=os.getenv("PINECONE_ENVIRONMENT")
        )
        
        # åˆ›å»ºå‘é‡å­˜å‚¨
        self.vector_store = PineconeVectorStore(
            pinecone_index=pinecone.Index("resume-index"),
            namespace="resumes"
        )
        
        # é…ç½®åµŒå…¥æ¨¡å‹
        self.embed_model = AzureOpenAIEmbedding(
            model="text-embedding-ada-002",
            deployment_name="text-embedding-ada-002"
        )
        
        # æœåŠ¡ä¸Šä¸‹æ–‡
        self.service_context = ServiceContext.from_defaults(
            embed_model=self.embed_model
        )
        
    def index_resume(self, resume_data: dict, resume_id: str):
        """å°†ç®€å†æ•°æ®å‘é‡åŒ–å¹¶å­˜å‚¨"""
        
        # æ„å»ºç®€å†æ–‡æ¡£
        resume_text = self._build_resume_text(resume_data)
        
        # åˆ›å»ºæ–‡æ¡£å¯¹è±¡
        from llama_index import Document
        document = Document(
            text=resume_text,
            metadata={
                "resume_id": resume_id,
                "name": resume_data["personal_info"]["name"],
                "skills": resume_data["skills"]["technical_skills"],
                "experience_years": self._calculate_experience(resume_data),
                "education_level": self._get_education_level(resume_data)
            }
        )
        
        # åˆ›å»ºç´¢å¼•
        index = VectorStoreIndex.from_documents(
            [document],
            vector_store=self.vector_store,
            service_context=self.service_context
        )
        
        return index
    
    def _build_resume_text(self, resume_data: dict) -> str:
        """æ„å»ºç”¨äºå‘é‡åŒ–çš„ç®€å†æ–‡æœ¬"""
        sections = []
        
        # ä¸ªäººä¿¡æ¯
        personal = resume_data["personal_info"]
        sections.append(f"å§“å: {personal['name']}")
        
        # æŠ€èƒ½éƒ¨åˆ†
        skills = resume_data["skills"]
        all_skills = (
            skills["technical_skills"] + 
            skills["programming_languages"] + 
            skills["frameworks"] + 
            skills["databases"] + 
            skills["cloud_platforms"]
        )
        sections.append(f"æŠ€èƒ½: {', '.join(all_skills)}")
        
        # å·¥ä½œç»éªŒ
        for exp in resume_data["work_experience"]:
            exp_text = f"""
            å…¬å¸: {exp['company']}
            èŒä½: {exp['position']}
            èŒè´£: {'; '.join(exp['responsibilities'])}
            æˆå°±: {'; '.join(exp['achievements'])}
            """
            sections.append(exp_text)
        
        # é¡¹ç›®ç»éªŒ
        for project in resume_data["projects"]:
            project_text = f"""
            é¡¹ç›®: {project['name']}
            æè¿°: {project['description']}
            æŠ€æœ¯: {', '.join(project['technologies'])}
            æˆæœ: {'; '.join(project['achievements'])}
            """
            sections.append(project_text)
        
        return "\n\n".join(sections)
```

### 3. èŒä½åŒ¹é…æ¨¡å—

#### è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—
```python
class JobMatcher:
    def __init__(self, vector_store: ResumeVectorStore):
        self.vector_store = vector_store
        self.llm = AzureOpenAI(temperature=0.1)
        
    def calculate_match_score(self, resume_data: dict, job_description: str) -> dict:
        """è®¡ç®—ç®€å†ä¸èŒä½çš„åŒ¹é…åº¦"""
        
        # 1. æŠ€èƒ½åŒ¹é…åˆ†æ
        skill_match = self._analyze_skill_match(resume_data, job_description)
        
        # 2. ç»éªŒåŒ¹é…åˆ†æ
        experience_match = self._analyze_experience_match(resume_data, job_description)
        
        # 3. æ•™è‚²èƒŒæ™¯åŒ¹é…
        education_match = self._analyze_education_match(resume_data, job_description)
        
        # 4. è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—
        semantic_similarity = self._calculate_semantic_similarity(resume_data, job_description)
        
        # 5. ç»¼åˆè¯„åˆ†
        overall_score = self._calculate_overall_score({
            "skill_match": skill_match,
            "experience_match": experience_match,
            "education_match": education_match,
            "semantic_similarity": semantic_similarity
        })
        
        return {
            "overall_score": overall_score,
            "detailed_scores": {
                "skill_match": skill_match,
                "experience_match": experience_match,
                "education_match": education_match,
                "semantic_similarity": semantic_similarity
            },
            "recommendations": self._generate_recommendations(resume_data, job_description)
        }
    
    def _analyze_skill_match(self, resume_data: dict, job_description: str) -> float:
        """åˆ†ææŠ€èƒ½åŒ¹é…åº¦"""
        
        # æå–èŒä½è¦æ±‚çš„æŠ€èƒ½
        skill_extraction_prompt = PromptTemplate(
            input_variables=["job_description"],
            template="""
            ä»ä»¥ä¸‹èŒä½æè¿°ä¸­æå–æ‰€éœ€çš„æŠ€èƒ½è¦æ±‚ï¼Œä»¥JSONæ ¼å¼è¿”å›ï¼š
            
            èŒä½æè¿°ï¼š
            {job_description}
            
            è¯·æå–ï¼š
            {{
                "required_skills": ["å¿…éœ€æŠ€èƒ½"],
                "preferred_skills": ["ä¼˜é€‰æŠ€èƒ½"],
                "programming_languages": ["ç¼–ç¨‹è¯­è¨€"],
                "frameworks": ["æ¡†æ¶å·¥å…·"],
                "databases": ["æ•°æ®åº“"],
                "cloud_platforms": ["äº‘å¹³å°"]
            }}
            """
        )
        
        chain = LLMChain(llm=self.llm, prompt=skill_extraction_prompt)
        job_skills_result = chain.run(job_description=job_description)
        job_skills = json.loads(job_skills_result)
        
        # è®¡ç®—æŠ€èƒ½åŒ¹é…åº¦
        resume_skills = set()
        for skill_category in resume_data["skills"].values():
            if isinstance(skill_category, list):
                resume_skills.update([s.lower() for s in skill_category])
        
        required_skills = set([s.lower() for s in job_skills["required_skills"]])
        preferred_skills = set([s.lower() for s in job_skills["preferred_skills"]])
        
        # å¿…éœ€æŠ€èƒ½åŒ¹é…åº¦ (æƒé‡70%)
        required_match = len(resume_skills & required_skills) / len(required_skills) if required_skills else 1.0
        
        # ä¼˜é€‰æŠ€èƒ½åŒ¹é…åº¦ (æƒé‡30%)
        preferred_match = len(resume_skills & preferred_skills) / len(preferred_skills) if preferred_skills else 0.0
        
        return required_match * 0.7 + preferred_match * 0.3
    
    def _calculate_semantic_similarity(self, resume_data: dict, job_description: str) -> float:
        """è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦"""
        
        # æ„å»ºç®€å†æ–‡æœ¬
        resume_text = self.vector_store._build_resume_text(resume_data)
        
        # ä½¿ç”¨å‘é‡ç›¸ä¼¼åº¦è®¡ç®—
        from sentence_transformers import SentenceTransformer
        model = SentenceTransformer('all-MiniLM-L6-v2')
        
        # è®¡ç®—åµŒå…¥å‘é‡
        resume_embedding = model.encode([resume_text])
        job_embedding = model.encode([job_description])
        
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        from sklearn.metrics.pairwise import cosine_similarity
        similarity = cosine_similarity(resume_embedding, job_embedding)[0][0]
        
        return float(similarity)
```

### 4. æ™ºèƒ½åˆ†æå’Œå»ºè®®æ¨¡å—

#### LangChainåˆ†æé“¾
```python
class ResumeAnalyzer:
    def __init__(self):
        self.llm = AzureOpenAI(temperature=0.3)
        
    def generate_improvement_suggestions(self, resume_data: dict, job_description: str, match_score: dict) -> dict:
        """ç”Ÿæˆç®€å†æ”¹è¿›å»ºè®®"""
        
        analysis_prompt = PromptTemplate(
            input_variables=["resume_summary", "job_description", "match_scores"],
            template="""
            ä½œä¸ºä¸€åèµ„æ·±çš„HRä¸“å®¶å’ŒèŒä¸šé¡¾é—®ï¼Œè¯·åŸºäºä»¥ä¸‹ä¿¡æ¯ä¸ºå€™é€‰äººæä¾›è¯¦ç»†çš„ç®€å†æ”¹è¿›å»ºè®®ï¼š
            
            ç®€å†æ‘˜è¦ï¼š
            {resume_summary}
            
            ç›®æ ‡èŒä½æè¿°ï¼š
            {job_description}
            
            å½“å‰åŒ¹é…åº¦è¯„åˆ†ï¼š
            {match_scores}
            
            è¯·æä¾›ä»¥ä¸‹æ–¹é¢çš„å»ºè®®ï¼š
            
            1. æŠ€èƒ½æå‡å»ºè®®ï¼š
            - éœ€è¦å­¦ä¹ çš„æ–°æŠ€èƒ½
            - éœ€è¦åŠ å¼ºçš„ç°æœ‰æŠ€èƒ½
            - æ¨èçš„å­¦ä¹ èµ„æº
            
            2. ç»éªŒä¼˜åŒ–å»ºè®®ï¼š
            - å¦‚ä½•æ›´å¥½åœ°æè¿°ç°æœ‰ç»éªŒ
            - éœ€è¦è¡¥å……çš„é¡¹ç›®ç»éªŒ
            - é‡åŒ–æˆæœçš„å»ºè®®
            
            3. ç®€å†ç»“æ„ä¼˜åŒ–ï¼š
            - å†…å®¹ç»„ç»‡å»ºè®®
            - å…³é”®è¯ä¼˜åŒ–
            - æ ¼å¼æ”¹è¿›å»ºè®®
            
            4. èŒä¸šå‘å±•è·¯å¾„ï¼š
            - çŸ­æœŸç›®æ ‡ (3-6ä¸ªæœˆ)
            - ä¸­æœŸç›®æ ‡ (6-12ä¸ªæœˆ)
            - é•¿æœŸè§„åˆ’ (1-2å¹´)
            
            è¯·ä»¥JSONæ ¼å¼è¿”å›å»ºè®®ï¼š
            {{
                "skill_improvements": {{
                    "new_skills_to_learn": ["æŠ€èƒ½åˆ—è¡¨"],
                    "skills_to_strengthen": ["æŠ€èƒ½åˆ—è¡¨"],
                    "learning_resources": ["èµ„æºæ¨è"]
                }},
                "experience_optimization": {{
                    "description_improvements": ["æ”¹è¿›å»ºè®®"],
                    "missing_experiences": ["ç¼ºå¤±ç»éªŒ"],
                    "quantification_suggestions": ["é‡åŒ–å»ºè®®"]
                }},
                "resume_structure": {{
                    "content_organization": ["ç»„ç»‡å»ºè®®"],
                    "keyword_optimization": ["å…³é”®è¯å»ºè®®"],
                    "format_improvements": ["æ ¼å¼å»ºè®®"]
                }},
                "career_roadmap": {{
                    "short_term": ["çŸ­æœŸç›®æ ‡"],
                    "medium_term": ["ä¸­æœŸç›®æ ‡"],
                    "long_term": ["é•¿æœŸç›®æ ‡"]
                }}
            }}
            """
        )
        
        # æ„å»ºç®€å†æ‘˜è¦
        resume_summary = self._build_resume_summary(resume_data)
        
        # æ‰§è¡Œåˆ†æ
        analysis_chain = LLMChain(llm=self.llm, prompt=analysis_prompt)
        result = analysis_chain.run(
            resume_summary=resume_summary,
            job_description=job_description,
            match_scores=json.dumps(match_score, ensure_ascii=False, indent=2)
        )
        
        return json.loads(result)
    
    def generate_interview_questions(self, resume_data: dict, job_description: str) -> List[str]:
        """ç”Ÿæˆé’ˆå¯¹æ€§é¢è¯•é—®é¢˜"""
        
        question_prompt = PromptTemplate(
            input_variables=["resume_summary", "job_description"],
            template="""
            åŸºäºå€™é€‰äººçš„ç®€å†å’Œç›®æ ‡èŒä½ï¼Œç”Ÿæˆ10ä¸ªæœ‰é’ˆå¯¹æ€§çš„é¢è¯•é—®é¢˜ï¼š
            
            ç®€å†æ‘˜è¦ï¼š
            {resume_summary}
            
            èŒä½æè¿°ï¼š
            {job_description}
            
            è¯·ç”Ÿæˆä»¥ä¸‹ç±»å‹çš„é—®é¢˜ï¼š
            1. æŠ€æœ¯æ·±åº¦é—®é¢˜ (3ä¸ª)
            2. é¡¹ç›®ç»éªŒé—®é¢˜ (3ä¸ª)
            3. é—®é¢˜è§£å†³èƒ½åŠ›é—®é¢˜ (2ä¸ª)
            4. å›¢é˜Ÿåä½œé—®é¢˜ (2ä¸ª)
            
            ä»¥JSONæ ¼å¼è¿”å›ï¼š
            {{
                "technical_questions": ["æŠ€æœ¯é—®é¢˜"],
                "project_questions": ["é¡¹ç›®é—®é¢˜"],
                "problem_solving_questions": ["é—®é¢˜è§£å†³é—®é¢˜"],
                "teamwork_questions": ["å›¢é˜Ÿåä½œé—®é¢˜"]
            }}
            """
        )
        
        resume_summary = self._build_resume_summary(resume_data)
        
        question_chain = LLMChain(llm=self.llm, prompt=question_prompt)
        result = question_chain.run(
            resume_summary=resume_summary,
            job_description=job_description
        )
        
        return json.loads(result)
```

## ğŸš€ APIæ¥å£è®¾è®¡

### FastAPIå®ç°
```python
from fastapi import FastAPI, UploadFile, File, HTTPException
from pydantic import BaseModel
from typing import List, Optional

app = FastAPI(title="æ™ºèƒ½ç®€å†åˆ†æç³»ç»Ÿ", version="1.0.0")

class JobMatchRequest(BaseModel):
    resume_id: str
    job_description: str

class MatchResult(BaseModel):
    overall_score: float
    detailed_scores: dict
    recommendations: dict
    interview_questions: dict

@app.post("/api/resume/upload")
async def upload_resume(file: UploadFile = File(...)):
    """ä¸Šä¼ å¹¶è§£æç®€å†"""
    try:
        # ä¿å­˜æ–‡ä»¶
        file_path = f"uploads/{file.filename}"
        with open(file_path, "wb") as buffer:
            content = await file.read()
            buffer.write(content)
        
        # è§£æç®€å†
        parser = ResumeParser()
        resume_data = parser.parse_resume(file_path)
        
        # ç”Ÿæˆç®€å†ID
        resume_id = str(uuid.uuid4())
        
        # å‘é‡åŒ–å­˜å‚¨
        vector_store = ResumeVectorStore()
        vector_store.index_resume(resume_data, resume_id)
        
        # å­˜å‚¨åˆ°æ•°æ®åº“
        # ... æ•°æ®åº“æ“ä½œ
        
        return {
            "resume_id": resume_id,
            "status": "success",
            "data": resume_data
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/job/match", response_model=MatchResult)
async def calculate_job_match(request: JobMatchRequest):
    """è®¡ç®—èŒä½åŒ¹é…åº¦"""
    try:
        # è·å–ç®€å†æ•°æ®
        resume_data = get_resume_from_db(request.resume_id)
        
        # è®¡ç®—åŒ¹é…åº¦
        matcher = JobMatcher(ResumeVectorStore())
        match_result = matcher.calculate_match_score(
            resume_data, 
            request.job_description
        )
        
        # ç”Ÿæˆæ”¹è¿›å»ºè®®
        analyzer = ResumeAnalyzer()
        suggestions = analyzer.generate_improvement_suggestions(
            resume_data, 
            request.job_description, 
            match_result
        )
        
        # ç”Ÿæˆé¢è¯•é—®é¢˜
        interview_questions = analyzer.generate_interview_questions(
            resume_data, 
            request.job_description
        )
        
        return MatchResult(
            overall_score=match_result["overall_score"],
            detailed_scores=match_result["detailed_scores"],
            recommendations=suggestions,
            interview_questions=interview_questions
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/resume/search")
async def search_resumes(
    query: str,
    skills: Optional[List[str]] = None,
    experience_years: Optional[int] = None,
    limit: int = 10
):
    """è¯­ä¹‰æœç´¢ç®€å†"""
    try:
        # æ„å»ºæœç´¢æŸ¥è¯¢
        search_query = query
        if skills:
            search_query += f" æŠ€èƒ½: {', '.join(skills)}"
        if experience_years:
            search_query += f" ç»éªŒ: {experience_years}å¹´"
        
        # å‘é‡æœç´¢
        vector_store = ResumeVectorStore()
        results = vector_store.search(search_query, limit=limit)
        
        return {
            "query": search_query,
            "results": results,
            "total": len(results)
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
```

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

### 1. ç¼“å­˜ç­–ç•¥
```python
import redis
from functools import wraps

redis_client = redis.Redis(host='localhost', port=6379, db=0)

def cache_result(expiration=3600):
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            # ç”Ÿæˆç¼“å­˜é”®
            cache_key = f"{func.__name__}:{hash(str(args) + str(kwargs))}"
            
            # å°è¯•ä»ç¼“å­˜è·å–
            cached_result = redis_client.get(cache_key)
            if cached_result:
                return json.loads(cached_result)
            
            # æ‰§è¡Œå‡½æ•°å¹¶ç¼“å­˜ç»“æœ
            result = func(*args, **kwargs)
            redis_client.setex(
                cache_key, 
                expiration, 
                json.dumps(result, ensure_ascii=False)
            )
            
            return result
        return wrapper
    return decorator

# ä½¿ç”¨ç¼“å­˜è£…é¥°å™¨
@cache_result(expiration=1800)  # 30åˆ†é’Ÿç¼“å­˜
def calculate_skill_match(resume_skills, job_skills):
    # è®¡ç®—é€»è¾‘
    pass
```

### 2. æ‰¹é‡å¤„ç†
```python
class BatchProcessor:
    def __init__(self, batch_size=10):
        self.batch_size = batch_size
        
    async def process_resumes_batch(self, resume_files: List[UploadFile]):
        """æ‰¹é‡å¤„ç†ç®€å†"""
        results = []
        
        for i in range(0, len(resume_files), self.batch_size):
            batch = resume_files[i:i + self.batch_size]
            
            # å¹¶è¡Œå¤„ç†æ‰¹æ¬¡
            tasks = [self.process_single_resume(file) for file in batch]
            batch_results = await asyncio.gather(*tasks)
            
            results.extend(batch_results)
            
        return results
    
    async def process_single_resume(self, file: UploadFile):
        """å¤„ç†å•ä¸ªç®€å†"""
        # å¼‚æ­¥å¤„ç†é€»è¾‘
        pass
```

## ğŸ”§ éƒ¨ç½²å’Œç›‘æ§

### Azureéƒ¨ç½²é…ç½®
```yaml
# docker-compose.yml
version: '3.8'
services:
  api:
    build: .
    ports:
      - "8000:8000"
    environment:
      - AZURE_OPENAI_API_KEY=${AZURE_OPENAI_API_KEY}
      - PINECONE_API_KEY=${PINECONE_API_KEY}
      - DATABASE_URL=${DATABASE_URL}
    depends_on:
      - redis
      - postgres
      
  redis:
    image: redis:alpine
    ports:
      - "6379:6379"
      
  postgres:
    image: postgres:13
    environment:
      - POSTGRES_DB=resume_db
      - POSTGRES_USER=admin
      - POSTGRES_PASSWORD=${DB_PASSWORD}
    ports:
      - "5432:5432"
```

### ç›‘æ§å’Œæ—¥å¿—
```python
import logging
from prometheus_client import Counter, Histogram, generate_latest

# æŒ‡æ ‡å®šä¹‰
REQUEST_COUNT = Counter('requests_total', 'Total requests', ['method', 'endpoint'])
REQUEST_DURATION = Histogram('request_duration_seconds', 'Request duration')

# æ—¥å¿—é…ç½®
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('app.log'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

@app.middleware("http")
async def monitor_requests(request, call_next):
    start_time = time.time()
    
    response = await call_next(request)
    
    # è®°å½•æŒ‡æ ‡
    REQUEST_COUNT.labels(
        method=request.method, 
        endpoint=request.url.path
    ).inc()
    
    REQUEST_DURATION.observe(time.time() - start_time)
    
    # è®°å½•æ—¥å¿—
    logger.info(f"{request.method} {request.url.path} - {response.status_code}")
    
    return response
```

## ğŸ“ˆ å­¦ä¹ è·¯å¾„å»ºè®®

### ç¬¬1-2å‘¨ï¼šåŸºç¡€æ¡†æ¶å­¦ä¹ 
1. **LangChainåŸºç¡€**
   - å®‰è£…å’Œé…ç½®
   - åŸºæœ¬é“¾å¼è°ƒç”¨
   - æç¤ºæ¨¡æ¿ä½¿ç”¨
   - æ–‡æ¡£åŠ è½½å™¨

2. **LlamaIndexå…¥é—¨**
   - ç´¢å¼•æ„å»º
   - æŸ¥è¯¢å¼•æ“
   - å‘é‡å­˜å‚¨é›†æˆ

### ç¬¬3-4å‘¨ï¼šå‘é‡æ•°æ®åº“å®è·µ
1. **Pineconeä½¿ç”¨**
   - ç´¢å¼•åˆ›å»ºå’Œç®¡ç†
   - å‘é‡æ’å…¥å’ŒæŸ¥è¯¢
   - å…ƒæ•°æ®è¿‡æ»¤

2. **è¯­ä¹‰æœç´¢å®ç°**
   - åµŒå…¥æ¨¡å‹é€‰æ‹©
   - ç›¸ä¼¼åº¦è®¡ç®—
   - ç»“æœæ’åº

### ç¬¬5-6å‘¨ï¼šé¡¹ç›®é›†æˆå¼€å‘
1. **APIå¼€å‘**
   - FastAPIæ¡†æ¶
   - å¼‚æ­¥å¤„ç†
   - é”™è¯¯å¤„ç†

2. **ç³»ç»Ÿé›†æˆ**
   - æ•°æ®åº“è®¾è®¡
   - ç¼“å­˜ç­–ç•¥
   - æ€§èƒ½ä¼˜åŒ–

### ç¬¬7-8å‘¨ï¼šéƒ¨ç½²å’Œä¼˜åŒ–
1. **å®¹å™¨åŒ–éƒ¨ç½²**
   - Dockeré…ç½®
   - Azureéƒ¨ç½²
   - ç›‘æ§è®¾ç½®

2. **æ€§èƒ½è°ƒä¼˜**
   - æ‰¹é‡å¤„ç†
   - ç¼“å­˜ä¼˜åŒ–
   - å¹¶å‘æ§åˆ¶

## ğŸ¯ é¡¹ç›®ä»·å€¼

### æŠ€æœ¯ä»·å€¼
- **LLMåº”ç”¨å®è·µ**: æ·±å…¥ç†è§£LangChainå’ŒLlamaIndexçš„å®é™…åº”ç”¨
- **å‘é‡æ•°æ®åº“**: æŒæ¡è¯­ä¹‰æœç´¢å’Œç›¸ä¼¼åº¦è®¡ç®—æŠ€æœ¯
- **ç³»ç»Ÿæ¶æ„**: è®¾è®¡ä¼ä¸šçº§AIåº”ç”¨çš„å®Œæ•´æ¶æ„

### å•†ä¸šä»·å€¼
- **HRæ•ˆç‡æå‡**: è‡ªåŠ¨åŒ–ç®€å†ç­›é€‰ï¼Œæé«˜æ‹›è˜æ•ˆç‡
- **åŒ¹é…ç²¾åº¦**: åŸºäºè¯­ä¹‰ç†è§£çš„ç²¾å‡†åŒ¹é…
- **æ•°æ®æ´å¯Ÿ**: æä¾›æ·±å…¥çš„äººæ‰å¸‚åœºåˆ†æ

### èŒä¸šå‘å±•ä»·å€¼
- **æŠ€èƒ½è¯æ˜**: å±•ç¤ºLLMæŠ€æœ¯çš„å®é™…åº”ç”¨èƒ½åŠ›
- **é¡¹ç›®ç»éªŒ**: å®Œæ•´çš„ç«¯åˆ°ç«¯é¡¹ç›®å¼€å‘ç»éªŒ
- **è¡Œä¸šå½±å“**: åœ¨HRç§‘æŠ€é¢†åŸŸå»ºç«‹æŠ€æœ¯å½±å“åŠ›

è¿™ä¸ªé¡¹ç›®å°†å®Œç¾ç»“åˆä½ ç°æœ‰çš„æ•°æ®å·¥ç¨‹èƒŒæ™¯å’Œå³å°†å­¦ä¹ çš„LLMæŠ€æœ¯ï¼Œä¸ºä½ çš„èŒä¸šå‘å±•æä¾›å¼ºæœ‰åŠ›çš„æŠ€æœ¯æ”¯æ’‘ã€‚