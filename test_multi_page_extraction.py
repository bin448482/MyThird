"""
æµ‹è¯•å¤šé¡µèŒä½æå–åŠŸèƒ½
éªŒè¯æ•°æ®ä¸¢å¤±é—®é¢˜æ˜¯å¦å·²ä¿®å¤
"""

import asyncio
import logging
import yaml
from src.extraction.content_extractor import ContentExtractor

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def load_integration_config():
    """åŠ è½½integration_config.yamlé…ç½®"""
    try:
        with open('config/integration_config.yaml', 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        return config
    except Exception as e:
        logger.error(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
        return {}

def test_multi_page_extraction():
    """æµ‹è¯•å¤šé¡µèŒä½æå–"""
    logger.info("ğŸš€ å¼€å§‹æµ‹è¯•å¤šé¡µèŒä½æå–åŠŸèƒ½")
    
    # åŠ è½½é…ç½®
    config = load_integration_config()
    logger.info(f"âœ… é…ç½®åŠ è½½æˆåŠŸ")
    
    # ä»é…ç½®ä¸­è·å–æµ‹è¯•å‚æ•°
    search_config = config.get('search', {})
    keywords = search_config.get('keywords', {}).get('priority_1', ['Pythonå¼€å‘'])
    test_keyword = keywords[0] if keywords else 'Pythonå¼€å‘'
    
    logger.info(f"ğŸ” ä½¿ç”¨æµ‹è¯•å…³é”®è¯: {test_keyword}")
    
    # åˆå§‹åŒ–ContentExtractor
    content_extractor = ContentExtractor(config)
    
    try:
        # æµ‹è¯•ä¸åŒçš„å‚æ•°ç»„åˆ
        test_cases = [
            {
                'max_results': 40,
                'max_pages': 2,
                'description': '40ä¸ªèŒä½ï¼Œ2é¡µ'
            },
            {
                'max_results': 30,
                'max_pages': 2,
                'description': '30ä¸ªèŒä½ï¼Œ2é¡µ'
            },
            {
                'max_results': 25,
                'max_pages': 2,
                'description': '25ä¸ªèŒä½ï¼Œ2é¡µ'
            }
        ]
        
        for i, test_case in enumerate(test_cases, 1):
            logger.info(f"\n{'='*60}")
            logger.info(f"ğŸ§ª æµ‹è¯•ç”¨ä¾‹ {i}: {test_case['description']}")
            logger.info(f"{'='*60}")
            
            try:
                # è°ƒç”¨extract_from_keywordè¿›è¡Œå¤šé¡µæå–
                extracted_jobs = content_extractor.extract_from_keyword(
                    keyword=test_keyword,
                    max_results=test_case['max_results'],
                    save_results=False,  # æµ‹è¯•æ—¶ä¸ä¿å­˜
                    extract_details=False,  # æµ‹è¯•æ—¶ä¸æå–è¯¦æƒ…ï¼ŒåŠ å¿«é€Ÿåº¦
                    max_pages=test_case['max_pages']
                )
                
                logger.info(f"ğŸ“Š æå–ç»“æœåˆ†æ:")
                logger.info(f"   - æœŸæœ›èŒä½æ•°é‡: {test_case['max_results']}")
                logger.info(f"   - å®é™…æå–æ•°é‡: {len(extracted_jobs)}")
                logger.info(f"   - æœ€å¤§é¡µæ•°: {test_case['max_pages']}")
                
                # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœŸæœ›æ•°é‡
                if len(extracted_jobs) >= test_case['max_results']:
                    logger.info(f"âœ… æµ‹è¯•é€šè¿‡: æˆåŠŸæå–åˆ°è¶³å¤Ÿçš„èŒä½æ•°é‡")
                elif len(extracted_jobs) >= test_case['max_results'] * 0.8:  # 80%ä»¥ä¸Šè®¤ä¸ºåŸºæœ¬æˆåŠŸ
                    logger.warning(f"âš ï¸ éƒ¨åˆ†æˆåŠŸ: æå–æ•°é‡æ¥è¿‘æœŸæœ›å€¼")
                else:
                    logger.error(f"âŒ æµ‹è¯•å¤±è´¥: æå–æ•°é‡è¿œä½äºæœŸæœ›å€¼")
                
                # åˆ†æå‰5ä¸ªèŒä½
                logger.info(f"ğŸ“ å‰5ä¸ªèŒä½è¯¦æƒ…:")
                for j, job in enumerate(extracted_jobs[:5], 1):
                    logger.info(f"   {j}. æ ‡é¢˜: {job.get('title', 'N/A')}")
                    logger.info(f"      å…¬å¸: {job.get('company', 'N/A')}")
                    logger.info(f"      é¡µç : {job.get('page_number', 'N/A')}")
                
                # åˆ†æé¡µé¢åˆ†å¸ƒ
                page_distribution = {}
                for job in extracted_jobs:
                    page_num = job.get('page_number', 1)
                    page_distribution[page_num] = page_distribution.get(page_num, 0) + 1
                
                logger.info(f"ğŸ“„ é¡µé¢åˆ†å¸ƒ:")
                for page_num, count in sorted(page_distribution.items()):
                    logger.info(f"   - ç¬¬{page_num}é¡µ: {count}ä¸ªèŒä½")
                
                # æ£€æŸ¥æ˜¯å¦æœ‰å¤šé¡µæ•°æ®
                if len(page_distribution) > 1:
                    logger.info(f"âœ… å¤šé¡µæå–æˆåŠŸ: å…±å¤„ç†äº† {len(page_distribution)} é¡µ")
                else:
                    logger.warning(f"âš ï¸ åªå¤„ç†äº†1é¡µï¼Œå¯èƒ½å­˜åœ¨ç¿»é¡µé—®é¢˜")
                
            except Exception as e:
                logger.error(f"âŒ æµ‹è¯•ç”¨ä¾‹ {i} æ‰§è¡Œå¤±è´¥: {e}")
                import traceback
                logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
        
        logger.info(f"\n{'='*60}")
        logger.info("ğŸ‰ å¤šé¡µæå–æµ‹è¯•å®Œæˆ")
        logger.info(f"{'='*60}")
        
    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
    
    finally:
        # æ¸…ç†èµ„æº
        try:
            content_extractor.close()
            logger.info("âœ… èµ„æºæ¸…ç†å®Œæˆ")
        except:
            pass

if __name__ == "__main__":
    test_multi_page_extraction()