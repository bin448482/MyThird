#!/usr/bin/env python3
"""
RAGç³»ç»Ÿç»Ÿä¸€CLIæ¥å£

æä¾›å®Œæ•´çš„RAGç³»ç»Ÿå‘½ä»¤è¡Œæ¥å£ï¼ŒåŒ…æ‹¬æ•°æ®å¤„ç†ã€ç®€å†ä¼˜åŒ–ã€èŒä½åŒ¹é…ç­‰åŠŸèƒ½
"""

import sys
import asyncio
import argparse
import logging
import json
from pathlib import Path
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(str(Path(__file__).parent))

from src.rag.rag_system_coordinator import RAGSystemCoordinator
from src.rag.data_pipeline import RAGDataPipeline, create_progress_callback
from src.rag.resume_optimizer import ResumeOptimizer
from src.rag.vector_manager import ChromaDBManager
from src.rag.resume_document_parser import ResumeDocumentParser
from src.rag.resume_document_processor import ResumeDocumentProcessor
from src.rag.llm_factory import create_llm
from src.matcher import (
    GenericResumeJobMatcher
)
from src.matcher.generic_resume_models import (
    GenericResumeProfile
)
from src.matcher.generic_resume_matcher import GenericResumeJobMatcher
from src.matcher.generic_resume_vectorizer import GenericResumeVectorizer
from src.analysis_tools.agent import create_analysis_agent

def setup_logging(log_level: str = 'INFO', log_file: str = None):
    """è®¾ç½®æ—¥å¿—é…ç½®"""
    handlers = [logging.StreamHandler()]
    
    if log_file:
        Path(log_file).parent.mkdir(parents=True, exist_ok=True)
        handlers.append(logging.FileHandler(log_file, encoding='utf-8'))
    
    logging.basicConfig(
        level=getattr(logging, log_level.upper()),
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=handlers
    )

def load_config(config_file: str = None) -> dict:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    if config_file and Path(config_file).exists():
        with open(config_file, 'r', encoding='utf-8') as f:
            if config_file.endswith('.json'):
                return json.load(f)
            else:
                import yaml
                return yaml.safe_load(f)
    
    # é»˜è®¤é…ç½®
    return {
        'rag_system': {
            'database': {
                'path': './data/jobs.db',
                'batch_size': 50
            },
            'llm': {
                'provider': 'zhipu',
                'model': 'glm-4-flash',
                'api_key': 'your-api-key-here',
                'temperature': 0.1,
                'max_tokens': 2000
            },
            'vector_db': {
                'persist_directory': './data/test_chroma_db',
                'collection_name': 'job_positions',
                'embeddings': {
                    'model_name': 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',
                    'device': 'cpu',
                    'normalize_embeddings': True
                }
            },
            'documents': {
                'types': ['overview', 'responsibility', 'requirement', 'skills', 'basic_requirements'],
                'create_comprehensive_doc': False
            },
            'processing': {
                'skip_processed': True,
                'force_reprocess': False,
                'batch_size': 50,
                'max_retry_attempts': 3
            }
        }
    }

def load_resume(resume_file: str) -> dict:
    """åŠ è½½ç®€å†æ–‡ä»¶"""
    if not Path(resume_file).exists():
        raise FileNotFoundError(f"ç®€å†æ–‡ä»¶ä¸å­˜åœ¨: {resume_file}")
    
    with open(resume_file, 'r', encoding='utf-8') as f:
        if resume_file.endswith('.json'):
            return json.load(f)
        else:
            import yaml
            return yaml.safe_load(f)

def create_default_zhanbin_profile() -> GenericResumeProfile:
    """åˆ›å»ºé»˜è®¤çš„å å½¬ç®€å†æ¡£æ¡ˆï¼ˆé€šç”¨æ ¼å¼ï¼‰"""
    from src.matcher.generic_resume_models import GenericResumeProfile, SkillCategory, WorkExperience
    
    profile = GenericResumeProfile(
        name="å å½¬",
        phone="",
        email="",
        location="ä¸­å›½",
        total_experience_years=20,
        current_position="é«˜çº§æŠ€æœ¯ä¸“å®¶",
        current_company="",
        profile_type="zhanbin_default"
    )
    
    # æ·»åŠ æŠ€èƒ½åˆ†ç±»
    profile.add_skill_category("core_skills", [
        "Python", "Java", "JavaScript", "React", "Vue.js", "Node.js",
        "Spring Boot", "Django", "Flask", "MySQL", "PostgreSQL", "MongoDB"
    ], "advanced")
    
    profile.add_skill_category("cloud_platforms", [
        "AWS", "Azure", "Google Cloud", "Docker", "Kubernetes", "Jenkins"
    ], "advanced")
    
    profile.add_skill_category("ai_ml_skills", [
        "Machine Learning", "Deep Learning", "TensorFlow", "PyTorch",
        "Scikit-learn", "Pandas", "NumPy", "Data Science"
    ], "expert")
    
    profile.add_skill_category("data_engineering_skills", [
        "Apache Spark", "Hadoop", "Kafka", "Elasticsearch", "Redis",
        "ETL", "Data Pipeline", "Big Data"
    ], "advanced")
    
    profile.add_skill_category("management_skills", [
        "å›¢é˜Ÿç®¡ç†", "é¡¹ç›®ç®¡ç†", "æ•æ·å¼€å‘", "Scrum", "æŠ€æœ¯æ¶æ„", "ç³»ç»Ÿè®¾è®¡"
    ], "expert")
    
    # è®¾ç½®è¡Œä¸šç»éªŒ
    profile.industry_experience = {
        "äº’è”ç½‘": 0.8,
        "äººå·¥æ™ºèƒ½": 0.9,
        "é‡‘èç§‘æŠ€": 0.7,
        "ä¼ä¸šæœåŠ¡": 0.6
    }
    
    # è®¾ç½®æœŸæœ›èŒä½
    profile.preferred_positions = [
        "æŠ€æœ¯æ€»ç›‘", "æ¶æ„å¸ˆ", "AIå·¥ç¨‹å¸ˆ", "æ•°æ®ç§‘å­¦å®¶", "æŠ€æœ¯ä¸“å®¶"
    ]
    
    # è®¾ç½®è–ªèµ„æœŸæœ›
    profile.expected_salary_range = {"min": 500000, "max": 800000}
    
    # è®¾ç½®èŒä¸šç›®æ ‡
    profile.career_objectives = [
        "åœ¨äººå·¥æ™ºèƒ½å’Œå¤§æ•°æ®é¢†åŸŸå‘æŒ¥æŠ€æœ¯ä¸“é•¿",
        "å¸¦é¢†å›¢é˜Ÿå®Œæˆå…·æœ‰æŒ‘æˆ˜æ€§çš„æŠ€æœ¯é¡¹ç›®",
        "æ¨åŠ¨ä¼ä¸šæ•°å­—åŒ–è½¬å‹å’ŒæŠ€æœ¯åˆ›æ–°"
    ]
    
    return profile

def load_resume_profile(resume_data: dict) -> GenericResumeProfile:
    """ç»Ÿä¸€çš„ç®€å†åŠ è½½å‡½æ•°"""
    try:
        # ç›´æ¥ä½¿ç”¨é€šç”¨æ ¼å¼åŠ è½½
        if 'skill_categories' in resume_data:
            # æ–°çš„é€šç”¨æ ¼å¼
            return GenericResumeProfile.from_dict(resume_data)
        else:
            # æ—§æ ¼å¼æ•°æ®ï¼Œè½¬æ¢ä¸ºé€šç”¨æ ¼å¼
            # åˆ›å»ºåŸºæœ¬çš„ç®€å†æ¡£æ¡ˆ
            profile = GenericResumeProfile(
                name=resume_data.get('name', 'æœªçŸ¥ç”¨æˆ·'),
                phone=resume_data.get('phone', ''),
                email=resume_data.get('email', ''),
                location=resume_data.get('location', ''),
                total_experience_years=resume_data.get('total_experience_years', 0),
                current_position=resume_data.get('current_position', ''),
                current_company=resume_data.get('current_company', ''),
                certifications=resume_data.get('certifications', []),
                industry_experience=resume_data.get('industry_experience', {}),
                preferred_positions=resume_data.get('preferred_positions', []),
                expected_salary_range=resume_data.get('expected_salary_range', {"min": 0, "max": 0}),
                career_objectives=resume_data.get('career_objectives', []),
                profile_type='converted_from_legacy'
            )
            
            # è½¬æ¢æŠ€èƒ½æ•°æ®åˆ°æŠ€èƒ½åˆ†ç±»
            skill_mappings = [
                ('core_skills', 'core_skills'),
                ('programming_languages', 'programming_languages'),
                ('cloud_platforms', 'cloud_platforms'),
                ('ai_ml_skills', 'ai_ml_skills'),
                ('data_engineering_skills', 'data_engineering_skills'),
                ('management_skills', 'management_skills')
            ]
            
            for category_name, data_key in skill_mappings:
                if data_key in resume_data and resume_data[data_key]:
                    profile.add_skill_category(
                        category_name=category_name,
                        skills=resume_data[data_key],
                        proficiency_level='advanced'
                    )
            
            # è½¬æ¢å·¥ä½œç»éªŒ
            for exp_data in resume_data.get('work_history', []):
                from src.matcher.generic_resume_models import WorkExperience
                experience = WorkExperience(
                    company=exp_data.get('company', ''),
                    position=exp_data.get('position', ''),
                    start_date=exp_data.get('start_date', ''),
                    end_date=exp_data.get('end_date'),
                    duration_years=exp_data.get('duration_years', 0),
                    responsibilities=exp_data.get('responsibilities', []),
                    achievements=exp_data.get('achievements', []),
                    technologies=exp_data.get('technologies', []),
                    industry=exp_data.get('industry', '')
                )
                profile.add_work_experience(experience)
            
            return profile
            
    except Exception as e:
        print(f"âš ï¸ ç®€å†åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤æ¡£æ¡ˆ: {e}")
        return create_default_zhanbin_profile()

async def pipeline_command(args):
    """æ•°æ®æµæ°´çº¿å‘½ä»¤"""
    print("ğŸš€ RAGæ•°æ®æµæ°´çº¿")
    print("=" * 40)
    
    try:
        config = load_config(args.config)
        
        # è¦†ç›–å‘½ä»¤è¡Œå‚æ•°
        if args.batch_size:
            config['rag_system']['processing']['batch_size'] = args.batch_size
        if args.force_reprocess:
            config['rag_system']['processing']['force_reprocess'] = True
        
        # åˆ›å»ºè¿›åº¦å›è°ƒ
        progress_callback = create_progress_callback() if args.show_progress else None
        
        # è¿è¡Œæµæ°´çº¿
        pipeline = RAGDataPipeline(config, progress_callback)
        result = await pipeline.run_full_pipeline(
            batch_size=args.batch_size or 50,
            max_jobs=args.max_jobs,
            force_reprocess=args.force_reprocess,
            save_progress=not args.no_save
        )
        
        # æ˜¾ç¤ºç»“æœæ‘˜è¦
        print("\nğŸ“Š æ‰§è¡Œç»“æœæ‘˜è¦:")
        exec_summary = result.get('execution_summary', {})
        proc_stats = result.get('processing_statistics', {})
        
        print(f"   çŠ¶æ€: {exec_summary.get('status', 'unknown')}")
        print(f"   æ‰§è¡Œæ—¶é—´: {exec_summary.get('execution_time', 0):.1f} ç§’")
        print(f"   å¤„ç†èŒä½: {proc_stats.get('processed_jobs', 0)}")
        print(f"   æˆåŠŸç‡: {proc_stats.get('success_rate', 0):.1f}%")
        
        return True
        
    except Exception as e:
        print(f"âŒ æµæ°´çº¿æ‰§è¡Œå¤±è´¥: {e}")
        return False

async def status_command(args):
    """ç³»ç»ŸçŠ¶æ€å‘½ä»¤"""
    print("ğŸ“Š RAGç³»ç»ŸçŠ¶æ€")
    print("=" * 30)
    
    try:
        config = load_config(args.config)
        coordinator = RAGSystemCoordinator(config)
        
        if not coordinator.initialize_system():
            print("âŒ ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥")
            return False
        
        # è·å–ç³»ç»ŸçŠ¶æ€
        system_status = coordinator.get_system_status()
        progress = coordinator.get_processing_progress()
        
        # æ˜¾ç¤ºç»„ä»¶çŠ¶æ€
        print("ç»„ä»¶çŠ¶æ€:")
        components = system_status.get('components', {})
        for comp_name, comp_status in components.items():
            status_icon = "âœ…" if comp_status else "âŒ"
            print(f"  {comp_name}: {status_icon}")
        
        # æ˜¾ç¤ºæ•°æ®ç»Ÿè®¡
        db_stats = progress.get('database_stats', {})
        print(f"\næ•°æ®åº“ç»Ÿè®¡:")
        print(f"  æ€»èŒä½æ•°: {db_stats.get('total', 0)}")
        print(f"  å·²å¤„ç†: {db_stats.get('processed', 0)}")
        print(f"  æœªå¤„ç†: {db_stats.get('unprocessed', 0)}")
        print(f"  å¤„ç†ç‡: {db_stats.get('processing_rate', 0):.1f}%")
        
        vector_stats = progress.get('vector_stats', {})
        print(f"\nå‘é‡æ•°æ®åº“:")
        print(f"  æ–‡æ¡£æ•°é‡: {vector_stats.get('document_count', 0)}")
        print(f"  é›†åˆåç§°: {vector_stats.get('collection_name', 'N/A')}")
        
        return True
        
    except Exception as e:
        print(f"âŒ çŠ¶æ€æŸ¥è¯¢å¤±è´¥: {e}")
        return False

async def optimize_command(args):
    """ç®€å†ä¼˜åŒ–å‘½ä»¤"""
    print("ğŸ“ ç®€å†ä¼˜åŒ–")
    print("=" * 20)
    
    try:
        # åŠ è½½é…ç½®å’Œç®€å†
        config = load_config(args.config)
        resume = load_resume(args.resume)
        
        # åˆå§‹åŒ–ç³»ç»Ÿ
        coordinator = RAGSystemCoordinator(config)
        if not coordinator.initialize_system():
            print("âŒ ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥")
            return False
        
        optimizer = ResumeOptimizer(coordinator, config['rag_system']['llm'])
        
        # æ ¹æ®å­å‘½ä»¤æ‰§è¡Œä¸åŒæ“ä½œ
        if args.action == 'analyze':
            # ç®€å†å·®è·åˆ†æ
            target_jobs = args.target_jobs.split(',') if args.target_jobs else []
            if not target_jobs:
                # è‡ªåŠ¨æŸ¥æ‰¾ç›¸å…³èŒä½
                all_jobs = coordinator.db_reader.get_jobs_for_rag_processing(limit=5)
                target_jobs = [job['job_id'] for job in all_jobs[:3]]
            
            print(f"åˆ†æç›®æ ‡èŒä½: {len(target_jobs)} ä¸ª")
            
            if not args.dry_run:
                result = await optimizer.analyze_resume_gaps(resume, target_jobs)
                
                if 'error' in result:
                    print(f"âŒ åˆ†æå¤±è´¥: {result['error']}")
                    return False
                
                # æ˜¾ç¤ºåˆ†æç»“æœ
                summary = result.get('summary', {})
                print(f"\nğŸ“Š åˆ†æç»“æœ:")
                print(f"   å¹³å‡åŒ¹é…åº¦: {summary.get('average_match_score', 0)}")
                print(f"   åˆ†æèŒä½æ•°: {summary.get('total_jobs_analyzed', 0)}")
                
                # ä¿å­˜ç»“æœ
                if args.output:
                    with open(args.output, 'w', encoding='utf-8') as f:
                        json.dump(result, f, ensure_ascii=False, indent=2, default=str)
                    print(f"   ç»“æœå·²ä¿å­˜åˆ°: {args.output}")
            else:
                print("ğŸ” å¹²è¿è¡Œæ¨¡å¼ - è·³è¿‡å®é™…åˆ†æ")
        
        elif args.action == 'optimize':
            # ç®€å†å†…å®¹ä¼˜åŒ–
            target_job_id = args.target_job or coordinator.db_reader.get_jobs_for_rag_processing(limit=1)[0]['job_id']
            
            print(f"ä¼˜åŒ–ç›®æ ‡èŒä½: {target_job_id}")
            
            if not args.dry_run:
                result = await optimizer.optimize_resume_content(
                    resume, 
                    target_job_id, 
                    args.focus_areas.split(',') if args.focus_areas else None
                )
                
                if 'error' in result:
                    print(f"âŒ ä¼˜åŒ–å¤±è´¥: {result['error']}")
                    return False
                
                print("âœ… ç®€å†ä¼˜åŒ–å®Œæˆ")
                
                # ä¿å­˜ç»“æœ
                if args.output:
                    with open(args.output, 'w', encoding='utf-8') as f:
                        json.dump(result, f, ensure_ascii=False, indent=2, default=str)
                    print(f"   ç»“æœå·²ä¿å­˜åˆ°: {args.output}")
            else:
                print("ğŸ” å¹²è¿è¡Œæ¨¡å¼ - è·³è¿‡å®é™…ä¼˜åŒ–")
        
        elif args.action == 'cover-letter':
            # ç”Ÿæˆæ±‚èŒä¿¡
            target_job_id = args.target_job or coordinator.db_reader.get_jobs_for_rag_processing(limit=1)[0]['job_id']
            
            print(f"ç”Ÿæˆæ±‚èŒä¿¡ï¼Œç›®æ ‡èŒä½: {target_job_id}")
            
            if not args.dry_run:
                cover_letter = await optimizer.generate_cover_letter(resume, target_job_id)
                
                print("âœ… æ±‚èŒä¿¡ç”Ÿæˆå®Œæˆ")
                print(f"   é•¿åº¦: {len(cover_letter)} å­—ç¬¦")
                
                # ä¿å­˜æ±‚èŒä¿¡
                if args.output:
                    with open(args.output, 'w', encoding='utf-8') as f:
                        f.write(cover_letter)
                    print(f"   æ±‚èŒä¿¡å·²ä¿å­˜åˆ°: {args.output}")
                else:
                    print("\n" + "="*50)
                    print(cover_letter)
                    print("="*50)
            else:
                print("ğŸ” å¹²è¿è¡Œæ¨¡å¼ - è·³è¿‡å®é™…ç”Ÿæˆ")
        
        return True
        
    except Exception as e:
        print(f"âŒ ç®€å†ä¼˜åŒ–å¤±è´¥: {e}")
        return False

async def search_command(args):
    """èŒä½æœç´¢å‘½ä»¤"""
    print("ğŸ” èŒä½æœç´¢")
    print("=" * 20)
    
    try:
        config = load_config(args.config)
        coordinator = RAGSystemCoordinator(config)
        
        if not coordinator.initialize_system():
            print("âŒ ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥")
            return False
        
        # æ‰§è¡Œæœç´¢
        query = args.query
        k = args.limit or 10
        
        print(f"æœç´¢æŸ¥è¯¢: {query}")
        print(f"è¿”å›æ•°é‡: {k}")
        
        # ä½¿ç”¨å‘é‡æœç´¢
        similar_docs = coordinator.vector_manager.search_similar_jobs(query, k=k)
        
        if not similar_docs:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°ç›¸å…³èŒä½")
            return False
        
        print(f"\nğŸ“‹ æœç´¢ç»“æœ ({len(similar_docs)} ä¸ª):")
        
        for i, doc in enumerate(similar_docs, 1):
            metadata = doc.metadata
            print(f"\n{i}. {metadata.get('job_title', 'æœªçŸ¥èŒä½')}")
            print(f"   å…¬å¸: {metadata.get('company', 'æœªçŸ¥å…¬å¸')}")
            print(f"   åœ°ç‚¹: {metadata.get('location', 'æœªçŸ¥åœ°ç‚¹')}")
            print(f"   ç±»å‹: {metadata.get('type', 'æœªçŸ¥ç±»å‹')}")
            print(f"   å†…å®¹: {doc.page_content[:100]}...")
        
        # ä¿å­˜æœç´¢ç»“æœ
        if args.output:
            results = []
            for doc in similar_docs:
                results.append({
                    'content': doc.page_content,
                    'metadata': doc.metadata
                })
            
            with open(args.output, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2, default=str)
            print(f"\nğŸ’¾ æœç´¢ç»“æœå·²ä¿å­˜åˆ°: {args.output}")
        
        return True
        
    except Exception as e:
        print(f"âŒ æœç´¢å¤±è´¥: {e}")
        return False

async def test_command(args):
    """å‘é‡æ•°æ®åº“æµ‹è¯•å‘½ä»¤"""
    print("ğŸ§ª å‘é‡æ•°æ®åº“æµ‹è¯•")
    print("=" * 30)
    
    try:
        config = load_config(args.config)
        vector_config = config.get('rag_system', {}).get('vector_db', {})
        vector_manager = ChromaDBManager(vector_config)
        
        # è·å–ç»Ÿè®¡ä¿¡æ¯
        print("\nğŸ“Š æ•°æ®åº“ç»Ÿè®¡:")
        stats = vector_manager.get_collection_stats()
        print(f"   æ–‡æ¡£æ•°é‡: {stats.get('document_count', 0)}")
        print(f"   é›†åˆåç§°: {stats.get('collection_name', 'unknown')}")
        print(f"   å­˜å‚¨è·¯å¾„: {stats.get('persist_directory', 'unknown')}")
        
        if stats.get('document_count', 0) == 0:
            print("âš ï¸ å‘é‡æ•°æ®åº“ä¸ºç©º")
            vector_manager.close()
            return True
        
        # æ£€æŸ¥æ–‡æ¡£æ ·æœ¬
        print("\nğŸ“„ æ–‡æ¡£æ ·æœ¬:")
        collection = vector_manager.vectorstore._collection
        sample_data = collection.get(limit=args.sample_size or 3)
        
        if sample_data['ids']:
            for i, doc_id in enumerate(sample_data['ids']):
                content = sample_data['documents'][i]
                metadata = sample_data['metadatas'][i] if sample_data['metadatas'] else {}
                
                print(f"   æ–‡æ¡£ {i+1}:")
                print(f"     ID: {doc_id}")
                print(f"     é•¿åº¦: {len(content)} å­—ç¬¦")
                print(f"     é¢„è§ˆ: {content[:100]}...")
                print(f"     èŒä½ID: {metadata.get('job_id', 'æœªçŸ¥')}")
                print(f"     ç±»å‹: {metadata.get('document_type', 'æœªçŸ¥')}")
                print()
        
        # æµ‹è¯•æœç´¢åŠŸèƒ½
        if args.test_search:
            print("ğŸ” æœç´¢åŠŸèƒ½æµ‹è¯•:")
            test_queries = args.queries.split(',') if args.queries else ["Python", "å¼€å‘å·¥ç¨‹å¸ˆ", "å‰ç«¯"]
            
            for query in test_queries:
                results = vector_manager.search_similar_jobs(query.strip(), k=2)
                scored_results = vector_manager.similarity_search_with_score(query.strip(), k=2)
                
                print(f"   æŸ¥è¯¢ '{query.strip()}': {len(results)} ä¸ªç»“æœ")
                if scored_results:
                    top_score = scored_results[0][1]
                    print(f"     æœ€é«˜ç›¸ä¼¼åº¦: {top_score:.3f}")
        
        # æ£€æŸ¥å…ƒæ•°æ®å­—æ®µ
        print("\nğŸ·ï¸ å…ƒæ•°æ®å­—æ®µ:")
        if sample_data['metadatas']:
            all_fields = set()
            for metadata in sample_data['metadatas']:
                if metadata:
                    all_fields.update(metadata.keys())
            print(f"   å­—æ®µ: {list(all_fields)}")
        else:
            print("   âš ï¸ æ²¡æœ‰å…ƒæ•°æ®")
        
        # ä¿å­˜æµ‹è¯•æŠ¥å‘Š
        if args.output:
            test_report = {
                'timestamp': datetime.now().isoformat(),
                'stats': stats,
                'sample_documents': len(sample_data['ids']) if sample_data['ids'] else 0,
                'metadata_fields': list(all_fields) if sample_data['metadatas'] else []
            }
            
            with open(args.output, 'w', encoding='utf-8') as f:
                json.dump(test_report, f, ensure_ascii=False, indent=2, default=str)
            print(f"\nğŸ’¾ æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜åˆ°: {args.output}")
        
        vector_manager.close()
        print("\nâœ… å‘é‡æ•°æ®åº“æµ‹è¯•å®Œæˆ")
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        return False

async def clear_command(args):
    """æ¸…ç†å‘é‡æ•°æ®åº“å‘½ä»¤"""
    print("ğŸ—‘ï¸ æ¸…ç†å‘é‡æ•°æ®åº“")
    print("=" * 30)
    
    try:
        config = load_config(args.config)
        vector_config = config.get('rag_system', {}).get('vector_db', {})
        vector_manager = ChromaDBManager(vector_config)
        
        # è·å–å½“å‰ç»Ÿè®¡
        stats = vector_manager.get_collection_stats()
        doc_count = stats.get('document_count', 0)
        
        print(f"ğŸ“Š å½“å‰æ–‡æ¡£æ•°é‡: {doc_count}")
        
        if doc_count == 0:
            print("âš ï¸ å‘é‡æ•°æ®åº“å·²ç»æ˜¯ç©ºçš„")
            vector_manager.close()
            return True
        
        # ç¡®è®¤åˆ é™¤
        if not args.force:
            confirm = input(f"ç¡®å®šè¦æ¸…ç©ºæ‰€æœ‰ {doc_count} ä¸ªæ–‡æ¡£å—ï¼Ÿ(y/N): ")
            if confirm.lower() != 'y':
                print("æ“ä½œå·²å–æ¶ˆ")
                vector_manager.close()
                return True
        
        # æ‰§è¡Œæ¸…ç†
        if args.job_id:
            # åˆ é™¤ç‰¹å®šèŒä½çš„æ–‡æ¡£
            success = vector_manager.delete_documents(args.job_id)
            if success:
                print(f"âœ… æˆåŠŸåˆ é™¤èŒä½ {args.job_id} çš„æ–‡æ¡£")
            else:
                print(f"âŒ åˆ é™¤èŒä½ {args.job_id} çš„æ–‡æ¡£å¤±è´¥")
        else:
            # æ¸…ç©ºæ‰€æœ‰æ–‡æ¡£
            collection = vector_manager.vectorstore._collection
            all_data = collection.get()
            
            if all_data['ids']:
                collection.delete(ids=all_data['ids'])
                print(f"âœ… æˆåŠŸæ¸…ç©º {len(all_data['ids'])} ä¸ªæ–‡æ¡£")
            else:
                print("ğŸ“ å‘é‡æ•°æ®åº“å·²ç»æ˜¯ç©ºçš„")
        
        # éªŒè¯æ¸…ç†ç»“æœ
        new_stats = vector_manager.get_collection_stats()
        new_count = new_stats.get('document_count', 0)
        print(f"ğŸ“Š æ¸…ç†åæ–‡æ¡£æ•°é‡: {new_count}")
        
        vector_manager.close()
        return True
        
    except Exception as e:
        print(f"âŒ æ¸…ç†å¤±è´¥: {e}")
        return False

async def match_command(args):
    """ç®€å†èŒä½åŒ¹é…å‘½ä»¤"""
    print("ğŸ¯ ç®€å†èŒä½åŒ¹é…")
    print("=" * 30)
    
    try:
        # åŠ è½½é…ç½®
        config = load_config(args.config)
        
        # åŠ è½½ç®€å†åŒ¹é…é…ç½®
        resume_config_path = args.resume_config or 'config/resume_matching_config.yaml'
        resume_config = load_config(resume_config_path) if Path(resume_config_path).exists() else {}
        
        # åˆå¹¶é…ç½®
        if resume_config:
            config.update(resume_config)
        
        # åˆå§‹åŒ–ç³»ç»Ÿ
        coordinator = RAGSystemCoordinator(config)
        if not coordinator.initialize_system():
            print("âŒ ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥")
            return False
        
        # åŠ è½½ç®€å† - ç»Ÿä¸€ä½¿ç”¨é€šç”¨æ ¼å¼
        if args.resume:
            resume_data = load_resume(args.resume)
            resume_profile = load_resume_profile(resume_data)
            print(f"ğŸ“ åŠ è½½ç®€å†æ¡£æ¡ˆ: {resume_profile.name}")
        else:
            # ä½¿ç”¨é»˜è®¤ç®€å†æ¡£æ¡ˆ
            resume_profile = create_default_zhanbin_profile()
            print("ğŸ“ ä½¿ç”¨é»˜è®¤ç®€å†æ¡£æ¡ˆ")
        
        # ä½¿ç”¨é€šç”¨åŒ¹é…å¼•æ“
        matcher = GenericResumeJobMatcher(coordinator.vector_manager, config.get('resume_matching', {}))
        print("ğŸ”§ ä½¿ç”¨é€šç”¨åŒ¹é…å¼•æ“")
        
        print(f"ğŸ‘¤ ç®€å†æ¡£æ¡ˆ: {resume_profile.name}")
        print(f"ğŸ’¼ å½“å‰èŒä½: {resume_profile.current_position}")
        print(f"ğŸ“… å·¥ä½œç»éªŒ: {resume_profile.total_experience_years}å¹´")
        
        # æ ¹æ®å­å‘½ä»¤æ‰§è¡Œä¸åŒæ“ä½œ
        if args.action == 'find-jobs':
            # æŸ¥æ‰¾åŒ¹é…èŒä½
            print(f"\nğŸ” æŸ¥æ‰¾åŒ¹é…èŒä½ (è¿”å›{args.limit}ä¸ª)")
            
            # æ„å»ºè¿‡æ»¤æ¡ä»¶
            filters = {}
            if args.filters:
                try:
                    filters = json.loads(args.filters)
                except json.JSONDecodeError:
                    print("âš ï¸ è¿‡æ»¤æ¡ä»¶JSONæ ¼å¼é”™è¯¯ï¼Œå¿½ç•¥è¿‡æ»¤")
            
            if not args.dry_run:
                # æ‰§è¡ŒåŒ¹é…
                result = await matcher.find_matching_jobs(
                    resume_profile,
                    filters=filters,
                    top_k=args.limit
                )
                
                # æ˜¾ç¤ºåŒ¹é…æ‘˜è¦
                summary = result.matching_summary
                print(f"\nğŸ“Š åŒ¹é…ç»“æœæ‘˜è¦:")
                print(f"   æ€»åŒ¹é…æ•°: {summary.total_matches}")
                print(f"   é«˜ä¼˜å…ˆçº§: {summary.high_priority}")
                print(f"   ä¸­ä¼˜å…ˆçº§: {summary.medium_priority}")
                print(f"   ä½ä¼˜å…ˆçº§: {summary.low_priority}")
                print(f"   å¹³å‡åˆ†æ•°: {summary.average_score:.3f}")
                print(f"   å¤„ç†æ—¶é—´: {summary.processing_time:.2f}ç§’")
                
                # æ˜¾ç¤ºå‰å‡ ä¸ªåŒ¹é…ç»“æœ
                print(f"\nğŸ¯ å‰{min(5, len(result.matches))}ä¸ªåŒ¹é…èŒä½:")
                for i, match in enumerate(result.matches[:5], 1):
                    print(f"\n{i}. {match.job_title} - {match.company}")
                    print(f"   ç»¼åˆè¯„åˆ†: {match.overall_score:.3f} ({match.match_level.value})")
                    print(f"   æ¨èä¼˜å…ˆçº§: {match.recommendation_priority.value}")
                    print(f"   æŠ€èƒ½åŒ¹é…: {match.dimension_scores.get('skills_match', 0):.3f}")
                    print(f"   ç»éªŒåŒ¹é…: {match.dimension_scores.get('experience_match', 0):.3f}")
                    if match.location:
                        print(f"   åœ°ç‚¹: {match.location}")
                
                # ä¿å­˜ç»“æœ
                if args.output:
                    # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼
                    output_data = {
                        'matching_summary': {
                            'total_matches': summary.total_matches,
                            'high_priority': summary.high_priority,
                            'medium_priority': summary.medium_priority,
                            'low_priority': summary.low_priority,
                            'average_score': summary.average_score,
                            'processing_time': summary.processing_time,
                            'timestamp': summary.timestamp
                        },
                        'matches': [
                            {
                                'job_id': match.job_id,
                                'job_title': match.job_title,
                                'company': match.company,
                                'location': match.location,
                                'salary_range': match.salary_range,
                                'overall_score': match.overall_score,
                                'dimension_scores': match.dimension_scores,
                                'match_level': match.match_level.value,
                                'recommendation_priority': match.recommendation_priority.value,
                                'match_analysis': {
                                    'strengths': match.match_analysis.strengths,
                                    'weaknesses': match.match_analysis.weaknesses,
                                    'recommendations': match.match_analysis.recommendations,
                                    'matched_skills': match.match_analysis.matched_skills,
                                    'missing_skills': match.match_analysis.missing_skills,
                                    'skill_gap_score': match.match_analysis.skill_gap_score,
                                    'experience_alignment': match.match_analysis.experience_alignment,
                                    'industry_fit': match.match_analysis.industry_fit
                                },
                                'confidence_level': match.confidence_level,
                                'timestamp': match.timestamp
                            }
                            for match in result.matches
                        ],
                        'career_insights': {
                            'top_matching_positions': result.career_insights.top_matching_positions,
                            'skill_gap_analysis': result.career_insights.skill_gap_analysis,
                            'salary_analysis': result.career_insights.salary_analysis,
                            'market_trends': result.career_insights.market_trends,
                            'career_recommendations': result.career_insights.career_recommendations
                        },
                        'query_metadata': result.query_metadata
                    }
                    
                    with open(args.output, 'w', encoding='utf-8') as f:
                        json.dump(output_data, f, ensure_ascii=False, indent=2, default=str)
                    print(f"\nğŸ’¾ åŒ¹é…ç»“æœå·²ä¿å­˜åˆ°: {args.output}")
            else:
                print("ğŸ” å¹²è¿è¡Œæ¨¡å¼ - è·³è¿‡å®é™…åŒ¹é…")
        
        elif args.action == 'analyze-fit':
            # åˆ†æç‰¹å®šèŒä½åŒ¹é…åº¦
            if not args.job_id:
                print("âŒ éœ€è¦æŒ‡å®š --job-id å‚æ•°")
                return False
            
            print(f"\nğŸ” åˆ†æèŒä½åŒ¹é…åº¦: {args.job_id}")
            
            if not args.dry_run:
                # é€šç”¨åŒ¹é…å¼•æ“æš‚æ—¶ä¸æ”¯æŒå•ä¸ªèŒä½åˆ†æ
                print("âš ï¸ é€šç”¨åŒ¹é…å¼•æ“æš‚ä¸æ”¯æŒå•ä¸ªèŒä½åˆ†æï¼Œè¯·ä½¿ç”¨ find-jobs å‘½ä»¤")
                return False
                
                if not match_result:
                    print(f"âŒ æœªæ‰¾åˆ°èŒä½ {args.job_id} æˆ–åˆ†æå¤±è´¥")
                    return False
                
                # æ˜¾ç¤ºè¯¦ç»†åˆ†æç»“æœ
                print(f"\nğŸ“Š åŒ¹é…åˆ†æç»“æœ:")
                print(f"   èŒä½: {match_result.job_title} - {match_result.company}")
                print(f"   ç»¼åˆè¯„åˆ†: {match_result.overall_score:.3f} ({match_result.match_level.value})")
                print(f"   æ¨èä¼˜å…ˆçº§: {match_result.recommendation_priority.value}")
                print(f"   ç½®ä¿¡åº¦: {match_result.confidence_level:.3f}")
                
                print(f"\nğŸ“ˆ ç»´åº¦è¯„åˆ†:")
                for dimension, score in match_result.dimension_scores.items():
                    print(f"   {dimension}: {score:.3f}")
                
                print(f"\nğŸ’ª ä¼˜åŠ¿:")
                for strength in match_result.match_analysis.strengths:
                    print(f"   â€¢ {strength}")
                
                print(f"\nâš ï¸ åŠ£åŠ¿:")
                for weakness in match_result.match_analysis.weaknesses:
                    print(f"   â€¢ {weakness}")
                
                print(f"\nğŸ’¡ å»ºè®®:")
                for recommendation in match_result.match_analysis.recommendations:
                    print(f"   â€¢ {recommendation}")
                
                print(f"\nâœ… åŒ¹é…æŠ€èƒ½:")
                for skill in match_result.match_analysis.matched_skills[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
                    print(f"   â€¢ {skill}")
                
                print(f"\nâŒ ç¼ºå¤±æŠ€èƒ½:")
                for skill in match_result.match_analysis.missing_skills[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
                    print(f"   â€¢ {skill}")
                
                # ä¿å­˜ç»“æœ
                if args.output:
                    output_data = {
                        'job_id': match_result.job_id,
                        'job_title': match_result.job_title,
                        'company': match_result.company,
                        'overall_score': match_result.overall_score,
                        'match_level': match_result.match_level.value,
                        'dimension_scores': match_result.dimension_scores,
                        'match_analysis': {
                            'strengths': match_result.match_analysis.strengths,
                            'weaknesses': match_result.match_analysis.weaknesses,
                            'recommendations': match_result.match_analysis.recommendations,
                            'matched_skills': match_result.match_analysis.matched_skills,
                            'missing_skills': match_result.match_analysis.missing_skills
                        },
                        'timestamp': match_result.timestamp
                    }
                    
                    with open(args.output, 'w', encoding='utf-8') as f:
                        json.dump(output_data, f, ensure_ascii=False, indent=2, default=str)
                    print(f"\nğŸ’¾ åˆ†æç»“æœå·²ä¿å­˜åˆ°: {args.output}")
            else:
                print("ğŸ” å¹²è¿è¡Œæ¨¡å¼ - è·³è¿‡å®é™…åˆ†æ")
        
        elif args.action == 'batch-analyze':
            # æ‰¹é‡åˆ†æå¤šä¸ªèŒä½
            if not args.job_list:
                print("âŒ éœ€è¦æŒ‡å®š --job-list å‚æ•°")
                return False
            
            # è¯»å–èŒä½IDåˆ—è¡¨
            if not Path(args.job_list).exists():
                print(f"âŒ èŒä½åˆ—è¡¨æ–‡ä»¶ä¸å­˜åœ¨: {args.job_list}")
                return False
            
            with open(args.job_list, 'r', encoding='utf-8') as f:
                job_ids = [line.strip() for line in f if line.strip()]
            
            print(f"\nğŸ” æ‰¹é‡åˆ†æ {len(job_ids)} ä¸ªèŒä½")
            
            if not args.dry_run:
                print("âš ï¸ é€šç”¨åŒ¹é…å¼•æ“æš‚ä¸æ”¯æŒæ‰¹é‡åˆ†æï¼Œè¯·ä½¿ç”¨ find-jobs å‘½ä»¤")
                return False
                
                print(f"\nğŸ“Š æ‰¹é‡åˆ†æå®Œæˆï¼ŒæˆåŠŸåˆ†æ {len(results)} ä¸ªèŒä½")
                
                # æ˜¾ç¤ºå‰å‡ ä¸ªç»“æœ
                print(f"\nğŸ¯ å‰{min(5, len(results))}ä¸ªåŒ¹é…ç»“æœ:")
                for i, match in enumerate(results[:5], 1):
                    print(f"\n{i}. {match.job_title} - {match.company}")
                    print(f"   è¯„åˆ†: {match.overall_score:.3f} ({match.match_level.value})")
                    print(f"   ä¼˜å…ˆçº§: {match.recommendation_priority.value}")
                
                # ä¿å­˜ç»“æœ
                if args.output:
                    output_data = [
                        {
                            'job_id': match.job_id,
                            'job_title': match.job_title,
                            'company': match.company,
                            'overall_score': match.overall_score,
                            'match_level': match.match_level.value,
                            'recommendation_priority': match.recommendation_priority.value,
                            'dimension_scores': match.dimension_scores
                        }
                        for match in results
                    ]
                    
                    with open(args.output, 'w', encoding='utf-8') as f:
                        json.dump(output_data, f, ensure_ascii=False, indent=2, default=str)
                    print(f"\nğŸ’¾ æ‰¹é‡åˆ†æç»“æœå·²ä¿å­˜åˆ°: {args.output}")
            else:
                print("ğŸ” å¹²è¿è¡Œæ¨¡å¼ - è·³è¿‡å®é™…åˆ†æ")
        
        elif args.action == 'generate-report':
            # ç”ŸæˆåŒ¹é…æŠ¥å‘Š
            print(f"\nğŸ“„ ç”ŸæˆåŒ¹é…æŠ¥å‘Š")
            
            if not args.dry_run:
                # å…ˆæ‰§è¡ŒåŒ¹é…
                result = await matcher.find_matching_jobs(
                    resume_profile,
                    top_k=args.limit
                )
                
                # ç”ŸæˆHTMLæŠ¥å‘Š
                html_report = generate_html_report(result, resume_profile)
                
                output_file = args.output or f"resume_matching_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.html"
                
                with open(output_file, 'w', encoding='utf-8') as f:
                    f.write(html_report)
                
                print(f"âœ… åŒ¹é…æŠ¥å‘Šå·²ç”Ÿæˆ: {output_file}")
            else:
                print("ğŸ” å¹²è¿è¡Œæ¨¡å¼ - è·³è¿‡æŠ¥å‘Šç”Ÿæˆ")
        
        return True
        
    except Exception as e:
        print(f"âŒ åŒ¹é…å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

async def resume_command(args):
    """ç®€å†æ–‡æ¡£å¤„ç†å‘½ä»¤"""
    print("ğŸ“ ç®€å†æ–‡æ¡£å¤„ç†")
    print("=" * 30)
    
    try:
        config = load_config(args.config)
        
        if args.action == 'process':
            # å¤„ç†å•ä¸ªç®€å†æ–‡æ¡£
            print(f"ğŸ“„ å¤„ç†ç®€å†æ–‡æ¡£: {args.input}")
            
            if not Path(args.input).exists():
                print(f"âŒ ç®€å†æ–‡ä»¶ä¸å­˜åœ¨: {args.input}")
                return False
            
            # åˆ›å»ºæ–‡æ¡£è§£æå™¨
            parser_config = config.get('resume_processing', {}).get('document_parser', {})
            parser = ResumeDocumentParser(parser_config)
            
            # è§£ææ–‡æ¡£
            try:
                content = parser.extract_content(args.input)
                print(f"âœ… æ–‡æ¡£è§£æå®Œæˆï¼Œå†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
                
                if args.dry_run:
                    print("ğŸ” å¹²è¿è¡Œæ¨¡å¼ - è·³è¿‡RAGå¤„ç†")
                    if args.output:
                        with open(args.output, 'w', encoding='utf-8') as f:
                            f.write(content)
                        print(f"ğŸ“„ åŸå§‹å†…å®¹å·²ä¿å­˜åˆ°: {args.output}")
                    return True
                
                # åˆ›å»ºLLMå®¢æˆ·ç«¯
                llm_config = config.get('rag_system', {}).get('llm', {})
                llm_client = create_llm(
                    provider=llm_config.get('provider', 'zhipu'),
                    api_key=llm_config.get('api_key'),
                    model=llm_config.get('model', 'glm-4-flash'),
                    temperature=llm_config.get('temperature', 0.1),
                    max_tokens=llm_config.get('max_tokens', 2000)
                )
                
                # åˆ›å»ºRAGå¤„ç†å™¨
                processor_config = config.get('resume_processing', {}).get('rag_processor', {})
                processor = ResumeDocumentProcessor(llm_client, processor_config)
                
                # å¤„ç†ç®€å†
                user_hints = {}
                if hasattr(args, 'hints') and args.hints:
                    try:
                        user_hints = json.loads(args.hints)
                    except json.JSONDecodeError:
                        print("âš ï¸ ç”¨æˆ·æç¤ºJSONæ ¼å¼é”™è¯¯ï¼Œå¿½ç•¥")
                
                profile = await processor.process_resume_document(content, user_hints)
                
                print(f"âœ… ç®€å†å¤„ç†å®Œæˆ: {profile.name}")
                print(f"   å½“å‰èŒä½: {profile.current_position}")
                print(f"   å·¥ä½œç»éªŒ: {profile.total_experience_years}å¹´")
                print(f"   æŠ€èƒ½åˆ†ç±»: {len(profile.skill_categories)}ä¸ª")
                print(f"   å·¥ä½œç»å†: {len(profile.work_history)}æ®µ")
                
                # ä¿å­˜ç»“æœ
                if args.output:
                    output_data = profile.to_dict()
                    with open(args.output, 'w', encoding='utf-8') as f:
                        json.dump(output_data, f, ensure_ascii=False, indent=2, default=str)
                    print(f"ğŸ’¾ å¤„ç†ç»“æœå·²ä¿å­˜åˆ°: {args.output}")
                
                return True
                
            except Exception as e:
                print(f"âŒ æ–‡æ¡£å¤„ç†å¤±è´¥: {e}")
                return False
        
        elif args.action == 'batch-process':
            # æ‰¹é‡å¤„ç†ç®€å†æ–‡æ¡£
            print(f"ğŸ“ æ‰¹é‡å¤„ç†ç®€å†æ–‡æ¡£")
            
            if not Path(args.input_dir).exists():
                print(f"âŒ è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {args.input_dir}")
                return False
            
            # è·å–æ”¯æŒçš„æ ¼å¼
            formats = args.formats.split(',') if args.formats else ['md', 'docx', 'pdf', 'txt']
            print(f"ğŸ“‹ æ”¯æŒæ ¼å¼: {formats}")
            
            # æŸ¥æ‰¾æ–‡ä»¶
            input_dir = Path(args.input_dir)
            files_to_process = []
            
            for format_ext in formats:
                pattern = f"*.{format_ext}"
                files_to_process.extend(input_dir.glob(pattern))
            
            if not files_to_process:
                print(f"âŒ åœ¨ç›®å½• {args.input_dir} ä¸­æœªæ‰¾åˆ°æ”¯æŒçš„ç®€å†æ–‡ä»¶")
                return False
            
            print(f"ğŸ“Š æ‰¾åˆ° {len(files_to_process)} ä¸ªæ–‡ä»¶å¾…å¤„ç†")
            
            if args.dry_run:
                print("ğŸ” å¹²è¿è¡Œæ¨¡å¼ - ä»…æ˜¾ç¤ºæ–‡ä»¶åˆ—è¡¨")
                for i, file_path in enumerate(files_to_process, 1):
                    print(f"  {i}. {file_path.name}")
                return True
            
            # åˆ›å»ºè¾“å‡ºç›®å½•
            output_dir = Path(args.output_dir) if args.output_dir else Path('./processed_resumes')
            output_dir.mkdir(parents=True, exist_ok=True)
            
            # åˆ›å»ºè§£æå™¨å’Œå¤„ç†å™¨
            parser_config = config.get('resume_processing', {}).get('document_parser', {})
            parser = ResumeDocumentParser(parser_config)
            
            llm_config = config.get('rag_system', {}).get('llm', {})
            llm_client = create_llm(
                provider=llm_config.get('provider', 'zhipu'),
                api_key=llm_config.get('api_key'),
                model=llm_config.get('model', 'glm-4-flash'),
                temperature=llm_config.get('temperature', 0.1),
                max_tokens=llm_config.get('max_tokens', 2000)
            )
            
            processor_config = config.get('resume_processing', {}).get('rag_processor', {})
            processor = ResumeDocumentProcessor(llm_client, processor_config)
            
            # æ‰¹é‡å¤„ç†
            successful = 0
            failed = 0
            
            for i, file_path in enumerate(files_to_process, 1):
                try:
                    print(f"\nğŸ“„ å¤„ç†æ–‡ä»¶ {i}/{len(files_to_process)}: {file_path.name}")
                    
                    # è§£ææ–‡æ¡£
                    content = parser.extract_content(str(file_path))
                    
                    # RAGå¤„ç†
                    profile = await processor.process_resume_document(content)
                    
                    # ä¿å­˜ç»“æœ
                    output_file = output_dir / f"{file_path.stem}_processed.json"
                    output_data = profile.to_dict()
                    
                    with open(output_file, 'w', encoding='utf-8') as f:
                        json.dump(output_data, f, ensure_ascii=False, indent=2, default=str)
                    
                    print(f"   âœ… æˆåŠŸ: {profile.name}")
                    successful += 1
                    
                except Exception as e:
                    print(f"   âŒ å¤±è´¥: {e}")
                    failed += 1
                
                # å¹¶å‘æ§åˆ¶
                if args.parallel and i % args.parallel == 0:
                    await asyncio.sleep(1)  # ç®€å•çš„é€Ÿç‡é™åˆ¶
            
            print(f"\nğŸ“Š æ‰¹é‡å¤„ç†å®Œæˆ:")
            print(f"   æˆåŠŸ: {successful}")
            print(f"   å¤±è´¥: {failed}")
            print(f"   æ€»è®¡: {len(files_to_process)}")
            print(f"   è¾“å‡ºç›®å½•: {output_dir}")
            
            return successful > 0
        
        elif args.action == 'validate':
            # éªŒè¯ç®€å†JSONæ ¼å¼
            print(f"ğŸ” éªŒè¯ç®€å†JSON: {args.input}")
            
            if not Path(args.input).exists():
                print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {args.input}")
                return False
            
            try:
                with open(args.input, 'r', encoding='utf-8') as f:
                    resume_data = json.load(f)
                
                # åŸºæœ¬æ ¼å¼æ£€æŸ¥
                if args.schema_check:
                    print("ğŸ“‹ æ‰§è¡Œæ¨¡å¼éªŒè¯...")
                    
                    # æ£€æŸ¥å¿…éœ€å­—æ®µ
                    required_fields = ['name', 'skill_categories', 'work_history']
                    missing_fields = []
                    
                    for field in required_fields:
                        if field not in resume_data:
                            missing_fields.append(field)
                    
                    if missing_fields:
                        print(f"âŒ ç¼ºå°‘å¿…éœ€å­—æ®µ: {missing_fields}")
                        return False
                    
                    print("âœ… æ¨¡å¼éªŒè¯é€šè¿‡")
                
                # å®Œæ•´æ€§æ£€æŸ¥
                if args.completeness_check:
                    print("ğŸ“Š æ‰§è¡Œå®Œæ•´æ€§æ£€æŸ¥...")
                    
                    issues = []
                    
                    # æ£€æŸ¥åŸºæœ¬ä¿¡æ¯å®Œæ•´æ€§
                    if not resume_data.get('name'):
                        issues.append("å§“åä¸ºç©º")
                    if not resume_data.get('email'):
                        issues.append("é‚®ç®±ä¸ºç©º")
                    
                    # æ£€æŸ¥æŠ€èƒ½åˆ†ç±»
                    skill_categories = resume_data.get('skill_categories', [])
                    if not skill_categories:
                        issues.append("æ²¡æœ‰æŠ€èƒ½åˆ†ç±»")
                    else:
                        for i, category in enumerate(skill_categories):
                            if not category.get('skills'):
                                issues.append(f"æŠ€èƒ½åˆ†ç±» {i+1} æ²¡æœ‰æŠ€èƒ½åˆ—è¡¨")
                    
                    # æ£€æŸ¥å·¥ä½œç»å†
                    work_history = resume_data.get('work_history', [])
                    if not work_history:
                        issues.append("æ²¡æœ‰å·¥ä½œç»å†")
                    else:
                        for i, work in enumerate(work_history):
                            if not work.get('company'):
                                issues.append(f"å·¥ä½œç»å† {i+1} ç¼ºå°‘å…¬å¸åç§°")
                            if not work.get('position'):
                                issues.append(f"å·¥ä½œç»å† {i+1} ç¼ºå°‘èŒä½åç§°")
                    
                    if issues:
                        print("âš ï¸ å‘ç°å®Œæ•´æ€§é—®é¢˜:")
                        for issue in issues:
                            print(f"   â€¢ {issue}")
                    else:
                        print("âœ… å®Œæ•´æ€§æ£€æŸ¥é€šè¿‡")
                
                # å°è¯•åˆ›å»ºGenericResumeProfile
                try:
                    profile = GenericResumeProfile.from_dict(resume_data)
                    print(f"âœ… æˆåŠŸåˆ›å»ºç®€å†æ¡£æ¡ˆ: {profile.name}")
                    print(f"   æŠ€èƒ½åˆ†ç±»: {len(profile.skill_categories)}ä¸ª")
                    print(f"   å·¥ä½œç»å†: {len(profile.work_history)}æ®µ")
                    print(f"   æ•™è‚²èƒŒæ™¯: {len(profile.education)}æ¡")
                    print(f"   é¡¹ç›®ç»éªŒ: {len(profile.projects)}ä¸ª")
                except Exception as e:
                    print(f"âŒ åˆ›å»ºç®€å†æ¡£æ¡ˆå¤±è´¥: {e}")
                    return False
                
                return True
                
            except json.JSONDecodeError as e:
                print(f"âŒ JSONæ ¼å¼é”™è¯¯: {e}")
                return False
            except Exception as e:
                print(f"âŒ éªŒè¯å¤±è´¥: {e}")
                return False
        
        elif args.action == 'match':
            # å®Œæ•´æµç¨‹ï¼šæ–‡æ¡£å¤„ç† + èŒä½åŒ¹é…
            print(f"ğŸ¯ å®Œæ•´æµç¨‹: æ–‡æ¡£å¤„ç† + èŒä½åŒ¹é…")
            
            if not Path(args.input).exists():
                print(f"âŒ ç®€å†æ–‡ä»¶ä¸å­˜åœ¨: {args.input}")
                return False
            
            # æ­¥éª¤1: å¤„ç†ç®€å†æ–‡æ¡£
            print("ğŸ“„ æ­¥éª¤1: å¤„ç†ç®€å†æ–‡æ¡£")
            
            parser_config = config.get('resume_processing', {}).get('document_parser', {})
            parser = ResumeDocumentParser(parser_config)
            
            content = parser.extract_content(args.input)
            print(f"âœ… æ–‡æ¡£è§£æå®Œæˆ")
            
            if not args.dry_run:
                # åˆ›å»ºLLMå®¢æˆ·ç«¯å’Œå¤„ç†å™¨
                llm_config = config.get('rag_system', {}).get('llm', {})
                llm_client = create_llm(
                    provider=llm_config.get('provider', 'zhipu'),
                    api_key=llm_config.get('api_key'),
                    model=llm_config.get('model', 'glm-4-flash'),
                    temperature=llm_config.get('temperature', 0.1),
                    max_tokens=llm_config.get('max_tokens', 2000)
                )
                
                processor_config = config.get('resume_processing', {}).get('rag_processor', {})
                processor = ResumeDocumentProcessor(llm_client, processor_config)
                
                profile = await processor.process_resume_document(content)
                print(f"âœ… ç®€å†å¤„ç†å®Œæˆ: {profile.name}")
                
                # æ­¥éª¤2: èŒä½åŒ¹é…
                print("\nğŸ¯ æ­¥éª¤2: èŒä½åŒ¹é…")
                
                # åˆå§‹åŒ–åŒ¹é…ç³»ç»Ÿ
                coordinator = RAGSystemCoordinator(config)
                if not coordinator.initialize_system():
                    print("âŒ åŒ¹é…ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥")
                    return False
                
                matcher = GenericResumeJobMatcher(coordinator.vector_manager, config.get('resume_matching', {}))
                
                # æ‰§è¡ŒåŒ¹é…
                result = await matcher.find_matching_jobs(profile, top_k=args.limit)
                
                print(f"âœ… åŒ¹é…å®Œæˆï¼Œæ‰¾åˆ° {result.matching_summary.total_matches} ä¸ªèŒä½")
                print(f"   é«˜ä¼˜å…ˆçº§: {result.matching_summary.high_priority}")
                print(f"   ä¸­ä¼˜å…ˆçº§: {result.matching_summary.medium_priority}")
                print(f"   ä½ä¼˜å…ˆçº§: {result.matching_summary.low_priority}")
                
                # ä¿å­˜ç»“æœ
                if args.output:
                    output_data = {
                        'resume_profile': profile.to_dict(),
                        'matching_results': {
                            'matching_summary': {
                                'total_matches': result.matching_summary.total_matches,
                                'high_priority': result.matching_summary.high_priority,
                                'medium_priority': result.matching_summary.medium_priority,
                                'low_priority': result.matching_summary.low_priority,
                                'average_score': result.matching_summary.average_score,
                                'processing_time': result.matching_summary.processing_time
                            },
                            'matches': [
                                {
                                    'job_id': match.job_id,
                                    'job_title': match.job_title,
                                    'company': match.company,
                                    'overall_score': match.overall_score,
                                    'match_level': match.match_level.value,
                                    'recommendation_priority': match.recommendation_priority.value
                                }
                                for match in result.matches
                            ]
                        }
                    }
                    
                    with open(args.output, 'w', encoding='utf-8') as f:
                        json.dump(output_data, f, ensure_ascii=False, indent=2, default=str)
                    print(f"ğŸ’¾ å®Œæ•´ç»“æœå·²ä¿å­˜åˆ°: {args.output}")
                
                return True
            else:
                print("ğŸ” å¹²è¿è¡Œæ¨¡å¼ - è·³è¿‡RAGå¤„ç†å’ŒåŒ¹é…")
                return True
        
        else:
            print(f"âŒ æœªçŸ¥æ“ä½œ: {args.action}")
            return False
            
    except Exception as e:
        print(f"âŒ ç®€å†å¤„ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def generate_html_report(result, resume_profile):
    """ç”ŸæˆHTMLæ ¼å¼çš„åŒ¹é…æŠ¥å‘Š"""
    html_template = """
    <!DOCTYPE html>
    <html lang="zh-CN">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>ç®€å†èŒä½åŒ¹é…æŠ¥å‘Š - {name}</title>
        <style>
            body {{ font-family: 'Microsoft YaHei', Arial, sans-serif; margin: 20px; background-color: #f5f5f5; }}
            .container {{ max-width: 1200px; margin: 0 auto; background: white; padding: 30px; border-radius: 10px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }}
            .header {{ text-align: center; margin-bottom: 30px; padding-bottom: 20px; border-bottom: 2px solid #007bff; }}
            .summary {{ background: #f8f9fa; padding: 20px; border-radius: 8px; margin-bottom: 30px; }}
            .match-item {{ border: 1px solid #ddd; margin-bottom: 20px; padding: 20px; border-radius: 8px; }}
            .match-excellent {{ border-left: 5px solid #28a745; }}
            .match-good {{ border-left: 5px solid #17a2b8; }}
            .match-fair {{ border-left: 5px solid #ffc107; }}
            .match-poor {{ border-left: 5px solid #dc3545; }}
            .score {{ font-size: 24px; font-weight: bold; color: #007bff; }}
            .skills {{ display: flex; flex-wrap: wrap; gap: 5px; margin-top: 10px; }}
            .skill-tag {{ background: #007bff; color: white; padding: 3px 8px; border-radius: 12px; font-size: 12px; }}
            .missing-skill {{ background: #dc3545; }}
            .insights {{ background: #e9ecef; padding: 20px; border-radius: 8px; margin-top: 30px; }}
            .chart {{ margin: 20px 0; }}
        </style>
    </head>
    <body>
        <div class="container">
            <div class="header">
                <h1>ç®€å†èŒä½åŒ¹é…æŠ¥å‘Š</h1>
                <h2>{name} - {position}</h2>
                <p>ç”Ÿæˆæ—¶é—´: {timestamp}</p>
            </div>
            
            <div class="summary">
                <h3>ğŸ“Š åŒ¹é…æ‘˜è¦</h3>
                <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 20px;">
                    <div><strong>æ€»åŒ¹é…æ•°:</strong> {total_matches}</div>
                    <div><strong>é«˜ä¼˜å…ˆçº§:</strong> {high_priority}</div>
                    <div><strong>ä¸­ä¼˜å…ˆçº§:</strong> {medium_priority}</div>
                    <div><strong>ä½ä¼˜å…ˆçº§:</strong> {low_priority}</div>
                    <div><strong>å¹³å‡åˆ†æ•°:</strong> {average_score:.3f}</div>
                    <div><strong>å¤„ç†æ—¶é—´:</strong> {processing_time:.2f}ç§’</div>
                </div>
            </div>
            
            <h3>ğŸ¯ åŒ¹é…èŒä½è¯¦æƒ…</h3>
            {matches_html}
            
            <div class="insights">
                <h3>ğŸ’¡ èŒä¸šæ´å¯Ÿ</h3>
                <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 20px;">
                    <div>
                        <h4>çƒ­é—¨èŒä½</h4>
                        <ul>{top_positions}</ul>
                    </div>
                    <div>
                        <h4>æŠ€èƒ½å»ºè®®</h4>
                        <ul>{skill_recommendations}</ul>
                    </div>
                    <div>
                        <h4>èŒä¸šå»ºè®®</h4>
                        <ul>{career_recommendations}</ul>
                    </div>
                </div>
            </div>
        </div>
    </body>
    </html>
    """
    
    # ç”ŸæˆåŒ¹é…èŒä½HTML
    matches_html = ""
    for i, match in enumerate(result.matches, 1):
        match_class = f"match-{match.match_level.value}"
        
        matched_skills_html = "".join([f'<span class="skill-tag">{skill}</span>' for skill in match.match_analysis.matched_skills[:10]])
        missing_skills_html = "".join([f'<span class="skill-tag missing-skill">{skill}</span>' for skill in match.match_analysis.missing_skills[:5]])
        
        matches_html += f"""
        <div class="match-item {match_class}">
            <div style="display: flex; justify-content: space-between; align-items: center;">
                <div>
                    <h4>{match.job_title} - {match.company}</h4>
                    <p>{match.location or 'åœ°ç‚¹æœªçŸ¥'}</p>
                </div>
                <div class="score">{match.overall_score:.3f}</div>
            </div>
            
            <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(150px, 1fr)); gap: 10px; margin: 15px 0;">
                <div><strong>æŠ€èƒ½åŒ¹é…:</strong> {match.dimension_scores.get('skills_match', 0):.3f}</div>
                <div><strong>ç»éªŒåŒ¹é…:</strong> {match.dimension_scores.get('experience_match', 0):.3f}</div>
                <div><strong>è¡Œä¸šåŒ¹é…:</strong> {match.dimension_scores.get('industry_match', 0):.3f}</div>
                <div><strong>è–ªèµ„åŒ¹é…:</strong> {match.dimension_scores.get('salary_match', 0):.3f}</div>
            </div>
            
            <div>
                <strong>åŒ¹é…æŠ€èƒ½:</strong>
                <div class="skills">{matched_skills_html}</div>
            </div>
            
            <div style="margin-top: 10px;">
                <strong>ç¼ºå¤±æŠ€èƒ½:</strong>
                <div class="skills">{missing_skills_html}</div>
            </div>
            
            <div style="margin-top: 15px;">
                <strong>å»ºè®®:</strong>
                <ul>{"".join([f"<li>{rec}</li>" for rec in match.match_analysis.recommendations])}</ul>
            </div>
        </div>
        """
    
    # ç”Ÿæˆæ´å¯ŸHTML
    top_positions = "".join([f"<li>{pos}</li>" for pos in result.career_insights.top_matching_positions])
    skill_recommendations = "".join([f"<li>{skill}</li>" for skill in result.career_insights.skill_gap_analysis.get('high_demand_missing', [])])
    career_recommendations = "".join([f"<li>{rec}</li>" for rec in result.career_insights.career_recommendations])
    
    return html_template.format(
        name=resume_profile.name,
        position=resume_profile.current_position,
        timestamp=datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        total_matches=result.matching_summary.total_matches,
        high_priority=result.matching_summary.high_priority,
        medium_priority=result.matching_summary.medium_priority,
        low_priority=result.matching_summary.low_priority,
        average_score=result.matching_summary.average_score,
        processing_time=result.matching_summary.processing_time,
        matches_html=matches_html,
        top_positions=top_positions,
        skill_recommendations=skill_recommendations,
        career_recommendations=career_recommendations
    )

async def chat_command(args):
    """æ™ºèƒ½åˆ†æèŠå¤©å‘½ä»¤"""
    print("ğŸ¤– å°±ä¸šå¸‚åœºåˆ†æåŠ©æ‰‹")
    print("=" * 40)
    
    try:
        # åŠ è½½é…ç½®
        config = load_config(args.config)
        
        # åŠ è½½Agenté…ç½®
        agent_config_path = args.agent_config or 'config/agent_config.yaml'
        if Path(agent_config_path).exists():
            agent_config = load_config(agent_config_path)
            # åˆå¹¶é…ç½®
            config.update(agent_config)
        else:
            print(f"âš ï¸ Agenté…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {agent_config_path}")
            print("ä½¿ç”¨é»˜è®¤é…ç½®...")
        
        # åˆå§‹åŒ–RAGç³»ç»Ÿ
        coordinator = RAGSystemCoordinator(config)
        if not coordinator.initialize_system():
            print("âŒ RAGç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥")
            return False
        
        # åˆ›å»ºåˆ†æAgent
        try:
            agent = create_analysis_agent(coordinator, config)
            print("âœ… æ™ºèƒ½åˆ†æAgentåˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            print(f"âŒ Agentåˆå§‹åŒ–å¤±è´¥: {e}")
            return False
        
        # æ˜¾ç¤ºAgentçŠ¶æ€
        status = agent.get_agent_status()
        print(f"\nğŸ“Š AgentçŠ¶æ€:")
        print(f"   å¯ç”¨å·¥å…·: {len(status['tools_available'])}")
        print(f"   å·¥å…·åˆ—è¡¨: {', '.join(status['tools_available'])}")
        print(f"   LLMæä¾›å•†: {status['llm_provider']}")
        
        # æ˜¾ç¤ºæ¬¢è¿ä¿¡æ¯
        welcome_msg = config.get('langchain_agent', {}).get('user_experience', {}).get('interaction', {}).get('welcome_message')
        if welcome_msg:
            print(f"\nğŸ’¬ {welcome_msg}")
        
        # æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯
        if args.show_help:
            help_msg = config.get('langchain_agent', {}).get('user_experience', {}).get('interaction', {}).get('help_message')
            if help_msg:
                print(f"\nâ“ ä½¿ç”¨å¸®åŠ©:\n{help_msg}")
            
            # æ˜¾ç¤ºå»ºè®®é—®é¢˜
            suggested_questions = config.get('langchain_agent', {}).get('user_experience', {}).get('suggested_questions', [])
            if suggested_questions:
                print(f"\nğŸ’¡ å»ºè®®é—®é¢˜:")
                for i, question in enumerate(suggested_questions[:5], 1):
                    print(f"   {i}. {question}")
        
        print(f"\n{'='*40}")
        print("ğŸ’¡ è¾“å…¥ 'help' æŸ¥çœ‹å¸®åŠ©ï¼Œ'quit' æˆ– 'exit' é€€å‡º")
        print("ğŸ’¡ è¾“å…¥ 'clear' æ¸…é™¤å¯¹è¯å†å²ï¼Œ'status' æŸ¥çœ‹AgentçŠ¶æ€")
        print("ğŸ’¡ è¾“å…¥ 'stats' æŸ¥çœ‹åˆ†æç»Ÿè®¡ä¿¡æ¯")
        print("ğŸ’¡ æŒ‰ Ctrl+C å¯ä»¥éšæ—¶é€€å‡ºèŠå¤©")
        print("="*40)
        
        # äº¤äº’å¾ªç¯
        conversation_count = 0
        
        while True:
            try:
                # è·å–ç”¨æˆ·è¾“å…¥
                user_input = input("\nğŸ¤” æ‚¨çš„é—®é¢˜: ").strip()
                
                if not user_input:
                    continue
                
                # å¤„ç†ç‰¹æ®Šå‘½ä»¤
                if user_input.lower() in ['quit', 'exit', 'q']:
                    print("ğŸ‘‹ å†è§ï¼")
                    break
                
                elif user_input.lower() == 'help':
                    help_msg = config.get('langchain_agent', {}).get('user_experience', {}).get('interaction', {}).get('help_message')
                    if help_msg:
                        print(f"\nâ“ ä½¿ç”¨å¸®åŠ©:\n{help_msg}")
                    
                    suggested_questions = config.get('langchain_agent', {}).get('user_experience', {}).get('suggested_questions', [])
                    if suggested_questions:
                        print(f"\nğŸ’¡ å»ºè®®é—®é¢˜:")
                        for i, question in enumerate(suggested_questions, 1):
                            print(f"   {i}. {question}")
                    continue
                
                elif user_input.lower() == 'clear':
                    agent.clear_memory()
                    conversation_count = 0
                    print("ğŸ§¹ å¯¹è¯å†å²å·²æ¸…é™¤")
                    continue
                
                elif user_input.lower() == 'status':
                    status = agent.get_agent_status()
                    print(f"\nğŸ“Š AgentçŠ¶æ€:")
                    print(f"   å¯ç”¨å·¥å…·: {len(status['tools_available'])}")
                    print(f"   å¯¹è¯æ¶ˆæ¯æ•°: {status['memory_messages_count']}")
                    print(f"   å›è°ƒæ­¥éª¤æ•°: {status['callback_steps']}")
                    print(f"   æœ€ååˆ†ææ—¶é—´: {status.get('last_analysis_time', 'æ— ')}")
                    continue
                
                elif user_input.lower() == 'stats':
                    stats = agent.get_analysis_statistics()
                    print(f"\nğŸ“ˆ åˆ†æç»Ÿè®¡:")
                    print(f"   æ€»åˆ†ææ¬¡æ•°: {stats.get('total_analyses', 0)}")
                    print(f"   æˆåŠŸåˆ†ææ¬¡æ•°: {stats.get('successful_analyses', 0)}")
                    print(f"   æˆåŠŸç‡: {stats.get('success_rate', 0):.1f}%")
                    print(f"   å¹³å‡å¤„ç†æ—¶é—´: {stats.get('average_processing_time', 0):.2f}ç§’")
                    print(f"   å¯¹è¯é•¿åº¦: {stats.get('conversation_length', 0)}")
                    
                    tool_usage = stats.get('tool_usage', {})
                    if tool_usage:
                        print(f"   å·¥å…·ä½¿ç”¨ç»Ÿè®¡:")
                        for tool, count in tool_usage.items():
                            print(f"     {tool}: {count}æ¬¡")
                    continue
                
                # å¤„ç†åˆ†æé—®é¢˜
                print(f"\nğŸ” æ­£åœ¨åˆ†ææ‚¨çš„é—®é¢˜...")
                
                # æ‰§è¡Œåˆ†æ
                result = agent.run(user_input)
                
                if result['success']:
                    print(f"\nğŸ¤– åˆ†æç»“æœ:")
                    print(f"{result['response']}")
                    
                    # æ˜¾ç¤ºå¤„ç†ä¿¡æ¯
                    if args.verbose:
                        print(f"\nğŸ“Š å¤„ç†ä¿¡æ¯:")
                        print(f"   å¤„ç†æ—¶é—´: {result['processing_time']:.2f}ç§’")
                        print(f"   ä½¿ç”¨å·¥å…·: {', '.join(result['tools_used']) if result['tools_used'] else 'æ— '}")
                        
                        if result.get('analysis_steps'):
                            print(f"   åˆ†ææ­¥éª¤: {len(result['analysis_steps'])}æ­¥")
                    
                    conversation_count += 1
                    
                    # ä¿å­˜å¯¹è¯è®°å½•
                    if args.save_conversations:
                        save_conversation(user_input, result, conversation_count, args.conversation_dir)
                    
                else:
                    print(f"\nâŒ åˆ†æå¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
                
                # æ€§èƒ½ä¼˜åŒ–
                if conversation_count % 10 == 0:
                    print("ğŸ”§ æ­£åœ¨ä¼˜åŒ–æ€§èƒ½...")
                    agent.optimize_performance()
                
            except KeyboardInterrupt:
                print("\n\nğŸ’¡ æ£€æµ‹åˆ° Ctrl+Cï¼Œæ­£åœ¨é€€å‡º...")
                print("ğŸ‘‹ å†è§ï¼")
                break
            except EOFError:
                print("\nğŸ‘‹ å†è§ï¼")
                break
            except Exception as e:
                print(f"\nâŒ å¤„ç†é”™è¯¯: {e}")
                if args.debug:
                    import traceback
                    traceback.print_exc()
                continue
        
        # æ˜¾ç¤ºä¼šè¯ç»Ÿè®¡
        final_stats = agent.get_analysis_statistics()
        print(f"\nğŸ“Š æœ¬æ¬¡ä¼šè¯ç»Ÿè®¡:")
        print(f"   æ€»é—®é¢˜æ•°: {conversation_count}")
        print(f"   æˆåŠŸåˆ†æ: {final_stats.get('successful_analyses', 0)}")
        print(f"   å¹³å‡å¤„ç†æ—¶é—´: {final_stats.get('average_processing_time', 0):.2f}ç§’")
        
        return True
        
    except Exception as e:
        print(f"âŒ èŠå¤©ç³»ç»Ÿå¯åŠ¨å¤±è´¥: {e}")
        if args.debug:
            import traceback
            traceback.print_exc()
        return False

def save_conversation(question: str, result: dict, count: int, conversation_dir: str = None):
    """ä¿å­˜å¯¹è¯è®°å½•"""
    try:
        if not conversation_dir:
            conversation_dir = "logs/conversations"
        
        Path(conversation_dir).mkdir(parents=True, exist_ok=True)
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f"conversation_{timestamp}_{count:03d}.json"
        filepath = Path(conversation_dir) / filename
        
        conversation_data = {
            'timestamp': result.get('timestamp'),
            'question': question,
            'response': result.get('response'),
            'processing_time': result.get('processing_time'),
            'tools_used': result.get('tools_used', []),
            'success': result.get('success', False),
            'analysis_steps': result.get('analysis_steps', [])
        }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(conversation_data, f, ensure_ascii=False, indent=2, default=str)
            
    except Exception as e:
        print(f"âš ï¸ ä¿å­˜å¯¹è¯è®°å½•å¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='RAGæ™ºèƒ½ç®€å†æŠ•é€’ç³»ç»ŸCLI',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  # è¿è¡Œæ•°æ®æµæ°´çº¿
  python rag_cli.py pipeline run --batch-size 20 --show-progress
  
  # æŸ¥çœ‹ç³»ç»ŸçŠ¶æ€
  python rag_cli.py status
  
  # åˆ†æç®€å†å·®è·
  python rag_cli.py optimize analyze --resume resume.json --output analysis.json
  
  # ä¼˜åŒ–ç®€å†å†…å®¹
  python rag_cli.py optimize optimize --resume resume.json --target-job job123
  
  # ç”Ÿæˆæ±‚èŒä¿¡
  python rag_cli.py optimize cover-letter --resume resume.json --target-job job123
  
  # æœç´¢ç›¸å…³èŒä½
  python rag_cli.py search "Pythonå¼€å‘å·¥ç¨‹å¸ˆ" --limit 5
  
  # ç®€å†èŒä½åŒ¹é…
  python rag_cli.py match find-jobs --resume data/zhanbin_resume.json --limit 20 --output matches.json
  
  # åˆ†æç‰¹å®šèŒä½åŒ¹é…åº¦
  python rag_cli.py match analyze-fit --resume data/zhanbin_resume.json --job-id job123 --output analysis.json
  
  # æ‰¹é‡åˆ†æå¤šä¸ªèŒä½
  python rag_cli.py match batch-analyze --resume data/zhanbin_resume.json --job-list jobs.txt --output batch_results.json
  
  # ç”ŸæˆHTMLåŒ¹é…æŠ¥å‘Š
  python rag_cli.py match generate-report --resume data/zhanbin_resume.json --output report.html
  
  # æµ‹è¯•å‘é‡æ•°æ®åº“
  python rag_cli.py test --test-search --queries "Python,Java,å‰ç«¯"
  
  # æ¸…ç†å‘é‡æ•°æ®åº“
  python rag_cli.py clear --force
  
  # åˆ é™¤ç‰¹å®šèŒä½æ–‡æ¡£
  python rag_cli.py clear --job-id job123
  
  # å¯åŠ¨æ™ºèƒ½åˆ†æèŠå¤©
  python rag_cli.py chat --show-help --verbose
  
  # å¯åŠ¨èŠå¤©å¹¶ä¿å­˜å¯¹è¯è®°å½•
  python rag_cli.py chat --save-conversations --conversation-dir logs/chat
        """
    )
    
    # å…¨å±€å‚æ•°
    parser.add_argument('--config', '-c', help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--log-level', default='INFO', 
                       choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'], 
                       help='æ—¥å¿—çº§åˆ«')
    parser.add_argument('--log-file', help='æ—¥å¿—æ–‡ä»¶è·¯å¾„')
    
    # å­å‘½ä»¤
    subparsers = parser.add_subparsers(dest='command', help='å¯ç”¨å‘½ä»¤')
    
    # æµæ°´çº¿å‘½ä»¤
    pipeline_parser = subparsers.add_parser('pipeline', help='æ•°æ®æµæ°´çº¿ç®¡ç†')
    pipeline_parser.add_argument('action', choices=['run', 'resume'], help='æµæ°´çº¿æ“ä½œ')
    pipeline_parser.add_argument('--batch-size', '-b', type=int, help='æ‰¹å¤„ç†å¤§å°')
    pipeline_parser.add_argument('--max-jobs', '-m', type=int, help='æœ€å¤§å¤„ç†èŒä½æ•°é‡')
    pipeline_parser.add_argument('--force-reprocess', '-f', action='store_true', help='å¼ºåˆ¶é‡æ–°å¤„ç†')
    pipeline_parser.add_argument('--no-save', action='store_true', help='ä¸ä¿å­˜å¤„ç†ç»“æœ')
    pipeline_parser.add_argument('--show-progress', '-p', action='store_true', help='æ˜¾ç¤ºå¤„ç†è¿›åº¦')
    
    # çŠ¶æ€å‘½ä»¤
    status_parser = subparsers.add_parser('status', help='æŸ¥è¯¢ç³»ç»ŸçŠ¶æ€')
    
    # ç®€å†ä¼˜åŒ–å‘½ä»¤
    optimize_parser = subparsers.add_parser('optimize', help='ç®€å†ä¼˜åŒ–')
    optimize_parser.add_argument('action', choices=['analyze', 'optimize', 'cover-letter'], help='ä¼˜åŒ–æ“ä½œ')
    optimize_parser.add_argument('--resume', '-r', required=True, help='ç®€å†æ–‡ä»¶è·¯å¾„')
    optimize_parser.add_argument('--target-jobs', help='ç›®æ ‡èŒä½IDåˆ—è¡¨ï¼ˆé€—å·åˆ†éš”ï¼‰')
    optimize_parser.add_argument('--target-job', help='å•ä¸ªç›®æ ‡èŒä½ID')
    optimize_parser.add_argument('--focus-areas', help='ä¼˜åŒ–é‡ç‚¹é¢†åŸŸï¼ˆé€—å·åˆ†éš”ï¼‰')
    optimize_parser.add_argument('--output', '-o', help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
    optimize_parser.add_argument('--dry-run', action='store_true', help='å¹²è¿è¡Œæ¨¡å¼ï¼ˆä¸è°ƒç”¨LLMï¼‰')
    
    # æœç´¢å‘½ä»¤
    search_parser = subparsers.add_parser('search', help='èŒä½æœç´¢')
    search_parser.add_argument('query', help='æœç´¢æŸ¥è¯¢')
    search_parser.add_argument('--limit', '-l', type=int, default=10, help='è¿”å›ç»“æœæ•°é‡')
    search_parser.add_argument('--output', '-o', help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
    
    # æµ‹è¯•å‘½ä»¤
    test_parser = subparsers.add_parser('test', help='å‘é‡æ•°æ®åº“æµ‹è¯•')
    test_parser.add_argument('--sample-size', '-s', type=int, default=3, help='æ ·æœ¬æ–‡æ¡£æ•°é‡')
    test_parser.add_argument('--test-search', action='store_true', help='æµ‹è¯•æœç´¢åŠŸèƒ½')
    test_parser.add_argument('--queries', help='æµ‹è¯•æŸ¥è¯¢ï¼ˆé€—å·åˆ†éš”ï¼‰')
    test_parser.add_argument('--output', '-o', help='æµ‹è¯•æŠ¥å‘Šè¾“å‡ºè·¯å¾„')
    
    # æ¸…ç†å‘½ä»¤
    clear_parser = subparsers.add_parser('clear', help='æ¸…ç†å‘é‡æ•°æ®åº“')
    clear_parser.add_argument('--job-id', help='åˆ é™¤ç‰¹å®šèŒä½çš„æ–‡æ¡£')
    clear_parser.add_argument('--force', '-f', action='store_true', help='å¼ºåˆ¶åˆ é™¤ï¼Œä¸è¯¢é—®ç¡®è®¤')
    
    # ç®€å†åŒ¹é…å‘½ä»¤
    match_parser = subparsers.add_parser('match', help='ç®€å†èŒä½åŒ¹é…')
    match_parser.add_argument('action', choices=[
        'find-jobs', 'analyze-fit', 'batch-analyze', 'generate-report'
    ], help='åŒ¹é…æ“ä½œ')
    match_parser.add_argument('--resume', '-r', help='ç®€å†æ–‡ä»¶è·¯å¾„')
    match_parser.add_argument('--resume-config', help='ç®€å†åŒ¹é…é…ç½®æ–‡ä»¶è·¯å¾„')
    match_parser.add_argument('--limit', '-l', type=int, default=20, help='è¿”å›åŒ¹é…èŒä½æ•°é‡')
    match_parser.add_argument('--output', '-o', help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
    match_parser.add_argument('--filters', help='è¿‡æ»¤æ¡ä»¶ï¼ˆJSONæ ¼å¼ï¼‰')
    match_parser.add_argument('--threshold', type=float, default=0.5, help='åŒ¹é…åº¦é˜ˆå€¼')
    match_parser.add_argument('--job-id', help='ç‰¹å®šèŒä½IDï¼ˆç”¨äºanalyze-fitï¼‰')
    match_parser.add_argument('--job-list', help='èŒä½IDåˆ—è¡¨æ–‡ä»¶è·¯å¾„ï¼ˆç”¨äºbatch-analyzeï¼‰')
    match_parser.add_argument('--dry-run', action='store_true', help='å¹²è¿è¡Œæ¨¡å¼ï¼ˆä¸æ‰§è¡Œå®é™…åŒ¹é…ï¼‰')
    
    # ç®€å†å¤„ç†å‘½ä»¤
    resume_parser = subparsers.add_parser('resume', help='ç®€å†æ–‡æ¡£å¤„ç†')
    resume_parser.add_argument('action', choices=[
        'process', 'batch-process', 'validate', 'match'
    ], help='å¤„ç†æ“ä½œ')
    
    # é€šç”¨å‚æ•°
    resume_parser.add_argument('--input', '-i', help='è¾“å…¥æ–‡ä»¶è·¯å¾„')
    resume_parser.add_argument('--output', '-o', help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
    resume_parser.add_argument('--format', choices=['md', 'docx', 'pdf', 'auto'],
                              default='auto', help='æ–‡æ¡£æ ¼å¼')
    resume_parser.add_argument('--dry-run', action='store_true', help='å¹²è¿è¡Œæ¨¡å¼')
    resume_parser.add_argument('--hints', help='ç”¨æˆ·æç¤ºä¿¡æ¯ï¼ˆJSONæ ¼å¼ï¼‰')
    
    # æ‰¹é‡å¤„ç†å‚æ•°
    resume_parser.add_argument('--input-dir', help='è¾“å…¥ç›®å½•è·¯å¾„')
    resume_parser.add_argument('--output-dir', help='è¾“å‡ºç›®å½•è·¯å¾„')
    resume_parser.add_argument('--formats', help='æ”¯æŒçš„æ ¼å¼åˆ—è¡¨ï¼ˆé€—å·åˆ†éš”ï¼‰')
    resume_parser.add_argument('--parallel', type=int, default=1, help='å¹¶è¡Œå¤„ç†æ•°é‡')
    
    # éªŒè¯å‚æ•°
    resume_parser.add_argument('--schema-check', action='store_true', help='æ¨¡å¼éªŒè¯')
    resume_parser.add_argument('--completeness-check', action='store_true', help='å®Œæ•´æ€§æ£€æŸ¥')
    
    # åŒ¹é…å‚æ•°
    resume_parser.add_argument('--limit', type=int, default=20, help='åŒ¹é…èŒä½æ•°é‡')
    
    # æ™ºèƒ½èŠå¤©å‘½ä»¤
    chat_parser = subparsers.add_parser('chat', help='æ™ºèƒ½åˆ†æèŠå¤©')
    chat_parser.add_argument('--agent-config', help='Agenté…ç½®æ–‡ä»¶è·¯å¾„')
    chat_parser.add_argument('--show-help', action='store_true', help='æ˜¾ç¤ºè¯¦ç»†å¸®åŠ©ä¿¡æ¯')
    chat_parser.add_argument('--verbose', '-v', action='store_true', help='æ˜¾ç¤ºè¯¦ç»†å¤„ç†ä¿¡æ¯')
    chat_parser.add_argument('--debug', action='store_true', help='è°ƒè¯•æ¨¡å¼')
    chat_parser.add_argument('--save-conversations', action='store_true', help='ä¿å­˜å¯¹è¯è®°å½•')
    chat_parser.add_argument('--conversation-dir', default='logs/conversations', help='å¯¹è¯è®°å½•ä¿å­˜ç›®å½•')
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—
    setup_logging(args.log_level, args.log_file)
    
    # ç¡®ä¿å¿…è¦ç›®å½•å­˜åœ¨
    Path('./logs').mkdir(exist_ok=True)
    Path('./pipeline_results').mkdir(exist_ok=True)
    
    # æ‰§è¡Œå‘½ä»¤
    success = False
    
    try:
        if args.command == 'pipeline':
            success = asyncio.run(pipeline_command(args))
        elif args.command == 'status':
            success = asyncio.run(status_command(args))
        elif args.command == 'optimize':
            success = asyncio.run(optimize_command(args))
        elif args.command == 'search':
            success = asyncio.run(search_command(args))
        elif args.command == 'test':
            success = asyncio.run(test_command(args))
        elif args.command == 'clear':
            success = asyncio.run(clear_command(args))
        elif args.command == 'match':
            success = asyncio.run(match_command(args))
        elif args.command == 'resume':
            success = asyncio.run(resume_command(args))
        elif args.command == 'chat':
            success = asyncio.run(chat_command(args))
        else:
            parser.print_help()
            success = False
    except KeyboardInterrupt:
        print("\nâš ï¸ ç”¨æˆ·ä¸­æ–­æ“ä½œ")
        success = False
    except Exception as e:
        print(f"âŒ æ‰§è¡Œå¤±è´¥: {e}")
        success = False
    
    sys.exit(0 if success else 1)

if __name__ == "__main__":
    main()