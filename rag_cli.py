#!/usr/bin/env python3
"""
RAGç³»ç»Ÿç»Ÿä¸€CLIæ¥å£

æä¾›å®Œæ•´çš„RAGç³»ç»Ÿå‘½ä»¤è¡Œæ¥å£ï¼ŒåŒ…æ‹¬æ•°æ®å¤„ç†ã€ç®€å†ä¼˜åŒ–ã€èŒä½åŒ¹é…ç­‰åŠŸèƒ½
"""

import sys
import asyncio
import argparse
import logging
import json
from pathlib import Path
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(str(Path(__file__).parent))

from src.rag.rag_system_coordinator import RAGSystemCoordinator
from src.rag.data_pipeline import RAGDataPipeline, create_progress_callback
from src.rag.resume_optimizer import ResumeOptimizer

def setup_logging(log_level: str = 'INFO', log_file: str = None):
    """è®¾ç½®æ—¥å¿—é…ç½®"""
    handlers = [logging.StreamHandler()]
    
    if log_file:
        Path(log_file).parent.mkdir(parents=True, exist_ok=True)
        handlers.append(logging.FileHandler(log_file, encoding='utf-8'))
    
    logging.basicConfig(
        level=getattr(logging, log_level.upper()),
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=handlers
    )

def load_config(config_file: str = None) -> dict:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    if config_file and Path(config_file).exists():
        with open(config_file, 'r', encoding='utf-8') as f:
            if config_file.endswith('.json'):
                return json.load(f)
            else:
                import yaml
                return yaml.safe_load(f)
    
    # é»˜è®¤é…ç½®
    return {
        'rag_system': {
            'database': {
                'path': './data/jobs.db',
                'batch_size': 50
            },
            'llm': {
                'provider': 'zhipu',
                'model': 'glm-4-flash',
                'api_key': 'your-api-key-here',
                'temperature': 0.1,
                'max_tokens': 2000
            },
            'vector_db': {
                'persist_directory': './chroma_db',
                'collection_name': 'job_positions',
                'embeddings': {
                    'model_name': 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',
                    'device': 'cpu',
                    'normalize_embeddings': True
                }
            },
            'documents': {
                'types': ['overview', 'responsibility', 'requirement', 'skills', 'basic_requirements'],
                'create_comprehensive_doc': False
            },
            'processing': {
                'skip_processed': True,
                'force_reprocess': False,
                'batch_size': 50,
                'max_retry_attempts': 3
            }
        }
    }

def load_resume(resume_file: str) -> dict:
    """åŠ è½½ç®€å†æ–‡ä»¶"""
    if not Path(resume_file).exists():
        raise FileNotFoundError(f"ç®€å†æ–‡ä»¶ä¸å­˜åœ¨: {resume_file}")
    
    with open(resume_file, 'r', encoding='utf-8') as f:
        if resume_file.endswith('.json'):
            return json.load(f)
        else:
            import yaml
            return yaml.safe_load(f)

async def pipeline_command(args):
    """æ•°æ®æµæ°´çº¿å‘½ä»¤"""
    print("ğŸš€ RAGæ•°æ®æµæ°´çº¿")
    print("=" * 40)
    
    try:
        config = load_config(args.config)
        
        # è¦†ç›–å‘½ä»¤è¡Œå‚æ•°
        if args.batch_size:
            config['rag_system']['processing']['batch_size'] = args.batch_size
        if args.force_reprocess:
            config['rag_system']['processing']['force_reprocess'] = True
        
        # åˆ›å»ºè¿›åº¦å›è°ƒ
        progress_callback = create_progress_callback() if args.show_progress else None
        
        # è¿è¡Œæµæ°´çº¿
        pipeline = RAGDataPipeline(config, progress_callback)
        result = await pipeline.run_full_pipeline(
            batch_size=args.batch_size or 50,
            max_jobs=args.max_jobs,
            force_reprocess=args.force_reprocess,
            save_progress=not args.no_save
        )
        
        # æ˜¾ç¤ºç»“æœæ‘˜è¦
        print("\nğŸ“Š æ‰§è¡Œç»“æœæ‘˜è¦:")
        exec_summary = result.get('execution_summary', {})
        proc_stats = result.get('processing_statistics', {})
        
        print(f"   çŠ¶æ€: {exec_summary.get('status', 'unknown')}")
        print(f"   æ‰§è¡Œæ—¶é—´: {exec_summary.get('execution_time', 0):.1f} ç§’")
        print(f"   å¤„ç†èŒä½: {proc_stats.get('processed_jobs', 0)}")
        print(f"   æˆåŠŸç‡: {proc_stats.get('success_rate', 0):.1f}%")
        
        return True
        
    except Exception as e:
        print(f"âŒ æµæ°´çº¿æ‰§è¡Œå¤±è´¥: {e}")
        return False

async def status_command(args):
    """ç³»ç»ŸçŠ¶æ€å‘½ä»¤"""
    print("ğŸ“Š RAGç³»ç»ŸçŠ¶æ€")
    print("=" * 30)
    
    try:
        config = load_config(args.config)
        coordinator = RAGSystemCoordinator(config)
        
        if not coordinator.initialize_system():
            print("âŒ ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥")
            return False
        
        # è·å–ç³»ç»ŸçŠ¶æ€
        system_status = coordinator.get_system_status()
        progress = coordinator.get_processing_progress()
        
        # æ˜¾ç¤ºç»„ä»¶çŠ¶æ€
        print("ç»„ä»¶çŠ¶æ€:")
        components = system_status.get('components', {})
        for comp_name, comp_status in components.items():
            status_icon = "âœ…" if comp_status else "âŒ"
            print(f"  {comp_name}: {status_icon}")
        
        # æ˜¾ç¤ºæ•°æ®ç»Ÿè®¡
        db_stats = progress.get('database_stats', {})
        print(f"\næ•°æ®åº“ç»Ÿè®¡:")
        print(f"  æ€»èŒä½æ•°: {db_stats.get('total', 0)}")
        print(f"  å·²å¤„ç†: {db_stats.get('processed', 0)}")
        print(f"  æœªå¤„ç†: {db_stats.get('unprocessed', 0)}")
        print(f"  å¤„ç†ç‡: {db_stats.get('processing_rate', 0):.1f}%")
        
        vector_stats = progress.get('vector_stats', {})
        print(f"\nå‘é‡æ•°æ®åº“:")
        print(f"  æ–‡æ¡£æ•°é‡: {vector_stats.get('document_count', 0)}")
        print(f"  é›†åˆåç§°: {vector_stats.get('collection_name', 'N/A')}")
        
        return True
        
    except Exception as e:
        print(f"âŒ çŠ¶æ€æŸ¥è¯¢å¤±è´¥: {e}")
        return False

async def optimize_command(args):
    """ç®€å†ä¼˜åŒ–å‘½ä»¤"""
    print("ğŸ“ ç®€å†ä¼˜åŒ–")
    print("=" * 20)
    
    try:
        # åŠ è½½é…ç½®å’Œç®€å†
        config = load_config(args.config)
        resume = load_resume(args.resume)
        
        # åˆå§‹åŒ–ç³»ç»Ÿ
        coordinator = RAGSystemCoordinator(config)
        if not coordinator.initialize_system():
            print("âŒ ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥")
            return False
        
        optimizer = ResumeOptimizer(coordinator, config['rag_system']['llm'])
        
        # æ ¹æ®å­å‘½ä»¤æ‰§è¡Œä¸åŒæ“ä½œ
        if args.action == 'analyze':
            # ç®€å†å·®è·åˆ†æ
            target_jobs = args.target_jobs.split(',') if args.target_jobs else []
            if not target_jobs:
                # è‡ªåŠ¨æŸ¥æ‰¾ç›¸å…³èŒä½
                all_jobs = coordinator.db_reader.get_jobs_for_rag_processing(limit=5)
                target_jobs = [job['job_id'] for job in all_jobs[:3]]
            
            print(f"åˆ†æç›®æ ‡èŒä½: {len(target_jobs)} ä¸ª")
            
            if not args.dry_run:
                result = await optimizer.analyze_resume_gaps(resume, target_jobs)
                
                if 'error' in result:
                    print(f"âŒ åˆ†æå¤±è´¥: {result['error']}")
                    return False
                
                # æ˜¾ç¤ºåˆ†æç»“æœ
                summary = result.get('summary', {})
                print(f"\nğŸ“Š åˆ†æç»“æœ:")
                print(f"   å¹³å‡åŒ¹é…åº¦: {summary.get('average_match_score', 0)}")
                print(f"   åˆ†æèŒä½æ•°: {summary.get('total_jobs_analyzed', 0)}")
                
                # ä¿å­˜ç»“æœ
                if args.output:
                    with open(args.output, 'w', encoding='utf-8') as f:
                        json.dump(result, f, ensure_ascii=False, indent=2, default=str)
                    print(f"   ç»“æœå·²ä¿å­˜åˆ°: {args.output}")
            else:
                print("ğŸ” å¹²è¿è¡Œæ¨¡å¼ - è·³è¿‡å®é™…åˆ†æ")
        
        elif args.action == 'optimize':
            # ç®€å†å†…å®¹ä¼˜åŒ–
            target_job_id = args.target_job or coordinator.db_reader.get_jobs_for_rag_processing(limit=1)[0]['job_id']
            
            print(f"ä¼˜åŒ–ç›®æ ‡èŒä½: {target_job_id}")
            
            if not args.dry_run:
                result = await optimizer.optimize_resume_content(
                    resume, 
                    target_job_id, 
                    args.focus_areas.split(',') if args.focus_areas else None
                )
                
                if 'error' in result:
                    print(f"âŒ ä¼˜åŒ–å¤±è´¥: {result['error']}")
                    return False
                
                print("âœ… ç®€å†ä¼˜åŒ–å®Œæˆ")
                
                # ä¿å­˜ç»“æœ
                if args.output:
                    with open(args.output, 'w', encoding='utf-8') as f:
                        json.dump(result, f, ensure_ascii=False, indent=2, default=str)
                    print(f"   ç»“æœå·²ä¿å­˜åˆ°: {args.output}")
            else:
                print("ğŸ” å¹²è¿è¡Œæ¨¡å¼ - è·³è¿‡å®é™…ä¼˜åŒ–")
        
        elif args.action == 'cover-letter':
            # ç”Ÿæˆæ±‚èŒä¿¡
            target_job_id = args.target_job or coordinator.db_reader.get_jobs_for_rag_processing(limit=1)[0]['job_id']
            
            print(f"ç”Ÿæˆæ±‚èŒä¿¡ï¼Œç›®æ ‡èŒä½: {target_job_id}")
            
            if not args.dry_run:
                cover_letter = await optimizer.generate_cover_letter(resume, target_job_id)
                
                print("âœ… æ±‚èŒä¿¡ç”Ÿæˆå®Œæˆ")
                print(f"   é•¿åº¦: {len(cover_letter)} å­—ç¬¦")
                
                # ä¿å­˜æ±‚èŒä¿¡
                if args.output:
                    with open(args.output, 'w', encoding='utf-8') as f:
                        f.write(cover_letter)
                    print(f"   æ±‚èŒä¿¡å·²ä¿å­˜åˆ°: {args.output}")
                else:
                    print("\n" + "="*50)
                    print(cover_letter)
                    print("="*50)
            else:
                print("ğŸ” å¹²è¿è¡Œæ¨¡å¼ - è·³è¿‡å®é™…ç”Ÿæˆ")
        
        return True
        
    except Exception as e:
        print(f"âŒ ç®€å†ä¼˜åŒ–å¤±è´¥: {e}")
        return False

async def search_command(args):
    """èŒä½æœç´¢å‘½ä»¤"""
    print("ğŸ” èŒä½æœç´¢")
    print("=" * 20)
    
    try:
        config = load_config(args.config)
        coordinator = RAGSystemCoordinator(config)
        
        if not coordinator.initialize_system():
            print("âŒ ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥")
            return False
        
        # æ‰§è¡Œæœç´¢
        query = args.query
        k = args.limit or 10
        
        print(f"æœç´¢æŸ¥è¯¢: {query}")
        print(f"è¿”å›æ•°é‡: {k}")
        
        # ä½¿ç”¨å‘é‡æœç´¢
        similar_docs = coordinator.vector_manager.search_similar_jobs(query, k=k)
        
        if not similar_docs:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°ç›¸å…³èŒä½")
            return False
        
        print(f"\nğŸ“‹ æœç´¢ç»“æœ ({len(similar_docs)} ä¸ª):")
        
        for i, doc in enumerate(similar_docs, 1):
            metadata = doc.metadata
            print(f"\n{i}. {metadata.get('job_title', 'æœªçŸ¥èŒä½')}")
            print(f"   å…¬å¸: {metadata.get('company', 'æœªçŸ¥å…¬å¸')}")
            print(f"   åœ°ç‚¹: {metadata.get('location', 'æœªçŸ¥åœ°ç‚¹')}")
            print(f"   ç±»å‹: {metadata.get('type', 'æœªçŸ¥ç±»å‹')}")
            print(f"   å†…å®¹: {doc.page_content[:100]}...")
        
        # ä¿å­˜æœç´¢ç»“æœ
        if args.output:
            results = []
            for doc in similar_docs:
                results.append({
                    'content': doc.page_content,
                    'metadata': doc.metadata
                })
            
            with open(args.output, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2, default=str)
            print(f"\nğŸ’¾ æœç´¢ç»“æœå·²ä¿å­˜åˆ°: {args.output}")
        
        return True
        
    except Exception as e:
        print(f"âŒ æœç´¢å¤±è´¥: {e}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='RAGæ™ºèƒ½ç®€å†æŠ•é€’ç³»ç»ŸCLI',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  # è¿è¡Œæ•°æ®æµæ°´çº¿
  python rag_cli.py pipeline run --batch-size 20 --show-progress
  
  # æŸ¥çœ‹ç³»ç»ŸçŠ¶æ€
  python rag_cli.py status
  
  # åˆ†æç®€å†å·®è·
  python rag_cli.py optimize analyze --resume resume.json --output analysis.json
  
  # ä¼˜åŒ–ç®€å†å†…å®¹
  python rag_cli.py optimize optimize --resume resume.json --target-job job123
  
  # ç”Ÿæˆæ±‚èŒä¿¡
  python rag_cli.py optimize cover-letter --resume resume.json --target-job job123
  
  # æœç´¢ç›¸å…³èŒä½
  python rag_cli.py search "Pythonå¼€å‘å·¥ç¨‹å¸ˆ" --limit 5
        """
    )
    
    # å…¨å±€å‚æ•°
    parser.add_argument('--config', '-c', help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--log-level', default='INFO', 
                       choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'], 
                       help='æ—¥å¿—çº§åˆ«')
    parser.add_argument('--log-file', help='æ—¥å¿—æ–‡ä»¶è·¯å¾„')
    
    # å­å‘½ä»¤
    subparsers = parser.add_subparsers(dest='command', help='å¯ç”¨å‘½ä»¤')
    
    # æµæ°´çº¿å‘½ä»¤
    pipeline_parser = subparsers.add_parser('pipeline', help='æ•°æ®æµæ°´çº¿ç®¡ç†')
    pipeline_parser.add_argument('action', choices=['run', 'resume'], help='æµæ°´çº¿æ“ä½œ')
    pipeline_parser.add_argument('--batch-size', '-b', type=int, help='æ‰¹å¤„ç†å¤§å°')
    pipeline_parser.add_argument('--max-jobs', '-m', type=int, help='æœ€å¤§å¤„ç†èŒä½æ•°é‡')
    pipeline_parser.add_argument('--force-reprocess', '-f', action='store_true', help='å¼ºåˆ¶é‡æ–°å¤„ç†')
    pipeline_parser.add_argument('--no-save', action='store_true', help='ä¸ä¿å­˜å¤„ç†ç»“æœ')
    pipeline_parser.add_argument('--show-progress', '-p', action='store_true', help='æ˜¾ç¤ºå¤„ç†è¿›åº¦')
    
    # çŠ¶æ€å‘½ä»¤
    status_parser = subparsers.add_parser('status', help='æŸ¥è¯¢ç³»ç»ŸçŠ¶æ€')
    
    # ç®€å†ä¼˜åŒ–å‘½ä»¤
    optimize_parser = subparsers.add_parser('optimize', help='ç®€å†ä¼˜åŒ–')
    optimize_parser.add_argument('action', choices=['analyze', 'optimize', 'cover-letter'], help='ä¼˜åŒ–æ“ä½œ')
    optimize_parser.add_argument('--resume', '-r', required=True, help='ç®€å†æ–‡ä»¶è·¯å¾„')
    optimize_parser.add_argument('--target-jobs', help='ç›®æ ‡èŒä½IDåˆ—è¡¨ï¼ˆé€—å·åˆ†éš”ï¼‰')
    optimize_parser.add_argument('--target-job', help='å•ä¸ªç›®æ ‡èŒä½ID')
    optimize_parser.add_argument('--focus-areas', help='ä¼˜åŒ–é‡ç‚¹é¢†åŸŸï¼ˆé€—å·åˆ†éš”ï¼‰')
    optimize_parser.add_argument('--output', '-o', help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
    optimize_parser.add_argument('--dry-run', action='store_true', help='å¹²è¿è¡Œæ¨¡å¼ï¼ˆä¸è°ƒç”¨LLMï¼‰')
    
    # æœç´¢å‘½ä»¤
    search_parser = subparsers.add_parser('search', help='èŒä½æœç´¢')
    search_parser.add_argument('query', help='æœç´¢æŸ¥è¯¢')
    search_parser.add_argument('--limit', '-l', type=int, default=10, help='è¿”å›ç»“æœæ•°é‡')
    search_parser.add_argument('--output', '-o', help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—
    setup_logging(args.log_level, args.log_file)
    
    # ç¡®ä¿å¿…è¦ç›®å½•å­˜åœ¨
    Path('./logs').mkdir(exist_ok=True)
    Path('./pipeline_results').mkdir(exist_ok=True)
    
    # æ‰§è¡Œå‘½ä»¤
    success = False
    
    try:
        if args.command == 'pipeline':
            success = asyncio.run(pipeline_command(args))
        elif args.command == 'status':
            success = asyncio.run(status_command(args))
        elif args.command == 'optimize':
            success = asyncio.run(optimize_command(args))
        elif args.command == 'search':
            success = asyncio.run(search_command(args))
        else:
            parser.print_help()
            success = False
    except KeyboardInterrupt:
        print("\nâš ï¸ ç”¨æˆ·ä¸­æ–­æ“ä½œ")
        success = False
    except Exception as e:
        print(f"âŒ æ‰§è¡Œå¤±è´¥: {e}")
        success = False
    
    sys.exit(0 if success else 1)

if __name__ == "__main__":
    main()