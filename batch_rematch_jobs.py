#!/usr/bin/env python3
"""
æ‰¹é‡é‡æ–°åŒ¹é…è„šæœ¬
ç”¨äºä¿®å¤ä½åŒ¹é…ç‡é—®é¢˜ï¼Œé‡æ–°åŒ¹é…æ‰€æœ‰æœªåŒ¹é…çš„èŒä½
å‚è€ƒ test_optimized_matching.py çš„ä¼˜åŒ–é…ç½®
"""

import asyncio
import sys
import os
import time
import json
import sqlite3
import yaml
from typing import List, Dict, Any
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(__file__))

from src.database.operations import DatabaseManager
from src.rag.vector_manager import ChromaDBManager
from src.matcher.generic_resume_matcher import GenericResumeJobMatcher
from src.matcher.generic_resume_models import GenericResumeProfile, SkillCategory, WorkExperience
from src.utils.logger import get_logger


def load_integration_config(config_path: str = "config/integration_config.yaml") -> dict:
    """åŠ è½½é›†æˆé…ç½®æ–‡ä»¶"""
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)
    except Exception as e:
        print(f"âš ï¸ æ— æ³•åŠ è½½é…ç½®æ–‡ä»¶ {config_path}: {e}")
        return {}


class BatchRematcher:
    """æ‰¹é‡é‡æ–°åŒ¹é…å™¨"""
    
    def __init__(self, db_path: str = 'data/jobs.db'):
        self.db_path = db_path
        self.logger = get_logger(__name__)
        
        # åŠ è½½é›†æˆé…ç½®
        self.integration_config = load_integration_config()
        
        # åˆå§‹åŒ–å‘é‡ç®¡ç†å™¨
        self.vector_manager = self._init_vector_manager()
        
        # åˆå§‹åŒ–æ•°æ®åº“ç®¡ç†å™¨
        self.db_manager = DatabaseManager(db_path)
        
        # åŠ è½½ä¼˜åŒ–é…ç½®å¹¶åˆå§‹åŒ–åŒ¹é…å™¨
        config = self._load_optimized_config()
        self.matcher = GenericResumeJobMatcher(self.vector_manager, config)
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_jobs': 0,
            'unmatched_jobs': 0,
            'processed_jobs': 0,
            'new_matches': 0,
            'failed_jobs': 0,
            'start_time': None,
            'end_time': None
        }
    
    def _init_vector_manager(self) -> ChromaDBManager:
        """åˆå§‹åŒ–å‘é‡ç®¡ç†å™¨ - å‚è€ƒ test_optimized_matching.py"""
        try:
            # ä»é›†æˆé…ç½®ä¸­æå–å‘é‡æ•°æ®åº“é…ç½®
            rag_config = self.integration_config.get('rag_system', {})
            vector_db_config = rag_config.get('vector_db', {})
            llm_config = rag_config.get('llm', {})
            
            # æ„å»ºå‘é‡ç®¡ç†å™¨é…ç½®
            vector_manager_config = {
                'persist_directory': vector_db_config.get('persist_directory', './chroma_db'),
                'collection_name': vector_db_config.get('collection_name', 'job_positions'),
                'embeddings': {
                    'model_name': 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',
                    'device': 'cpu',
                    'normalize_embeddings': True
                },
                'llm': {
                    'provider': llm_config.get('provider', 'zhipu'),
                    'model': llm_config.get('model', 'glm-4-flash'),
                    'api_key': llm_config.get('api_key', ''),
                    'temperature': llm_config.get('temperature', 0.1),
                    'max_tokens': llm_config.get('max_tokens', 1500)
                }
            }
            
            self.logger.info(f"ğŸ“ å‘é‡æ•°æ®åº“ç›®å½•: {vector_manager_config['persist_directory']}")
            self.logger.info(f"ğŸ“š é›†åˆåç§°: {vector_manager_config['collection_name']}")
            
            return ChromaDBManager(vector_manager_config)
            
        except Exception as e:
            self.logger.error(f"å‘é‡ç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            self.logger.info("è¿™å¯èƒ½æ˜¯ç”±äºHugging Faceè¿æ¥é—®é¢˜å¯¼è‡´çš„")
            self.logger.info("å»ºè®®è§£å†³æ–¹æ¡ˆ:")
            self.logger.info("1. æ£€æŸ¥ç½‘ç»œè¿æ¥")
            self.logger.info("2. ä½¿ç”¨VPNæˆ–ä»£ç†")
            self.logger.info("3. é¢„å…ˆä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°")
            raise
    
    def _load_optimized_config(self) -> Dict[str, Any]:
        """åŠ è½½ä¼˜åŒ–é…ç½® - å‚è€ƒ test_optimized_matching.py"""
        # ä»é›†æˆé…ç½®ä¸­æå–åŒ¹é…ç®—æ³•é…ç½®
        resume_matching_config = self.integration_config.get('modules', {}).get('resume_matching', {})
        
        # ä¼˜å…ˆä½¿ç”¨é«˜çº§åŒ¹é…é…ç½®
        advanced_matching_config = self.integration_config.get('resume_matching_advanced', {})
        
        if advanced_matching_config:
            self.logger.info("ğŸš€ ä½¿ç”¨é«˜çº§åŒ¹é…é…ç½®")
            # ä½¿ç”¨é«˜çº§é…ç½®çš„æƒé‡å’Œé˜ˆå€¼
            config = {
                'min_score_threshold': advanced_matching_config.get('match_thresholds', {}).get('poor', 0.25),
                'default_search_k': 80,
                'max_results': resume_matching_config.get('max_matches_per_resume', 30),
                'weights': advanced_matching_config.get('matching_weights', {
                    'semantic_similarity': 0.40,
                    'skills_match': 0.35,
                    'experience_match': 0.15,
                    'industry_match': 0.08,
                    'salary_match': 0.02
                })
            }
            self.logger.info(f"ğŸ“Š é«˜çº§é…ç½®æƒé‡: {config['weights']}")
        else:
            self.logger.info("ğŸ“‹ ä½¿ç”¨æ ‡å‡†ä¼˜åŒ–é…ç½®")
            # æ„å»ºä¼˜åŒ–çš„åŒ¹é…å™¨é…ç½®
            config = {
                'min_score_threshold': 0.25,  # é™ä½é˜ˆå€¼ä»0.3åˆ°0.25
                'default_search_k': 80,       # å¢åŠ æœç´¢èŒƒå›´
                'max_results': resume_matching_config.get('max_matches_per_resume', 30),
                'weights': {
                    'semantic_similarity': 0.40,  # æé«˜è¯­ä¹‰ç›¸ä¼¼åº¦æƒé‡
                    'skills_match': 0.35,         # æé«˜æŠ€èƒ½åŒ¹é…æƒé‡
                    'experience_match': 0.15,     # é™ä½ç»éªŒæƒé‡
                    'industry_match': 0.08,       # é™ä½è¡Œä¸šæƒé‡
                    'salary_match': 0.02          # å¤§å¹…é™ä½è–ªèµ„æƒé‡
                }
            }
        
        self.logger.info(f"ğŸ“‹ ä½¿ç”¨é…ç½®: é˜ˆå€¼={config['min_score_threshold']}, æœ€å¤§ç»“æœ={config['max_results']}")
        return config
    
    async def run_batch_rematch(self, limit: int = None) -> Dict[str, Any]:
        """è¿è¡Œæ‰¹é‡é‡æ–°åŒ¹é…"""
        self.stats['start_time'] = datetime.now()
        
        try:
            self.logger.info("ğŸš€ å¼€å§‹æ‰¹é‡é‡æ–°åŒ¹é…ä»»åŠ¡")
            
            # 1. åˆ†æå½“å‰çŠ¶æ€
            await self._analyze_current_state()
            
            # 2. è·å–æœªåŒ¹é…çš„èŒä½
            unmatched_jobs = await self._get_unmatched_jobs(limit)
            
            if not unmatched_jobs:
                self.logger.info("âœ… æ²¡æœ‰æ‰¾åˆ°éœ€è¦é‡æ–°åŒ¹é…çš„èŒä½")
                return self._generate_report()
            
            self.logger.info(f"ğŸ“‹ æ‰¾åˆ° {len(unmatched_jobs)} ä¸ªæœªåŒ¹é…èŒä½")
            
            # 3. åˆ›å»ºç®€å†æ¡£æ¡ˆ
            resume_profile = await self._create_resume_profile()
            
            # 4. æ‰¹é‡åŒ¹é…
            await self._batch_match_jobs(unmatched_jobs, resume_profile)
            
            # 5. ç”ŸæˆæŠ¥å‘Š
            self.stats['end_time'] = datetime.now()
            report = self._generate_report()
            
            self.logger.info("âœ… æ‰¹é‡é‡æ–°åŒ¹é…å®Œæˆ")
            self.logger.info(f"ğŸ“Š å¤„ç†ç»“æœ: æ–°å¢åŒ¹é… {self.stats['new_matches']}, "
                           f"å¤±è´¥ {self.stats['failed_jobs']}, "
                           f"è€—æ—¶ {(self.stats['end_time'] - self.stats['start_time']).total_seconds():.1f}ç§’")
            
            return report
            
        except Exception as e:
            self.logger.error(f"ğŸ’¥ æ‰¹é‡é‡æ–°åŒ¹é…å¤±è´¥: {str(e)}")
            raise
    
    async def _analyze_current_state(self):
        """åˆ†æå½“å‰åŒ¹é…çŠ¶æ€"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # æ€»èŒä½æ•°
            cursor.execute("SELECT COUNT(*) FROM jobs WHERE rag_processed = 1")
            self.stats['total_jobs'] = cursor.fetchone()[0]
            
            # å·²åŒ¹é…èŒä½æ•°
            cursor.execute("SELECT COUNT(DISTINCT job_id) FROM resume_matches")
            matched_jobs = cursor.fetchone()[0]
            
            # æœªåŒ¹é…èŒä½æ•°
            self.stats['unmatched_jobs'] = self.stats['total_jobs'] - matched_jobs
            
            # å½“å‰åŒ¹é…ç‡
            current_match_rate = matched_jobs / self.stats['total_jobs'] if self.stats['total_jobs'] > 0 else 0
            
            conn.close()
            
            self.logger.info(f"ğŸ“Š å½“å‰çŠ¶æ€åˆ†æ:")
            self.logger.info(f"   æ€»èŒä½æ•°: {self.stats['total_jobs']}")
            self.logger.info(f"   å·²åŒ¹é…: {matched_jobs}")
            self.logger.info(f"   æœªåŒ¹é…: {self.stats['unmatched_jobs']}")
            self.logger.info(f"   åŒ¹é…ç‡: {current_match_rate:.1%}")
            
        except Exception as e:
            self.logger.error(f"åˆ†æå½“å‰çŠ¶æ€å¤±è´¥: {str(e)}")
            raise
    
    async def _get_unmatched_jobs(self, limit: int = None) -> List[str]:
        """è·å–æœªåŒ¹é…çš„èŒä½IDåˆ—è¡¨"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            query = """
                SELECT job_id FROM jobs 
                WHERE rag_processed = 1 
                AND job_id NOT IN (SELECT DISTINCT job_id FROM resume_matches)
                ORDER BY created_at DESC
            """
            
            if limit:
                query += f" LIMIT {limit}"
            
            cursor.execute(query)
            unmatched_jobs = [row[0] for row in cursor.fetchall()]
            
            conn.close()
            return unmatched_jobs
            
        except Exception as e:
            self.logger.error(f"è·å–æœªåŒ¹é…èŒä½å¤±è´¥: {str(e)}")
            return []
    
    async def _create_resume_profile(self) -> GenericResumeProfile:
        """åˆ›å»ºç®€å†æ¡£æ¡ˆ - å‚è€ƒ test_optimized_matching.py"""
        try:
            # å°è¯•ä»æ–‡ä»¶åŠ è½½
            resume_file = 'testdata/resume.json'
            if os.path.exists(resume_file):
                with open(resume_file, 'r', encoding='utf-8') as f:
                    resume_data = json.load(f)
                profile = GenericResumeProfile.from_dict(resume_data)
                self.logger.info(f"âœ… ä» {resume_file} åŠ è½½ç®€å†æ¡£æ¡ˆ: {profile.name}")
                return profile
        except Exception as e:
            self.logger.warning(f"ä»æ–‡ä»¶åŠ è½½ç®€å†å¤±è´¥: {e}")
        
        # åˆ›å»ºé»˜è®¤æ¡£æ¡ˆ - ä¸ test_optimized_matching.py ä¿æŒä¸€è‡´
        self.logger.info("ä½¿ç”¨é»˜è®¤ç®€å†æ¡£æ¡ˆ")
        return GenericResumeProfile(
            name="å å½¬ (Zhan Bin)",
            total_experience_years=20,
            current_position="æ•°æ®å¹³å°æ¶æ„å¸ˆ",
            skill_categories=[
                SkillCategory(
                    category_name="programming_languages",
                    skills=["Python", "Java", "C#", "JavaScript", "Go"],
                    proficiency_level="expert"
                ),
                SkillCategory(
                    category_name="ai_ml_skills",
                    skills=["æœºå™¨å­¦ä¹ ", "æ·±åº¦å­¦ä¹ ", "AI", "PyTorch", "TensorFlow"],
                    proficiency_level="advanced"
                ),
                SkillCategory(
                    category_name="data_engineering_skills",
                    skills=["Databricks", "Spark", "æ•°æ®æ¶æ„", "æ•°æ®æ²»ç†", "é¡¹ç›®ç®¡ç†"],
                    proficiency_level="advanced"
                )
            ],
            work_history=[
                WorkExperience(
                    company="Zoetis",
                    position="æ•°æ®å¹³å°æ¶æ„å¸ˆ",
                    industry="åˆ¶è¯/åŒ»ç–—",
                    duration_years=0.8,
                    technologies=["Databricks", "Azure", "Python"]
                )
            ],
            expected_salary_range={"min": 300000, "max": 500000},
            industry_experience={"åˆ¶è¯/åŒ»ç–—": 6.4, "IT/äº’è”ç½‘": 5.8}
        )
    
    async def _batch_match_jobs(self, job_ids: List[str], resume_profile: GenericResumeProfile):
        """æ‰¹é‡åŒ¹é…èŒä½"""
        batch_size = 10  # å‡å°‘æ‰¹æ¬¡å¤§å°ï¼Œæé«˜ç¨³å®šæ€§
        
        for i in range(0, len(job_ids), batch_size):
            batch = job_ids[i:i + batch_size]
            batch_num = i//batch_size + 1
            total_batches = (len(job_ids) + batch_size - 1)//batch_size
            
            self.logger.info(f"ğŸ”„ å¤„ç†æ‰¹æ¬¡ {batch_num}/{total_batches}, èŒä½æ•°: {len(batch)}")
            self.logger.info(f"ğŸ“Š å½“å‰è¿›åº¦: {i + len(batch)}/{len(job_ids)} ({((i + len(batch))/len(job_ids)*100):.1f}%)")
            
            await self._process_job_batch(batch, resume_profile)
            
            # çŸ­æš‚ä¼‘æ¯é¿å…è¿‡è½½
            await asyncio.sleep(2)  # å¢åŠ ä¼‘æ¯æ—¶é—´
    
    async def _process_job_batch(self, job_ids: List[str], resume_profile: GenericResumeProfile):
        """å¤„ç†ä¸€æ‰¹èŒä½"""
        for job_id in job_ids:
            try:
                self.stats['processed_jobs'] += 1
                
                # æ„å»ºèŒä½è¿‡æ»¤å™¨
                filters = {"job_id": job_id}
                
                # æ‰§è¡ŒåŒ¹é…
                result = await self.matcher.find_matching_jobs(
                    resume_profile,
                    filters=filters,
                    top_k=1
                )
                
                # ä¿å­˜åŒ¹é…ç»“æœ
                if result.matches:
                    await self._save_match_result(result.matches[0])
                    self.stats['new_matches'] += 1
                    self.logger.info(f"âœ… èŒä½ {job_id} åŒ¹é…æˆåŠŸï¼Œåˆ†æ•°: {result.matches[0].overall_score:.3f}")
                else:
                    # è®°å½•æŸ¥è¯¢å…ƒæ•°æ®ä»¥ä¾¿è°ƒè¯•
                    metadata = result.query_metadata
                    self.logger.debug(f"âš ï¸ èŒä½ {job_id} æœªäº§ç”ŸåŒ¹é…ç»“æœ")
                    self.logger.debug(f"   æœç´¢ç»“æœæ•°: {metadata.get('search_results_count', 0)}")
                    self.logger.debug(f"   å€™é€‰èŒä½æ•°: {metadata.get('candidate_jobs_count', 0)}")
                    self.logger.debug(f"   æˆåŠŸåŒ¹é…æ•°: {metadata.get('successful_matches', 0)}")
                    self.logger.debug(f"   ä½åˆ†æ•°é‡: {metadata.get('below_threshold', 0)}")
                
            except Exception as e:
                self.stats['failed_jobs'] += 1
                self.logger.warning(f"âŒ å¤„ç†èŒä½ {job_id} å¤±è´¥: {str(e)}")
                # æ·»åŠ æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
                import traceback
                self.logger.debug(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
    
    async def _save_match_result(self, match_result):
        """ä¿å­˜åŒ¹é…ç»“æœåˆ°æ•°æ®åº“"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute("""
                INSERT OR REPLACE INTO resume_matches 
                (job_id, resume_profile_id, match_score, priority_level, 
                 semantic_score, skill_match_score, experience_match_score, 
                 location_match_score, salary_match_score, match_details, 
                 match_reasons, created_at, processed)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, datetime('now'), 0)
            """, (
                match_result.job_id,
                'default',
                match_result.overall_score,
                match_result.match_level.value if hasattr(match_result.match_level, 'value') else str(match_result.match_level),
                match_result.dimension_scores.get('semantic_similarity', 0),
                match_result.dimension_scores.get('skills_match', 0),
                match_result.dimension_scores.get('experience_match', 0),
                match_result.dimension_scores.get('industry_match', 0),
                match_result.dimension_scores.get('salary_match', 0),
                json.dumps(match_result.dimension_scores),
                f"æ‰¹é‡é‡æ–°åŒ¹é…: {match_result.job_title} at {match_result.company}"
            ))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜åŒ¹é…ç»“æœå¤±è´¥: {str(e)}")
    
    def _generate_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆå¤„ç†æŠ¥å‘Š"""
        processing_time = 0
        if self.stats['start_time'] and self.stats['end_time']:
            processing_time = (self.stats['end_time'] - self.stats['start_time']).total_seconds()
        
        # è®¡ç®—æ–°çš„åŒ¹é…ç‡
        new_total_matches = self.stats['total_jobs'] - self.stats['unmatched_jobs'] + self.stats['new_matches']
        new_match_rate = new_total_matches / self.stats['total_jobs'] if self.stats['total_jobs'] > 0 else 0
        
        old_match_rate = (self.stats['total_jobs'] - self.stats['unmatched_jobs']) / self.stats['total_jobs'] if self.stats['total_jobs'] > 0 else 0
        
        return {
            "summary": {
                "total_jobs": self.stats['total_jobs'],
                "originally_unmatched": self.stats['unmatched_jobs'],
                "processed_jobs": self.stats['processed_jobs'],
                "new_matches": self.stats['new_matches'],
                "failed_jobs": self.stats['failed_jobs'],
                "processing_time_seconds": processing_time
            },
            "match_rate_improvement": {
                "before": f"{old_match_rate:.1%}",
                "after": f"{new_match_rate:.1%}",
                "improvement": f"{(new_match_rate - old_match_rate):.1%}"
            },
            "performance": {
                "success_rate": f"{(self.stats['new_matches'] / self.stats['processed_jobs'] * 100):.1f}%" if self.stats['processed_jobs'] > 0 else "0%",
                "jobs_per_second": f"{(self.stats['processed_jobs'] / processing_time):.1f}" if processing_time > 0 else "N/A"
            },
            "recommendations": self._generate_recommendations()
        }
    
    def _generate_recommendations(self) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        recommendations = []
        
        if self.stats['processed_jobs'] > 0:
            success_rate = self.stats['new_matches'] / self.stats['processed_jobs']
            
            if success_rate < 0.2:
                recommendations.append("åŒ¹é…æˆåŠŸç‡å¾ˆä½ï¼Œå»ºè®®è¿›ä¸€æ­¥é™ä½é˜ˆå€¼åˆ°0.20æˆ–æ£€æŸ¥å‘é‡æ•°æ®åº“è´¨é‡")
            elif success_rate < 0.4:
                recommendations.append("åŒ¹é…æˆåŠŸç‡åä½ï¼Œå»ºè®®æ£€æŸ¥åŒ¹é…ç®—æ³•æƒé‡é…ç½®")
            elif success_rate > 0.6:
                recommendations.append("åŒ¹é…æˆåŠŸç‡è‰¯å¥½ï¼Œç³»ç»Ÿè¿è¡Œæ­£å¸¸")
            
            if self.stats['failed_jobs'] > self.stats['processed_jobs'] * 0.1:
                recommendations.append("å¤±è´¥ç‡è¾ƒé«˜ï¼Œå»ºè®®æ£€æŸ¥æ•°æ®è´¨é‡å’Œé”™è¯¯å¤„ç†é€»è¾‘")
        
        if self.stats['new_matches'] > 0:
            recommendations.append("å»ºè®®å®šæœŸè¿è¡Œæ‰¹é‡é‡æ–°åŒ¹é…ä»¥ä¿æŒåŒ¹é…ç‡")
        else:
            recommendations.append("æœªäº§ç”Ÿæ–°åŒ¹é…ï¼Œå»ºè®®æ£€æŸ¥å‘é‡æ•°æ®åº“æ˜¯å¦æ­£ç¡®ç´¢å¼•äº†èŒä½æ•°æ®")
        
        # æ·»åŠ åŸºäºé…ç½®çš„å»ºè®®
        recommendations.append("å¦‚éœ€è¿›ä¸€æ­¥æé«˜åŒ¹é…ç‡ï¼Œå¯è€ƒè™‘:")
        recommendations.append("1. é™ä½åŒ¹é…é˜ˆå€¼åˆ°0.18-0.20")
        recommendations.append("2. è°ƒæ•´æƒé‡é…ç½®ï¼Œæé«˜è¯­ä¹‰ç›¸ä¼¼åº¦æƒé‡")
        recommendations.append("3. æ£€æŸ¥å‘é‡æ•°æ®åº“æ•°æ®è´¨é‡å’Œå®Œæ•´æ€§")
        recommendations.append("4. è¿è¡Œ python debug_matching_failure.py è¿›è¡Œè¯¦ç»†è¯Šæ–­")
        
        return recommendations


async def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='æ‰¹é‡é‡æ–°åŒ¹é…èŒä½')
    parser.add_argument('--limit', type=int, help='é™åˆ¶å¤„ç†çš„èŒä½æ•°é‡')
    parser.add_argument('--db-path', default='data/jobs.db', help='æ•°æ®åº“è·¯å¾„')
    parser.add_argument('--report-file', help='ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶')
    
    args = parser.parse_args()
    
    # åˆ›å»ºé‡æ–°åŒ¹é…å™¨
    rematcher = BatchRematcher(args.db_path)
    
    # è¿è¡Œæ‰¹é‡é‡æ–°åŒ¹é…
    report = await rematcher.run_batch_rematch(args.limit)
    
    # æ‰“å°æŠ¥å‘Š
    print("\n" + "="*60)
    print("ğŸ“Š æ‰¹é‡é‡æ–°åŒ¹é…æŠ¥å‘Š")
    print("="*60)
    print(json.dumps(report, indent=2, ensure_ascii=False))
    
    # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
    if args.report_file:
        with open(args.report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        print(f"\nğŸ“„ æŠ¥å‘Šå·²ä¿å­˜åˆ°: {args.report_file}")


if __name__ == "__main__":
    asyncio.run(main())