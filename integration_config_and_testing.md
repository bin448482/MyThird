# é›†æˆé…ç½®ç®¡ç†ä¸æµ‹è¯•æ¡†æ¶

## ğŸ“‹ ç»Ÿä¸€é…ç½®ç®¡ç†ç³»ç»Ÿ

### é…ç½®æ–‡ä»¶ç»“æ„

```yaml
# config/integration_config.yaml - ä¸»é›†æˆé…ç½®æ–‡ä»¶
integration_system:
  # ç³»ç»Ÿå…¨å±€é…ç½®
  global:
    system_name: "æ™ºèƒ½ç®€å†æŠ•é€’ç³»ç»Ÿ"
    version: "2.0.0"
    environment: "production"  # development, testing, production
    debug_mode: false
    log_level: "INFO"
    
  # ä¸»æ§åˆ¶å™¨é…ç½®
  master_controller:
    max_concurrent_pipelines: 5
    pipeline_timeout: 3600  # seconds
    checkpoint_interval: 100
    error_retry_attempts: 3
    enable_performance_monitoring: true
    enable_error_recovery: true
    
  # ä½œä¸šè°ƒåº¦å™¨é…ç½®
  job_scheduler:
    queue_size: 1000
    max_concurrent_jobs: 10
    batch_size: 50
    priority_levels: 3
    task_timeout: 1800
    retry_failed_tasks: true
    
  # æ•°æ®ä¼ é€’é…ç½®
  data_bridge:
    validation_enabled: true
    transformation_cache_enabled: true
    cache_ttl: 3600
    data_retention_days: 30
    backup_enabled: true
    
  # æ™ºèƒ½å†³ç­–é…ç½®
  decision_engine:
    enable_learning: true
    decision_model: "multi_criteria"
    weights:
      match_score: 0.35
      company_reputation: 0.20
      salary_attractiveness: 0.15
      location_preference: 0.10
      career_growth: 0.10
      competition_level: 0.10
    thresholds:
      submission_threshold: 0.70
      high_priority_threshold: 0.85
      low_priority_threshold: 0.55
      
  # è‡ªåŠ¨æŠ•é€’é…ç½®
  auto_submission:
    enabled: true
    max_submissions_per_day: 50
    submission_delay_min: 3
    submission_delay_max: 10
    retry_failed_submissions: true
    blacklist_companies: []
    preferred_companies: []

# æ¨¡å—é›†æˆé…ç½®
modules:
  job_extraction:
    enabled: true
    max_pages_per_site: 10
    extraction_timeout: 300
    concurrent_extractions: 3
    supported_sites: ["zhilian", "boss", "qiancheng"]
    
  rag_processing:
    enabled: true
    batch_size: 50
    llm_timeout: 30
    max_retry_attempts: 3
    fallback_mode_enabled: true
    
  resume_matching:
    enabled: true
    matching_threshold: 0.6
    max_matches_per_resume: 20
    enable_semantic_search: true
    cache_matching_results: true

# æ€§èƒ½ä¼˜åŒ–é…ç½®
performance:
  caching:
    enabled: true
    cache_type: "redis"  # memory, redis, file
    cache_size: 10000
    ttl_seconds: 3600
    
  concurrency:
    max_workers: 10
    semaphore_limit: 5
    use_thread_pool: true
    
  database:
    connection_pool_size: 20
    query_timeout: 30
    batch_insert_size: 100
    enable_query_cache: true

# ç›‘æ§é…ç½®
monitoring:
  enabled: true
  metrics_collection_interval: 30  # seconds
  real_time_dashboard: true
  alert_thresholds:
    error_rate: 0.1
    processing_speed: 100  # jobs/minute
    memory_usage: 0.8
    cpu_usage: 0.8
    disk_usage: 0.9
  
  logging:
    level: "INFO"
    file_rotation: "daily"
    max_file_size: "100MB"
    retention_days: 30
    structured_logging: true

# å®‰å…¨é…ç½®
security:
  api_rate_limiting: true
  request_timeout: 30
  max_request_size: "10MB"
  enable_cors: false
  allowed_origins: []
  
# å¤‡ä»½å’Œæ¢å¤é…ç½®
backup:
  enabled: true
  backup_interval: "daily"
  retention_days: 30
  backup_location: "./backups"
  compress_backups: true
```

### é…ç½®ç®¡ç†å™¨å®ç°è§„èŒƒ

```python
# src/integration/config_manager.py
class IntegrationConfigManager:
    """é›†æˆç³»ç»Ÿé…ç½®ç®¡ç†å™¨"""
    
    def __init__(self, config_path: str = "config/integration_config.yaml"):
        self.config_path = config_path
        self.config = {}
        self.watchers = []
        self.last_modified = None
        
    def load_config(self) -> Dict[str, Any]:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                self.config = yaml.safe_load(f)
            
            # éªŒè¯é…ç½®
            self._validate_config()
            
            # åº”ç”¨ç¯å¢ƒå˜é‡è¦†ç›–
            self._apply_env_overrides()
            
            # è®¾ç½®é»˜è®¤å€¼
            self._set_defaults()
            
            self.last_modified = os.path.getmtime(self.config_path)
            logger.info("é…ç½®åŠ è½½æˆåŠŸ")
            return self.config
            
        except Exception as e:
            logger.error(f"é…ç½®åŠ è½½å¤±è´¥: {e}")
            raise ConfigurationError(f"Failed to load config: {e}")
    
    def get_module_config(self, module_name: str) -> Dict[str, Any]:
        """è·å–ç‰¹å®šæ¨¡å—çš„é…ç½®"""
        return self.config.get('modules', {}).get(module_name, {})
    
    def get_integration_config(self, component: str) -> Dict[str, Any]:
        """è·å–é›†æˆç»„ä»¶é…ç½®"""
        return self.config.get('integration_system', {}).get(component, {})
    
    def watch_config_changes(self, callback):
        """ç›‘å¬é…ç½®æ–‡ä»¶å˜åŒ–"""
        self.watchers.append(callback)
        
    def _validate_config(self):
        """éªŒè¯é…ç½®å®Œæ•´æ€§"""
        required_sections = [
            'integration_system',
            'modules',
            'performance',
            'monitoring'
        ]
        
        for section in required_sections:
            if section not in self.config:
                raise ConfigurationError(f"Missing required config section: {section}")
    
    def _apply_env_overrides(self):
        """åº”ç”¨ç¯å¢ƒå˜é‡è¦†ç›–"""
        # æ”¯æŒé€šè¿‡ç¯å¢ƒå˜é‡è¦†ç›–é…ç½®
        env_mappings = {
            'INTEGRATION_LOG_LEVEL': 'integration_system.global.log_level',
            'INTEGRATION_DEBUG': 'integration_system.global.debug_mode',
            'MAX_CONCURRENT_JOBS': 'integration_system.job_scheduler.max_concurrent_jobs',
            'SUBMISSION_DAILY_LIMIT': 'auto_submission.max_submissions_per_day'
        }
        
        for env_var, config_path in env_mappings.items():
            if env_var in os.environ:
                self._set_nested_config(config_path, os.environ[env_var])
```

## ğŸ§ª é›†æˆæµ‹è¯•æ¡†æ¶

### æµ‹è¯•æ¶æ„è®¾è®¡

```python
# tests/integration/test_framework.py
class IntegrationTestFramework:
    """é›†æˆæµ‹è¯•æ¡†æ¶"""
    
    def __init__(self):
        self.test_config = self._load_test_config()
        self.test_data_manager = TestDataManager()
        self.mock_services = MockServiceManager()
        self.performance_monitor = PerformanceMonitor()
        
    async def run_full_integration_test(self):
        """è¿è¡Œå®Œæ•´é›†æˆæµ‹è¯•"""
        test_results = {
            'test_suite': 'full_integration',
            'start_time': datetime.now(),
            'tests': []
        }
        
        try:
            # 1. ç³»ç»Ÿåˆå§‹åŒ–æµ‹è¯•
            init_result = await self.test_system_initialization()
            test_results['tests'].append(init_result)
            
            # 2. ç«¯åˆ°ç«¯æµç¨‹æµ‹è¯•
            e2e_result = await self.test_end_to_end_pipeline()
            test_results['tests'].append(e2e_result)
            
            # 3. é”™è¯¯å¤„ç†æµ‹è¯•
            error_result = await self.test_error_handling()
            test_results['tests'].append(error_result)
            
            # 4. æ€§èƒ½æµ‹è¯•
            perf_result = await self.test_performance()
            test_results['tests'].append(perf_result)
            
            # 5. å¹¶å‘æµ‹è¯•
            concurrent_result = await self.test_concurrent_processing()
            test_results['tests'].append(concurrent_result)
            
            test_results['end_time'] = datetime.now()
            test_results['total_duration'] = (test_results['end_time'] - test_results['start_time']).total_seconds()
            test_results['success_rate'] = self._calculate_success_rate(test_results['tests'])
            
            return test_results
            
        except Exception as e:
            logger.error(f"é›†æˆæµ‹è¯•å¤±è´¥: {e}")
            test_results['error'] = str(e)
            return test_results
```

### ç«¯åˆ°ç«¯æµ‹è¯•ç”¨ä¾‹

```python
class EndToEndTestSuite:
    """ç«¯åˆ°ç«¯æµ‹è¯•å¥—ä»¶"""
    
    async def test_complete_pipeline_flow(self):
        """æµ‹è¯•å®Œæ•´æµæ°´çº¿æµç¨‹"""
        # å‡†å¤‡æµ‹è¯•æ•°æ®
        test_resume = self.test_data_manager.create_test_resume()
        test_config = self.test_data_manager.create_test_pipeline_config()
        
        # åˆå§‹åŒ–ç³»ç»Ÿ
        master_controller = MasterController(self.test_config)
        await master_controller.initialize_system()
        
        # æ‰§è¡Œå®Œæ•´æµç¨‹
        start_time = time.time()
        result = await master_controller.run_full_pipeline(test_resume, test_config)
        execution_time = time.time() - start_time
        
        # éªŒè¯ç»“æœ
        assert result.success == True
        assert result.extraction_result['success'] == True
        assert result.rag_result['success'] == True
        assert result.matching_result['success'] == True
        assert result.submission_result['total_attempts'] > 0
        assert execution_time < 1800  # 30åˆ†é’Ÿå†…å®Œæˆ
        
        return {
            'test_name': 'complete_pipeline_flow',
            'success': True,
            'execution_time': execution_time,
            'jobs_processed': result.total_jobs_processed,
            'submissions_made': result.submission_result.get('successful_submissions', 0)
        }
    
    async def test_pipeline_with_failures(self):
        """æµ‹è¯•åŒ…å«å¤±è´¥åœºæ™¯çš„æµæ°´çº¿"""
        # æ¨¡æ‹Ÿå„ç§å¤±è´¥åœºæ™¯
        failure_scenarios = [
            'extraction_partial_failure',
            'rag_processing_timeout',
            'matching_low_scores',
            'submission_rate_limit'
        ]
        
        results = []
        for scenario in failure_scenarios:
            result = await self._test_failure_scenario(scenario)
            results.append(result)
        
        return {
            'test_name': 'pipeline_with_failures',
            'scenarios_tested': len(failure_scenarios),
            'results': results,
            'recovery_rate': sum(1 for r in results if r['recovered']) / len(results)
        }
```

### æ€§èƒ½æµ‹è¯•è§„èŒƒ

```python
class PerformanceTestSuite:
    """æ€§èƒ½æµ‹è¯•å¥—ä»¶"""
    
    async def test_throughput_performance(self):
        """æµ‹è¯•ååé‡æ€§èƒ½"""
        test_loads = [10, 50, 100, 200, 500]
        results = []
        
        for load in test_loads:
            start_time = time.time()
            
            # åˆ›å»ºæµ‹è¯•ä»»åŠ¡
            tasks = []
            for i in range(load):
                task = self._create_performance_test_task(f"job_{i}")
                tasks.append(task)
            
            # å¹¶å‘æ‰§è¡Œ
            completed_tasks = await asyncio.gather(*tasks, return_exceptions=True)
            
            end_time = time.time()
            execution_time = end_time - start_time
            
            # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
            successful_tasks = len([t for t in completed_tasks if not isinstance(t, Exception)])
            throughput = successful_tasks / execution_time
            
            results.append({
                'load': load,
                'execution_time': execution_time,
                'successful_tasks': successful_tasks,
                'throughput': throughput,
                'error_rate': (load - successful_tasks) / load
            })
        
        return {
            'test_name': 'throughput_performance',
            'results': results,
            'max_throughput': max(r['throughput'] for r in results),
            'optimal_load': self._find_optimal_load(results)
        }
    
    async def test_memory_usage(self):
        """æµ‹è¯•å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        import psutil
        
        process = psutil.Process()
        initial_memory = process.memory_info().rss / 1024 / 1024  # MB
        
        # æ‰§è¡Œå¤§è´Ÿè½½æµ‹è¯•
        large_dataset = self.test_data_manager.create_large_test_dataset(1000)
        
        memory_samples = []
        for i in range(0, len(large_dataset), 100):
            batch = large_dataset[i:i+100]
            await self._process_test_batch(batch)
            
            current_memory = process.memory_info().rss / 1024 / 1024
            memory_samples.append({
                'processed_items': i + len(batch),
                'memory_usage': current_memory,
                'memory_increase': current_memory - initial_memory
            })
        
        return {
            'test_name': 'memory_usage',
            'initial_memory': initial_memory,
            'peak_memory': max(s['memory_usage'] for s in memory_samples),
            'memory_growth_rate': self._calculate_memory_growth_rate(memory_samples),
            'samples': memory_samples
        }
```

### é”™è¯¯å¤„ç†æµ‹è¯•

```python
class ErrorHandlingTestSuite:
    """é”™è¯¯å¤„ç†æµ‹è¯•å¥—ä»¶"""
    
    async def test_network_failures(self):
        """æµ‹è¯•ç½‘ç»œæ•…éšœå¤„ç†"""
        failure_types = [
            'connection_timeout',
            'dns_resolution_failure',
            'http_500_error',
            'rate_limit_exceeded'
        ]
        
        results = []
        for failure_type in failure_types:
            # æ¨¡æ‹Ÿç½‘ç»œæ•…éšœ
            with self.mock_services.simulate_network_failure(failure_type):
                result = await self._test_network_resilience()
                results.append({
                    'failure_type': failure_type,
                    'recovery_successful': result['recovered'],
                    'recovery_time': result['recovery_time'],
                    'data_integrity': result['data_integrity_maintained']
                })
        
        return {
            'test_name': 'network_failures',
            'results': results,
            'overall_resilience': sum(r['recovery_successful'] for r in results) / len(results)
        }
    
    async def test_data_corruption_recovery(self):
        """æµ‹è¯•æ•°æ®æŸåæ¢å¤"""
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        test_data = self.test_data_manager.create_test_pipeline_data()
        
        # æ¨¡æ‹Ÿæ•°æ®æŸååœºæ™¯
        corruption_scenarios = [
            'partial_database_corruption',
            'vector_db_index_corruption',
            'config_file_corruption',
            'checkpoint_file_corruption'
        ]
        
        recovery_results = []
        for scenario in corruption_scenarios:
            # å¤‡ä»½åŸå§‹æ•°æ®
            backup = self._create_data_backup(test_data)
            
            # æ¨¡æ‹ŸæŸå
            self._simulate_data_corruption(scenario, test_data)
            
            # æµ‹è¯•æ¢å¤
            recovery_start = time.time()
            recovery_success = await self._attempt_data_recovery(scenario)
            recovery_time = time.time() - recovery_start
            
            # éªŒè¯æ•°æ®å®Œæ•´æ€§
            integrity_check = self._verify_data_integrity(test_data, backup)
            
            recovery_results.append({
                'scenario': scenario,
                'recovery_successful': recovery_success,
                'recovery_time': recovery_time,
                'data_integrity_restored': integrity_check
            })
            
            # æ¢å¤åŸå§‹æ•°æ®
            self._restore_from_backup(backup)
        
        return {
            'test_name': 'data_corruption_recovery',
            'scenarios_tested': len(corruption_scenarios),
            'results': recovery_results,
            'recovery_success_rate': sum(r['recovery_successful'] for r in recovery_results) / len(recovery_results)
        }
```

### æµ‹è¯•æ•°æ®ç®¡ç†

```python
class TestDataManager:
    """æµ‹è¯•æ•°æ®ç®¡ç†å™¨"""
    
    def __init__(self):
        self.test_data_dir = Path("tests/data")
        self.test_data_dir.mkdir(exist_ok=True)
    
    def create_test_resume(self) -> GenericResumeProfile:
        """åˆ›å»ºæµ‹è¯•ç®€å†"""
        return GenericResumeProfile(
            name="æµ‹è¯•ç”¨æˆ·",
            phone="13800138000",
            email="test@example.com",
            location="åŒ—äº¬",
            total_experience_years=5,
            current_position="é«˜çº§Pythonå·¥ç¨‹å¸ˆ",
            skill_categories=[
                SkillCategory(
                    category_name="ç¼–ç¨‹è¯­è¨€",
                    skills=["Python", "Java", "JavaScript"],
                    proficiency_level="advanced",
                    years_experience=5
                ),
                SkillCategory(
                    category_name="æ¡†æ¶æŠ€æœ¯",
                    skills=["Django", "Flask", "React"],
                    proficiency_level="intermediate",
                    years_experience=3
                )
            ]
        )
    
    def create_test_pipeline_config(self) -> PipelineConfig:
        """åˆ›å»ºæµ‹è¯•æµæ°´çº¿é…ç½®"""
        return PipelineConfig(
            search_keywords=["Pythonå·¥ç¨‹å¸ˆ", "åç«¯å¼€å‘"],
            search_locations=["åŒ—äº¬", "ä¸Šæµ·"],
            max_pages=2,
            max_jobs_per_keyword=20,
            rag_batch_size=10,
            matching_threshold=0.5,
            auto_submit_threshold=0.7,
            max_submissions_per_day=5
        )
    
    def create_large_test_dataset(self, size: int) -> List[Dict]:
        """åˆ›å»ºå¤§å‹æµ‹è¯•æ•°æ®é›†"""
        dataset = []
        for i in range(size):
            job_data = {
                'job_id': f'test_job_{i}',
                'title': f'æµ‹è¯•èŒä½_{i}',
                'company': f'æµ‹è¯•å…¬å¸_{i % 100}',
                'location': ['åŒ—äº¬', 'ä¸Šæµ·', 'æ·±åœ³', 'æ­å·'][i % 4],
                'description': f'è¿™æ˜¯æµ‹è¯•èŒä½{i}çš„æè¿°ä¿¡æ¯...',
                'requirements': f'æµ‹è¯•èŒä½{i}çš„è¦æ±‚...',
                'salary': f'{15 + (i % 20)}-{25 + (i % 20)}K'
            }
            dataset.append(job_data)
        
        return dataset
    
    def save_test_results(self, test_name: str, results: Dict):
        """ä¿å­˜æµ‹è¯•ç»“æœ"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f"{test_name}_{timestamp}.json"
        filepath = self.test_data_dir / "results" / filename
        
        filepath.parent.mkdir(exist_ok=True)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2, default=str)
        
        logger.info(f"æµ‹è¯•ç»“æœå·²ä¿å­˜: {filepath}")
```

### æµ‹è¯•æ‰§è¡Œè„šæœ¬

```python
# tests/run_integration_tests.py
async def main():
    """ä¸»æµ‹è¯•æ‰§è¡Œå‡½æ•°"""
    # åˆå§‹åŒ–æµ‹è¯•æ¡†æ¶
    test_framework = IntegrationTestFramework()
    
    # è®¾ç½®æµ‹è¯•ç¯å¢ƒ
    await test_framework.setup_test_environment()
    
    try:
        # è¿è¡Œå®Œæ•´é›†æˆæµ‹è¯•
        logger.info("å¼€å§‹è¿è¡Œé›†æˆæµ‹è¯•...")
        results = await test_framework.run_full_integration_test()
        
        # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
        report = TestReportGenerator().generate_html_report(results)
        
        # ä¿å­˜ç»“æœ
        test_framework.test_data_manager.save_test_results("integration_test", results)
        
        # è¾“å‡ºæ‘˜è¦
        print("\n" + "="*50)
        print("é›†æˆæµ‹è¯•å®Œæˆ")
        print("="*50)
        print(f"æ€»æµ‹è¯•æ•°: {len(results['tests'])}")
        print(f"æˆåŠŸç‡: {results['success_rate']:.2%}")
        print(f"æ€»è€—æ—¶: {results['total_duration']:.2f}ç§’")
        print(f"æŠ¥å‘Šæ–‡ä»¶: {report['file_path']}")
        print("="*50)
        
        # æ ¹æ®æµ‹è¯•ç»“æœè®¾ç½®é€€å‡ºç 
        exit_code = 0 if results['success_rate'] >= 0.9 else 1
        sys.exit(exit_code)
        
    except Exception as e:
        logger.error(f"æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
        sys.exit(1)
    
    finally:
        # æ¸…ç†æµ‹è¯•ç¯å¢ƒ
        await test_framework.cleanup_test_environment()

if __name__ == "__main__":
    asyncio.run(main())
```

### æŒç»­é›†æˆé…ç½®

```yaml
# .github/workflows/integration_test.yml
name: Integration Tests

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

jobs:
  integration-test:
    runs-on: ubuntu-latest
    
    services:
      redis:
        image: redis:6
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-test.txt
    
    - name: Setup test environment
      run: |
        mkdir -p data/test
        mkdir -p logs/test
        cp config/test_config.yaml config/config.yaml
    
    - name: Run integration tests
      run: |
        python tests/run_integration_tests.py
      env:
        INTEGRATION_ENV: testing
        REDIS_URL: redis://localhost:6379
    
    - name: Upload test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: integration-test-results
        path: tests/data/results/
    
    - name: Upload test reports
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: integration-test-reports
        path: tests/reports/
```

## ğŸ“Š æµ‹è¯•æŠ¥å‘Šæ¨¡æ¿

### HTMLæµ‹è¯•æŠ¥å‘Š

```html
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <title>é›†æˆæµ‹è¯•æŠ¥å‘Š</title>
    <style>
        body { font-family: 'Microsoft YaHei', Arial, sans-serif; margin: 20px; }
        .header { text-align: center; margin-bottom: 30px; }
        .summary { background: #f8f9fa; padding: 20px; border-radius: 8px; margin-bottom: 30px; }
        .test-result { border: 1px solid #ddd; margin-bottom: 20px; padding: 15px; border-radius: 5px; }
        .success { border-left: 5px solid #28a745; }
        .failure { border-left: 5px solid #dc3545; }
        .metrics { display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 15px; }
        .metric { background: white; padding: 15px; border-radius: 5px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); }
    </style>
</head>
<body>
    <div class="header">
        <h1>æ™ºèƒ½ç®€å†æŠ•é€’ç³»ç»Ÿ - é›†æˆæµ‹è¯•æŠ¥å‘Š</h1>
        <p>ç”Ÿæˆæ—¶é—´: {{timestamp}}</p>
    </div>
    
    <div class="summary">
        <h2>æµ‹è¯•æ‘˜è¦</h2>
        <div class="metrics">
            <div class="metric">
                <h3>æ€»æµ‹è¯•æ•°</h3>
                <p>{{total_tests}}</p>
            </div>
            <div class="metric">
                <h3>æˆåŠŸç‡</h3>
                <p>{{success_rate}}%</p>
            </div>
            <div class="metric">
                <h3>æ€»è€—æ—¶</h3>
                <p>{{total_duration}}ç§’</p>
            </div>
            <div class="metric">
                <h3>å¹³å‡å“åº”æ—¶é—´</h3>
                <p>{{avg_response_time}}ms</p>
            </div>
        </div>
    </div>
    
    <div class="test-results">
        <h2>è¯¦ç»†æµ‹è¯•ç»“æœ</h2>
        {{#each test_results}}
        <div class="test-result {{#if success}}success{{else}}failure{{/if}}">
            <h3>{{test_name}}</h3>
            <p><strong>çŠ¶æ€:</strong> {{#if success}}é€šè¿‡{{else}}å¤±è´¥{{/if}}</p>
            <p><strong>æ‰§è¡Œæ—¶é—´:</strong> {{execution_time}}ç§’</p>
            {{#if error}}
            <p><strong>é”™è¯¯ä¿¡æ¯:</strong> {{error}}</p>
            {{/if}}
            <p><strong>è¯¦ç»†ä¿¡æ¯:</strong> {{details}}</p>
        </div>
        {{/each}}
    </div>
</body>
</html>
```

è¿™ä¸ªé…ç½®ç®¡ç†å’Œæµ‹è¯•æ¡†æ¶æä¾›äº†å®Œæ•´çš„é›†æˆæµ‹è¯•è§£å†³æ–¹æ¡ˆï¼Œç¡®ä¿ç³»ç»Ÿçš„å¯é æ€§å’Œæ€§èƒ½ã€‚