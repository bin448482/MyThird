# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

è¿™æ˜¯ä¸€ä¸ªåŸºäºPythonçš„æ™ºèƒ½ç®€å†æŠ•é€’ç³»ç»Ÿï¼Œé›†æˆäº†LangChain RAGæŠ€æœ¯è¿›è¡ŒèŒä½ä¿¡æ¯æ™ºèƒ½åˆ†æã€‚ç³»ç»Ÿæ”¯æŒæ™ºè”æ‹›è˜ã€å‰ç¨‹æ— å¿§ã€Bossç›´è˜ç­‰ä¸»æµæ‹›è˜ç½‘ç«™ï¼Œä½¿ç”¨Seleniumè¿›è¡Œç½‘é¡µè‡ªåŠ¨åŒ–ï¼Œé‡‡ç”¨äººå·¥ç™»å½•åè‡ªåŠ¨åŒ–æ“ä½œçš„æ–¹å¼ã€‚æ ¸å¿ƒç‰¹è‰²æ˜¯åŸºäºLangChainçš„RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰å¼•æ“ï¼Œèƒ½å¤Ÿå¯¹èŒä½ä¿¡æ¯è¿›è¡Œæ·±åº¦ç»“æ„åŒ–åˆ†æã€å‘é‡åŒ–å­˜å‚¨å’Œæ™ºèƒ½åŒ¹é…ï¼Œå¤§å¹…æå‡ç®€å†æŠ•é€’çš„ç²¾å‡†åº¦å’Œæ•ˆç‡ã€‚

## Technology Stack

- **æ ¸å¿ƒè¯­è¨€**: Python 3.8+
- **ç½‘é¡µè‡ªåŠ¨åŒ–**: Selenium WebDriver
- **AIåˆ†æ**: LangChain + OpenAI/æœ¬åœ°LLM
- **RAGå¼•æ“**: LangChain RAG + ChromaDBå‘é‡æ•°æ®åº“
- **å‘é‡åµŒå…¥**: sentence-transformers (å¤šè¯­è¨€æ”¯æŒ)
- **æ–‡æ¡£å¤„ç†**: LangChain TextSplitter + Document Loaders
- **æ•°æ®åº“**: SQLite (ç»“æ„åŒ–æ•°æ®) + ChromaDB (å‘é‡æ•°æ®)
- **é…ç½®ç®¡ç†**: YAML/JSON
- **å‘½ä»¤è¡Œç•Œé¢**: Click/argparse
- **æ—¥å¿—**: Python logging
- **æµ‹è¯•**: pytest

## Getting Started

When setting up this project:

1. Initialize version control: `git init`
2. Install Python dependencies: `pip install -r requirements.txt`
3. Configure settings in `config/config.yaml`
4. Run the tool: `python src/main.py --website zhilian`

## Development Commands

```bash
# å®‰è£…ä¾èµ–
pip install -r requirements.txt

# è¿è¡Œå·¥å…·
python src/main.py --website zhilian

# è¿è¡Œæµ‹è¯•
pytest tests/

# ç”Ÿæˆéœ€æ±‚æ–‡ä»¶
pip freeze > requirements.txt
```

## Architecture

### ç³»ç»Ÿæ¶æ„å›¾

```mermaid
graph TB
    CLI[å‘½ä»¤è¡Œç•Œé¢] --> Core[æ ¸å¿ƒæ§åˆ¶å™¨]
    Config[é…ç½®ç®¡ç†] --> Core
    
    Core --> Crawler[çˆ¬è™«å¼•æ“]
    Core --> RAGAnalyzer[RAGæ™ºèƒ½åˆ†æå™¨]
    Core --> Matcher[æ™ºèƒ½åŒ¹é…å¼•æ“]
    Core --> Submitter[æŠ•é€’å¼•æ“]
    
    Crawler --> WebDriver[Selenium WebDriver]
    Crawler --> AntiBot[é˜²åçˆ¬æœºåˆ¶]
    Submitter --> WebDriver
    
    RAGAnalyzer --> JobProcessor[LangChainèŒä½å¤„ç†å™¨]
    JobProcessor --> TextSplitter[æ™ºèƒ½æ–‡æœ¬åˆ†å‰²å™¨]
    JobProcessor --> StructureExtractor[ç»“æ„åŒ–æå–é“¾]
    JobProcessor --> DocumentCreator[æ–‡æ¡£åˆ›å»ºå™¨]
    
    RAGAnalyzer --> VectorStore[ChromaDBå‘é‡å­˜å‚¨]
    VectorStore --> Embeddings[å¤šè¯­è¨€åµŒå…¥æ¨¡å‹]
    VectorStore --> Retriever[å‹ç¼©æ£€ç´¢å™¨]
    
    RAGAnalyzer --> RAGChain[æ£€ç´¢é—®ç­”é“¾]
    RAGChain --> LLM[å¤§è¯­è¨€æ¨¡å‹]
    RAGChain --> QAPrompts[é—®ç­”æç¤ºè¯æ¨¡æ¿]
    
    Matcher --> Resume[ç®€å†æ•°æ®]
    Matcher --> VectorStore
    Matcher --> SemanticSearch[è¯­ä¹‰ç›¸ä¼¼åº¦æœç´¢]
    
    Core --> Database[SQLiteæ•°æ®åº“]
    Core --> VectorStore
    Core --> Logger[æ—¥å¿—ç³»ç»Ÿ]
    
    Adapters[ç½‘ç«™é€‚é…å™¨] --> Crawler
    Adapters --> Submitter
    
    HumanLogin[äººå·¥ç™»å½•] --> WebDriver
    
    subgraph "RAGæ ¸å¿ƒç»„ä»¶"
        JobProcessor
        VectorStore
        RAGChain
        SemanticSearch
    end
```

### æ¨¡å—æ¶æ„

#### 1. æ ¸å¿ƒæ§åˆ¶å™¨ (Core Controller)
- **èŒè´£**: åè°ƒå„ä¸ªæ¨¡å—ï¼Œæ§åˆ¶æ•´ä½“æµç¨‹
- **ä¸»è¦åŠŸèƒ½**: åˆå§‹åŒ–ç»„ä»¶ã€æ§åˆ¶çˆ¬å–æµç¨‹ã€å¼‚å¸¸å¤„ç†ã€çŠ¶æ€ç®¡ç†

#### 2. ç½‘ç«™é€‚é…å™¨ (Website Adapters)
- **èŒè´£**: ä¸ºä¸åŒæ‹›è˜ç½‘ç«™æä¾›ç»Ÿä¸€æ¥å£
- **è®¾è®¡æ¨¡å¼**: ç­–ç•¥æ¨¡å¼ + å·¥å‚æ¨¡å¼
- **æ”¯æŒç½‘ç«™**: æ™ºè”æ‹›è˜ã€å‰ç¨‹æ— å¿§ã€Bossç›´è˜

#### 3. çˆ¬è™«å¼•æ“ (Crawler Engine)
- **èŒè´£**: åŸºäºSeleniumæ‰§è¡Œç½‘é¡µè‡ªåŠ¨åŒ–æ“ä½œï¼ŒåŒ…å«ä¼šè¯ç®¡ç†
- **ä¸»è¦åŠŸèƒ½**: å¯åŠ¨æµè§ˆå™¨ã€ç­‰å¾…äººå·¥ç™»å½•ã€æ£€æµ‹ç™»å½•çŠ¶æ€ã€é¡µé¢å¯¼èˆªã€æ•°æ®æå–

#### 4. RAGæ™ºèƒ½åˆ†æå™¨ (RAG Analyzer)
- **èŒè´£**: åŸºäºLangChain RAGæŠ€æœ¯è¿›è¡ŒèŒä½ä¿¡æ¯æ·±åº¦åˆ†æ
- **æ ¸å¿ƒç»„ä»¶**:
  - **LangChainèŒä½å¤„ç†å™¨**: ä½¿ç”¨LLMè¿›è¡Œç»“æ„åŒ–æå–
  - **æ™ºèƒ½æ–‡æœ¬åˆ†å‰²å™¨**: è¯­ä¹‰çº§åˆ«çš„æ–‡æœ¬åˆ†å‰²
  - **å‘é‡åµŒå…¥å¼•æ“**: å¤šè¯­è¨€èŒä½ä¿¡æ¯å‘é‡åŒ–
  - **ChromaDBå­˜å‚¨**: é«˜æ•ˆçš„å‘é‡æ•°æ®åº“å­˜å‚¨
- **ä¸»è¦åŠŸèƒ½**:
  - èŒä½æè¿°æ™ºèƒ½ç»“æ„åŒ–ï¼ˆèŒè´£ã€è¦æ±‚ã€æŠ€èƒ½åˆ†ç¦»ï¼‰
  - è¯­ä¹‰çº§æŠ€èƒ½æ ‡ç­¾æå–å’Œåˆ†ç±»
  - è–ªèµ„èŒƒå›´æ™ºèƒ½è§£æ
  - èŒä½ä¿¡æ¯å‘é‡åŒ–å­˜å‚¨
  - åŸºäºè¯­ä¹‰çš„ç›¸ä¼¼èŒä½æ£€ç´¢

#### 5. æ™ºèƒ½åŒ¹é…å¼•æ“ (Smart Matching Engine)
- **èŒè´£**: åŸºäºRAGæŠ€æœ¯è¿›è¡Œç®€å†ä¸èŒä½çš„æ™ºèƒ½åŒ¹é…
- **æ ¸å¿ƒæŠ€æœ¯**:
  - **è¯­ä¹‰ç›¸ä¼¼åº¦åŒ¹é…**: åŸºäºå‘é‡åµŒå…¥çš„æ·±åº¦è¯­ä¹‰ç†è§£
  - **RAGæ£€ç´¢å¢å¼º**: åˆ©ç”¨å†å²åŒ¹é…æ•°æ®ä¼˜åŒ–åŒ¹é…ç®—æ³•
  - **å¤šç»´åº¦è¯„åˆ†**: ç»¼åˆæŠ€èƒ½ã€ç»éªŒã€è–ªèµ„ç­‰å¤šä¸ªç»´åº¦
- **åŒ¹é…ç»´åº¦**:
  - æŠ€èƒ½è¯­ä¹‰åŒ¹é…(50%): åŸºäºå‘é‡ç›¸ä¼¼åº¦çš„æŠ€èƒ½åŒ¹é…
  - å·¥ä½œç»éªŒåŒ¹é…(30%): ç»éªŒå¹´é™å’Œé¡¹ç›®ç»å†åŒ¹é…
  - è–ªèµ„èŒƒå›´åŒ¹é…(20%): æœŸæœ›è–ªèµ„ä¸èŒä½è–ªèµ„çš„åŒ¹é…åº¦
- **å¢å¼ºåŠŸèƒ½**:
  - èŒä½æ¨è: åŸºäºç”¨æˆ·ç”»åƒæ¨èç›¸ä¼¼èŒä½
  - åŒ¹é…è§£é‡Š: æä¾›è¯¦ç»†çš„åŒ¹é…åŸå› åˆ†æ
  - å­¦ä¹ ä¼˜åŒ–: æ ¹æ®æŠ•é€’åé¦ˆæŒç»­ä¼˜åŒ–åŒ¹é…ç®—æ³•

#### 6. æŠ•é€’å¼•æ“ (Submission Engine)
- **èŒè´£**: æ‰§è¡Œç®€å†æŠ•é€’æ“ä½œ
- **ä¸»è¦åŠŸèƒ½**: å®šä½æŠ•é€’æŒ‰é’®ã€æ¨¡æ‹Ÿç‚¹å‡»æŠ•é€’ã€çŠ¶æ€ç¡®è®¤

### é¡¹ç›®ç›®å½•ç»“æ„

```
resume_auto_submitter/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.py                 # ä¸»å…¥å£
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ controller.py       # æ ¸å¿ƒæ§åˆ¶å™¨
â”‚   â”‚   â”œâ”€â”€ config.py          # é…ç½®ç®¡ç†
â”‚   â”‚   â””â”€â”€ exceptions.py      # è‡ªå®šä¹‰å¼‚å¸¸
â”‚   â”œâ”€â”€ adapters/
â”‚   â”‚   â”œâ”€â”€ base.py            # åŸºç¡€é€‚é…å™¨
â”‚   â”‚   â”œâ”€â”€ zhilian.py         # æ™ºè”æ‹›è˜é€‚é…å™¨
â”‚   â”‚   â”œâ”€â”€ qiancheng.py       # å‰ç¨‹æ— å¿§é€‚é…å™¨
â”‚   â”‚   â””â”€â”€ boss.py            # Bossç›´è˜é€‚é…å™¨
â”‚   â”œâ”€â”€ crawler/
â”‚   â”‚   â”œâ”€â”€ engine.py          # çˆ¬è™«å¼•æ“
â”‚   â”‚   â”œâ”€â”€ anti_bot.py        # é˜²åçˆ¬æœºåˆ¶
â”‚   â”‚   â””â”€â”€ selenium_utils.py   # Seleniumå·¥å…·å‡½æ•°
â”‚   â”œâ”€â”€ rag/                   # RAGæ™ºèƒ½åˆ†ææ¨¡å—
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ job_processor.py   # LangChainèŒä½å¤„ç†å™¨
â”‚   â”‚   â”œâ”€â”€ vector_manager.py  # ChromaDBå‘é‡å­˜å‚¨ç®¡ç†
â”‚   â”‚   â”œâ”€â”€ rag_chain.py       # RAGæ£€ç´¢é—®ç­”é“¾
â”‚   â”‚   â”œâ”€â”€ document_creator.py # æ–‡æ¡£åˆ›å»ºå™¨
â”‚   â”‚   â””â”€â”€ semantic_search.py # è¯­ä¹‰æœç´¢å¼•æ“
â”‚   â”œâ”€â”€ analyzer/
â”‚   â”‚   â”œâ”€â”€ rag_analyzer.py    # RAGæ™ºèƒ½åˆ†æå™¨
â”‚   â”‚   â”œâ”€â”€ prompts.py         # LangChainæç¤ºè¯æ¨¡æ¿
â”‚   â”‚   â””â”€â”€ llm_client.py      # LLMå®¢æˆ·ç«¯
â”‚   â”œâ”€â”€ matcher/
â”‚   â”‚   â”œâ”€â”€ smart_matching.py  # æ™ºèƒ½åŒ¹é…å¼•æ“
â”‚   â”‚   â”œâ”€â”€ semantic_scorer.py # è¯­ä¹‰è¯„åˆ†ç®—æ³•
â”‚   â”‚   â””â”€â”€ recommendation.py  # èŒä½æ¨èå¼•æ“
â”‚   â”œâ”€â”€ submitter/
â”‚   â”‚   â””â”€â”€ submission_engine.py # æŠ•é€’å¼•æ“
â”‚   â”œâ”€â”€ database/
â”‚   â”‚   â”œâ”€â”€ models.py          # æ•°æ®æ¨¡å‹
â”‚   â”‚   â”œâ”€â”€ operations.py      # æ•°æ®åº“æ“ä½œ
â”‚   â”‚   â””â”€â”€ vector_ops.py      # å‘é‡æ•°æ®åº“æ“ä½œ
â”‚   â”œâ”€â”€ cli/
â”‚   â”‚   â”œâ”€â”€ commands.py        # å‘½ä»¤è¡Œå‘½ä»¤
â”‚   â”‚   â””â”€â”€ utils.py           # CLIå·¥å…·å‡½æ•°
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ logger.py          # æ—¥å¿—å·¥å…·
## ğŸš€ LangChain RAGæ™ºèƒ½åˆ†æç³»ç»Ÿ

### RAGç³»ç»Ÿæ¶æ„è®¾è®¡

åŸºäºLangChainçš„RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰ç³»ç»Ÿæ˜¯æœ¬é¡¹ç›®çš„æ ¸å¿ƒæ™ºèƒ½å¼•æ“ï¼Œè´Ÿè´£èŒä½ä¿¡æ¯çš„æ·±åº¦åˆ†æã€å‘é‡åŒ–å­˜å‚¨å’Œæ™ºèƒ½åŒ¹é…ã€‚

```mermaid
graph TD
    A[èŒä½JSONæ•°æ®] --> B[LangChainæ–‡æ¡£åŠ è½½å™¨]
    B --> C[LLMè¯­ä¹‰åˆ†å‰²å™¨]
    C --> D[ç»“æ„åŒ–æå–é“¾]
    D --> E[å‘é‡åµŒå…¥]
    E --> F[ChromaDBå­˜å‚¨]
    
    G[ç”¨æˆ·æŸ¥è¯¢] --> H[æ£€ç´¢é“¾]
    H --> I[é‡æ’åº]
    I --> J[ç”Ÿæˆå›ç­”]
    
    F --> H
    
    subgraph "LangChainç»„ä»¶"
        K[TextSplitter]
        L[LLMChain]
        M[VectorStore]
        N[RetrievalQA]
    end
```

### æ ¸å¿ƒç»„ä»¶å®ç°

#### 1. LangChainèŒä½å¤„ç†å™¨ (JobProcessor)

```python
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document
from langchain.llms import OpenAI
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
from langchain.output_parsers import PydanticOutputParser
from pydantic import BaseModel, Field
from typing import List, Dict

class JobStructure(BaseModel):
    """èŒä½ç»“æ„åŒ–æ•°æ®æ¨¡å‹"""
    job_title: str = Field(description="èŒä½åç§°")
    company: str = Field(description="å…¬å¸åç§°")
    responsibilities: List[str] = Field(description="å²—ä½èŒè´£åˆ—è¡¨")
    requirements: List[str] = Field(description="äººå‘˜è¦æ±‚åˆ—è¡¨")
    skills: List[str] = Field(description="æŠ€èƒ½è¦æ±‚åˆ—è¡¨")
    education: str = Field(description="å­¦å†è¦æ±‚")
    experience: str = Field(description="ç»éªŒè¦æ±‚")

class LangChainJobProcessor:
    """åŸºäºLangChainçš„èŒä½æ•°æ®å¤„ç†å™¨"""
    
    def __init__(self, llm_model="gpt-3.5-turbo"):
        self.llm = OpenAI(model_name=llm_model, temperature=0.1)
        self.output_parser = PydanticOutputParser(pydantic_object=JobStructure)
        self.extraction_chain = self._build_extraction_chain()
        self.semantic_splitter = self._build_semantic_splitter()
    
    def _build_extraction_chain(self) -> LLMChain:
        """æ„å»ºç»“æ„åŒ–æå–é“¾"""
        prompt_template = """
ä½ æ˜¯ä¸“ä¸šçš„HRæ•°æ®åˆ†æå¸ˆã€‚è¯·åˆ†æä»¥ä¸‹èŒä½æè¿°ï¼Œå°†å…¶ç»“æ„åŒ–æå–ã€‚

èŒä½æ–‡æœ¬ï¼š
{job_text}

æå–è¦æ±‚ï¼š
1. å‡†ç¡®åˆ†ç¦»å²—ä½èŒè´£å’Œäººå‘˜è¦æ±‚
2. æå–æ‰€æœ‰æŠ€èƒ½å…³é”®è¯
3. è¯†åˆ«å­¦å†å’Œç»éªŒè¦æ±‚
4. ç¡®ä¿ä¿¡æ¯å®Œæ•´ä¸”ä¸é‡å¤

{format_instructions}

ç»“æ„åŒ–è¾“å‡ºï¼š
"""
        
        prompt = PromptTemplate(
            template=prompt_template,
            input_variables=["job_text"],
            partial_variables={"format_instructions": self.output_parser.get_format_instructions()}
        )
        
        return LLMChain(llm=self.llm, prompt=prompt, output_parser=self.output_parser)
```

#### 2. ChromaDBå‘é‡å­˜å‚¨ç®¡ç†å™¨

```python
from langchain.vectorstores import Chroma
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import LLMChainExtractor

class ChromaDBManager:
    """ChromaDBå‘é‡å­˜å‚¨ç®¡ç†å™¨"""
    
    def __init__(self, persist_directory="./chroma_db"):
        # åˆå§‹åŒ–ä¸­æ–‡åµŒå…¥æ¨¡å‹
        self.embeddings = HuggingFaceEmbeddings(
            model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )
        
        # åˆå§‹åŒ–ChromaDB
        self.vectorstore = Chroma(
            persist_directory=persist_directory,
            embedding_function=self.embeddings,
            collection_name="job_positions"
        )
        
        # åˆå§‹åŒ–å‹ç¼©æ£€ç´¢å™¨
        self.compressor = LLMChainExtractor.from_llm(OpenAI(temperature=0))
        self.compression_retriever = ContextualCompressionRetriever(
            base_compressor=self.compressor,
            base_retriever=self.vectorstore.as_retriever(search_kwargs={"k": 10})
        )
    
    def add_job_documents(self, documents: List[Document]) -> List[str]:
        """æ·»åŠ èŒä½æ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“"""
        doc_ids = self.vectorstore.add_documents(documents)
        self.vectorstore.persist()
        return doc_ids
    
    def search_similar_jobs(self, query: str, k: int = 5) -> List[Document]:
        """æœç´¢ç›¸ä¼¼èŒä½"""
        compressed_docs = self.compression_retriever.get_relevant_documents(query)
        return compressed_docs[:k]
```

#### 3. RAGæ£€ç´¢é—®ç­”é“¾

```python
from langchain.chains import RetrievalQA
from langchain.chains.question_answering import load_qa_chain
from langchain.prompts import PromptTemplate

class JobRAGSystem:
    """èŒä½ä¿¡æ¯RAGç³»ç»Ÿ"""
    
    def __init__(self, vectorstore_manager: ChromaDBManager):
        self.vectorstore_manager = vectorstore_manager
        self.llm = OpenAI(temperature=0.2)
        
        # æ„å»ºæ£€ç´¢QAé“¾
        self.retrieval_qa = RetrievalQA.from_chain_type(
            llm=self.llm,
            chain_type="stuff",
            retriever=vectorstore_manager.vectorstore.as_retriever(search_kwargs={"k": 5}),
            return_source_documents=True,
            chain_type_kwargs={"prompt": self._build_qa_prompt()}
        )
    
    def _build_qa_prompt(self) -> PromptTemplate:
        """æ„å»ºé—®ç­”Prompt"""
        template = """
ä½ æ˜¯ä¸“ä¸šçš„èŒä½åŒ¹é…é¡¾é—®ã€‚åŸºäºä»¥ä¸‹èŒä½ä¿¡æ¯å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

èŒä½ä¿¡æ¯ï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š{question}

å›ç­”è¦æ±‚ï¼š
1. åŸºäºæä¾›çš„èŒä½ä¿¡æ¯å›ç­”
2. å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œæ˜ç¡®è¯´æ˜
3. æä¾›å…·ä½“çš„èŒä½åŒ¹é…å»ºè®®
4. å›ç­”è¦ä¸“ä¸šä¸”æœ‰å¸®åŠ©

å›ç­”ï¼š
"""
        
        return PromptTemplate(
            template=template,
            input_variables=["context", "question"]
        )
    
    async def ask_question(self, question: str, filters: Dict = None) -> Dict:
        """é—®ç­”æ¥å£"""
        relevant_docs = self.vectorstore_manager.hybrid_search(question, filters)
        
        result = await self.retrieval_qa.arun(
            query=question,
            source_documents=relevant_docs
        )
        
        return {
            "answer": result["result"],
            "source_documents": [
                {
                    "content": doc.page_content,
                    "metadata": doc.metadata
                }
                for doc in result["source_documents"]
            ]
        }
```

#### 4. å®Œæ•´çš„RAGå¤„ç†æµç¨‹

```python
class JobRAGPipeline:
    """å®Œæ•´çš„èŒä½RAGå¤„ç†æµç¨‹"""
    
    def __init__(self):
        self.processor = LangChainJobProcessor()
        self.vectorstore_manager = ChromaDBManager()
        self.rag_system = JobRAGSystem(self.vectorstore_manager)
    
    async def process_and_store_job(self, job_json: Dict) -> str:
        """å¤„ç†å¹¶å­˜å‚¨èŒä½ä¿¡æ¯"""
        
        # 1. ç»“æ„åŒ–æå–
        job_structure = await self.processor.process_job_data(job_json)
        
        # 2. åˆ›å»ºæ–‡æ¡£
        documents = self.processor.create_documents(job_structure)
        
        # 3. å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“
        doc_ids = self.vectorstore_manager.add_job_documents(documents)
        
        return job_structure.job_title
    
    async def query_jobs(self, question: str) -> Dict:
        """æŸ¥è¯¢èŒä½ä¿¡æ¯"""
        return await self.rag_system.ask_question(question)
    
    async def match_jobs(self, user_profile: str) -> List[Dict]:
        """èŒä½åŒ¹é…"""
        return await self.rag_system.find_matching_jobs(user_profile)
```

### RAGç³»ç»Ÿä¼˜åŠ¿

#### 1. æ™ºèƒ½æ–‡æœ¬å¤„ç†
- **è¯­ä¹‰åˆ†å‰²**: åŸºäºèŒä½å†…å®¹ç»“æ„çš„æ™ºèƒ½åˆ†å‰²
- **ç»“æ„åŒ–æå–**: LLMé©±åŠ¨çš„ç²¾ç¡®ä¿¡æ¯æå–
- **å¤šè¯­è¨€æ”¯æŒ**: æ”¯æŒä¸­è‹±æ–‡æ··åˆèŒä½æè¿°

#### 2. å‘é‡åŒ–å­˜å‚¨
- **é«˜æ•ˆæ£€ç´¢**: ChromaDBæä¾›æ¯«ç§’çº§å‘é‡æ£€ç´¢
- **è¯­ä¹‰ç†è§£**: åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦çš„èŒä½åŒ¹é…
- **æŒä¹…åŒ–å­˜å‚¨**: æ”¯æŒæ•°æ®æŒä¹…åŒ–å’Œå¢é‡æ›´æ–°

#### 3. æ™ºèƒ½é—®ç­”
- **ä¸Šä¸‹æ–‡ç†è§£**: åŸºäºæ£€ç´¢åˆ°çš„èŒä½ä¿¡æ¯å›ç­”é—®é¢˜
- **åŒ¹é…è§£é‡Š**: æä¾›è¯¦ç»†çš„åŒ¹é…åŸå› åˆ†æ
- **ä¸ªæ€§åŒ–æ¨è**: æ ¹æ®ç”¨æˆ·ç”»åƒæ¨èåˆé€‚èŒä½

#### 4. å¯æ‰©å±•æ¶æ„
- **æ¨¡å—åŒ–è®¾è®¡**: å„ç»„ä»¶å¯ç‹¬ç«‹å‡çº§å’Œæ›¿æ¢
- **å¤šæ¨¡å‹æ”¯æŒ**: æ”¯æŒä¸åŒçš„LLMå’ŒåµŒå…¥æ¨¡å‹
- **çµæ´»é…ç½®**: é€šè¿‡é…ç½®æ–‡ä»¶è°ƒæ•´RAGå‚æ•°

â”‚       â””â”€â”€ helpers.py         # è¾…åŠ©å‡½æ•°
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ config.yaml            # ä¸»é…ç½®æ–‡ä»¶
â”‚   â”œâ”€â”€ config.example.yaml    # é…ç½®ç¤ºä¾‹
â”‚   â””â”€â”€ prompts/
â”‚       â”œâ”€â”€ job_analysis.txt   # èŒä½åˆ†ææç¤ºè¯
â”‚       â”œâ”€â”€ matching.txt       # åŒ¹é…åˆ†ææç¤ºè¯
â”‚       â”œâ”€â”€ rag_extraction.txt # RAGç»“æ„åŒ–æå–æç¤ºè¯
â”‚       â””â”€â”€ qa_template.txt    # é—®ç­”æ¨¡æ¿æç¤ºè¯
â”œâ”€â”€ data/                      # æ•°æ®åº“æ–‡ä»¶
â”œâ”€â”€ chroma_db/                 # ChromaDBå‘é‡æ•°æ®åº“
â”œâ”€â”€ logs/                      # æ—¥å¿—æ–‡ä»¶
â”œâ”€â”€ tests/                     # æµ‹è¯•æ–‡ä»¶
â”œâ”€â”€ docs/                      # æ–‡æ¡£
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

### RAGå¢å¼ºçš„æ•°æ®åº“è®¾è®¡

#### èŒä½ä¿¡æ¯è¡¨ (jobs) - æ‰©å±•ç‰ˆ
```sql
CREATE TABLE jobs (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    job_id VARCHAR(100) UNIQUE NOT NULL,  -- èŒä½å”¯ä¸€æ ‡è¯†
    title VARCHAR(200) NOT NULL,          -- èŒä½æ ‡é¢˜
    company VARCHAR(200) NOT NULL,        -- å…¬å¸åç§°
    url VARCHAR(500) NOT NULL,            -- èŒä½è¯¦æƒ…é¡µURL
    application_status VARCHAR(50) DEFAULT 'pending',  -- æŠ•é€’çŠ¶æ€
    match_score FLOAT,                     -- ä¼ ç»ŸåŒ¹é…åº¦è¯„åˆ†
    semantic_score FLOAT,                  -- RAGè¯­ä¹‰åŒ¹é…åº¦è¯„åˆ†
    vector_id VARCHAR(100),                -- ChromaDBå‘é‡ID
    structured_data TEXT,                  -- JSONæ ¼å¼çš„ç»“æ„åŒ–æ•°æ®
    website VARCHAR(50) NOT NULL,         -- æ¥æºç½‘ç«™
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    submitted_at TIMESTAMP,               -- æŠ•é€’æ—¶é—´
    rag_processed BOOLEAN DEFAULT FALSE   -- æ˜¯å¦å·²è¿›è¡ŒRAGå¤„ç†
);
```

#### å‘é‡åŒ¹é…è®°å½•è¡¨ (vector_matches)
```sql
CREATE TABLE vector_matches (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    job_id VARCHAR(100) NOT NULL,
    user_query TEXT NOT NULL,
    similarity_score FLOAT NOT NULL,
    matched_content TEXT,
    match_type VARCHAR(50),  -- 'skill', 'responsibility', 'requirement'
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (job_id) REFERENCES jobs(job_id)
);
```

### æ•°æ®åº“è®¾è®¡

#### èŒä½ä¿¡æ¯è¡¨ (jobs)
```sql
CREATE TABLE jobs (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    job_id VARCHAR(100) UNIQUE NOT NULL,  -- èŒä½å”¯ä¸€æ ‡è¯†
    title VARCHAR(200) NOT NULL,          -- èŒä½æ ‡é¢˜
    company VARCHAR(200) NOT NULL,        -- å…¬å¸åç§°
    url VARCHAR(500) NOT NULL,            -- èŒä½è¯¦æƒ…é¡µURL
    application_status VARCHAR(50) DEFAULT 'pending',  -- æŠ•é€’çŠ¶æ€
    match_score FLOAT,                     -- åŒ¹é…åº¦è¯„åˆ†
    website VARCHAR(50) NOT NULL,         -- æ¥æºç½‘ç«™
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    submitted_at TIMESTAMP                 -- æŠ•é€’æ—¶é—´
);
```

### RAGå¢å¼ºçš„æ ¸å¿ƒæµç¨‹

1. **å¯åŠ¨æµè§ˆå™¨**: æ‰“å¼€æŒ‡å®šæ‹›è˜ç½‘ç«™
2. **äººå·¥ç™»å½•**: ç­‰å¾…ç”¨æˆ·æ‰‹åŠ¨å®Œæˆç™»å½•
3. **è‡ªåŠ¨çˆ¬å–**: è·å–èŒä½åˆ—è¡¨ï¼Œé€ä¸ªåˆ†æ
4. **RAGæ™ºèƒ½åˆ†æ**:
   - ä½¿ç”¨LangChainè¿›è¡ŒèŒä½ä¿¡æ¯ç»“æ„åŒ–æå–
   - åˆ›å»ºè¯­ä¹‰æ–‡æ¡£å¹¶å‘é‡åŒ–å­˜å‚¨åˆ°ChromaDB
   - å»ºç«‹èŒä½çŸ¥è¯†åº“ï¼Œæ”¯æŒè¯­ä¹‰æ£€ç´¢
5. **æ™ºèƒ½åŒ¹é…è¯„åˆ†**:
   - åŸºäºå‘é‡ç›¸ä¼¼åº¦è¿›è¡Œè¯­ä¹‰åŒ¹é…
   - ç»“åˆRAGæ£€ç´¢å¢å¼ºåŒ¹é…ç²¾åº¦
   - ç”Ÿæˆè¯¦ç»†çš„åŒ¹é…åˆ†ææŠ¥å‘Š
6. **è‡ªåŠ¨æŠ•é€’**: æ ¹æ®æ™ºèƒ½åŒ¹é…åº¦å†³å®šæ˜¯å¦æŠ•é€’
7. **çŸ¥è¯†åº“æ›´æ–°**: å°†èŒä½ä¿¡æ¯å’ŒåŒ¹é…ç»“æœå­˜å‚¨åˆ°å‘é‡æ•°æ®åº“
8. **è®°å½•ä¿å­˜**: å°†ç»“æ„åŒ–æ•°æ®ä¿å­˜åˆ°SQLiteï¼Œå‘é‡æ•°æ®ä¿å­˜åˆ°ChromaDB

### RAGå¢å¼ºçš„é…ç½®ç¤ºä¾‹

```yaml
# åŸºç¡€é…ç½®
app:
  name: "Smart Resume Auto Submitter"
  version: "2.0.0"
  description: "åŸºäºLangChain RAGçš„æ™ºèƒ½ç®€å†æŠ•é€’ç³»ç»Ÿ"

# ç½‘ç«™é…ç½®
websites:
  zhilian:
    enabled: true
    base_url: "https://www.zhaopin.com"
    submit_button_selector: ".btn-apply"

# RAGç³»ç»Ÿé…ç½®
rag:
  # LLMé…ç½®
  llm:
    provider: "openai"  # openai, claude, local
    model: "gpt-3.5-turbo"
    temperature: 0.1
    max_tokens: 2000
    
  # å‘é‡æ•°æ®åº“é…ç½®
  vectorstore:
    provider: "chromadb"
    persist_directory: "./chroma_db"
    collection_name: "job_positions"
    
  # åµŒå…¥æ¨¡å‹é…ç½®
  embeddings:
    model_name: "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
    device: "cpu"
    normalize_embeddings: true
    
  # æ–‡æœ¬åˆ†å‰²é…ç½®
  text_splitter:
    chunk_size: 500
    chunk_overlap: 50
    separators: ["\nå²—ä½èŒè´£ï¼š", "\näººå‘˜è¦æ±‚ï¼š", "\nä»»èŒè¦æ±‚ï¼š", "\n\n", "\n", "ã€‚"]
    
  # æ£€ç´¢é…ç½®
  retrieval:
    search_type: "similarity"
    k: 5
    score_threshold: 0.7
    use_compression: true

# æ™ºèƒ½åŒ¹é…ç®—æ³•é…ç½®
matching:
  # ä¼ ç»Ÿæƒé‡é…ç½®
  traditional_weights:
    skills: 0.5
    experience: 0.3
    salary: 0.2
    
  # RAGå¢å¼ºæƒé‡é…ç½®
  rag_weights:
    semantic_similarity: 0.6  # è¯­ä¹‰ç›¸ä¼¼åº¦
    traditional_score: 0.4    # ä¼ ç»Ÿè¯„åˆ†
    
  # åŒ¹é…é˜ˆå€¼
  thresholds:
    auto_submit: 0.85         # RAGå¢å¼ºåæé«˜é˜ˆå€¼
    manual_review: 0.7
    skip: 0.4

# ç®€å†é…ç½®
resume:
  skills: ["Python", "Java", "React", "Node.js", "LangChain", "RAG"]
  experience_years: 3
  expected_salary_min: 15000
  expected_salary_max: 25000
  preferred_locations: ["ä¸Šæµ·"]
  
  # ç®€å†å‘é‡åŒ–é…ç½®
  profile_description: |
    å…·æœ‰3å¹´Pythonå¼€å‘ç»éªŒï¼Œç†Ÿæ‚‰æœºå™¨å­¦ä¹ å’ŒLLMåº”ç”¨å¼€å‘ï¼Œ
    æœ‰RAGç³»ç»Ÿæ„å»ºç»éªŒï¼Œæ“…é•¿ä½¿ç”¨LangChainè¿›è¡ŒAIåº”ç”¨å¼€å‘ã€‚

# æç¤ºè¯æ¨¡æ¿é…ç½®
prompts:
  job_extraction: "config/prompts/rag_extraction.txt"
  matching_analysis: "config/prompts/matching.txt"
  qa_template: "config/prompts/qa_template.txt"
```

## RAGå¢å¼ºçš„AIæç¤ºè¯æ¨¡æ¿

### RAGç»“æ„åŒ–æå–æç¤ºè¯ (config/prompts/rag_extraction.txt)
```
ä½ æ˜¯ä¸“ä¸šçš„HRæ•°æ®åˆ†æå¸ˆã€‚è¯·åˆ†æä»¥ä¸‹èŒä½æè¿°ï¼Œå°†å…¶ç»“æ„åŒ–æå–ã€‚

èŒä½æ–‡æœ¬ï¼š
{job_text}

æå–è¦æ±‚ï¼š
1. å‡†ç¡®åˆ†ç¦»å²—ä½èŒè´£å’Œäººå‘˜è¦æ±‚
2. æå–æ‰€æœ‰æŠ€èƒ½å…³é”®è¯
3. è¯†åˆ«å­¦å†å’Œç»éªŒè¦æ±‚
4. ç¡®ä¿ä¿¡æ¯å®Œæ•´ä¸”ä¸é‡å¤

{format_instructions}

ç»“æ„åŒ–è¾“å‡ºï¼š
```

### RAGé—®ç­”æ¨¡æ¿æç¤ºè¯ (config/prompts/qa_template.txt)
```
ä½ æ˜¯ä¸“ä¸šçš„èŒä½åŒ¹é…é¡¾é—®ã€‚åŸºäºä»¥ä¸‹èŒä½ä¿¡æ¯å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

èŒä½ä¿¡æ¯ï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š{question}

å›ç­”è¦æ±‚ï¼š
1. åŸºäºæä¾›çš„èŒä½ä¿¡æ¯å›ç­”
2. å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œæ˜ç¡®è¯´æ˜
3. æä¾›å…·ä½“çš„èŒä½åŒ¹é…å»ºè®®
4. å›ç­”è¦ä¸“ä¸šä¸”æœ‰å¸®åŠ©

å›ç­”ï¼š
```

### æ™ºèƒ½åŒ¹é…åˆ†ææç¤ºè¯ (config/prompts/matching.txt)
```
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç®€å†åŒ¹é…åˆ†æå¸ˆã€‚è¯·åŸºäºRAGæ£€ç´¢åˆ°çš„èŒä½ä¿¡æ¯åˆ†æå€™é€‰äººåŒ¹é…åº¦ã€‚

å€™é€‰äººä¿¡æ¯ï¼š
{resume_info}

æ£€ç´¢åˆ°çš„ç›¸å…³èŒä½ä¿¡æ¯ï¼š
{retrieved_context}

èŒä½è¦æ±‚ï¼š
{job_requirements}

è¯·ä»ä»¥ä¸‹ç»´åº¦åˆ†æåŒ¹é…åº¦ï¼ˆ0-1åˆ†ï¼‰ï¼š
1. è¯­ä¹‰ç›¸ä¼¼åº¦åŒ¹é…ï¼ˆæƒé‡60%ï¼‰ï¼šåŸºäºå‘é‡ç›¸ä¼¼åº¦çš„æ·±åº¦è¯­ä¹‰ç†è§£
2. æŠ€èƒ½åŒ¹é…åº¦ï¼ˆæƒé‡25%ï¼‰ï¼šå…·ä½“æŠ€èƒ½è¦æ±‚çš„åŒ¹é…
3. ç»éªŒåŒ¹é…åº¦ï¼ˆæƒé‡15%ï¼‰ï¼šå·¥ä½œç»éªŒå’Œé¡¹ç›®ç»å†åŒ¹é…

è¾“å‡ºæ ¼å¼ï¼š
{
  "semantic_match": 0.85,
  "skills_match": 0.8,
  "experience_match": 0.7,
  "overall_score": 0.82,
  "match_reasons": ["åŒ¹é…åŸå› 1", "åŒ¹é…åŸå› 2"],
  "improvement_suggestions": ["æ”¹è¿›å»ºè®®1", "æ”¹è¿›å»ºè®®2"],
  "analysis": "è¯¦ç»†åˆ†æè¯´æ˜"
}
```

### èŒä½æ¨èæç¤ºè¯ (config/prompts/recommendation.txt)
```
ä½ æ˜¯ä¸“ä¸šçš„èŒä½æ¨èä¸“å®¶ã€‚åŸºäºç”¨æˆ·ç”»åƒå’ŒRAGæ£€ç´¢ç»“æœï¼Œæ¨èæœ€é€‚åˆçš„èŒä½ã€‚

ç”¨æˆ·ç”»åƒï¼š
{user_profile}

æ£€ç´¢åˆ°çš„èŒä½ä¿¡æ¯ï¼š
{retrieved_jobs}

æ¨èè¦æ±‚ï¼š
1. æ ¹æ®è¯­ä¹‰ç›¸ä¼¼åº¦æ’åº
2. è€ƒè™‘ç”¨æˆ·çš„æŠ€èƒ½åŒ¹é…åº¦
3. åˆ†æèŒä¸šå‘å±•æ½œåŠ›
4. æä¾›æ¨èç†ç”±

è¾“å‡ºæ ¼å¼ï¼š
{
  "recommended_jobs": [
    {
      "job_title": "èŒä½åç§°",
      "company": "å…¬å¸åç§°",
      "match_score": 0.9,
      "recommendation_reason": "æ¨èç†ç”±",
      "growth_potential": "å‘å±•æ½œåŠ›åˆ†æ"
    }
  ],
  "summary": "æ¨èæ€»ç»“"
}
```

## é‡æ„åçš„æ¨¡å—åŒ–æ¶æ„

### ç™»å½•åŠŸèƒ½åˆ†ç¦»

ç³»ç»Ÿå·²é‡æ„ä¸ºæ¨¡å—åŒ–æ¶æ„ï¼Œå°†ç™»å½•åŠŸèƒ½å’Œå†…å®¹æå–åŠŸèƒ½å®Œå…¨åˆ†ç¦»ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ç™»å½•æ¨¡å—      â”‚    â”‚   å†…å®¹æå–æ¨¡å—   â”‚
â”‚                 â”‚    â”‚                 â”‚
â”‚ â”œâ”€ LoginManager â”‚    â”‚ â”œâ”€ ContentExtractor
â”‚ â”œâ”€ SessionManagerâ”‚    â”‚ â”œâ”€ PageParser   â”‚
â”‚ â””â”€ BrowserManagerâ”‚    â”‚ â””â”€ DataStorage  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### æœ€æ–°æ›´æ–°ï¼šåˆ†é¡µåŠŸèƒ½å¢å¼º (2025-01-18)

#### ğŸš€ åˆ†é¡µåŠŸèƒ½æ¦‚è¿°

å†…å®¹æå–æ¨¡å—æ–°å¢å®Œæ•´çš„åˆ†é¡µåŠŸèƒ½ï¼Œæ”¯æŒè‡ªåŠ¨å¯¼èˆªå¤šé¡µå†…å®¹ï¼Œå¤§å¹…æå‡æ•°æ®é‡‡é›†è¦†ç›–èŒƒå›´ï¼š

- **é»˜è®¤é…ç½®**: è‡ªåŠ¨è¯»å–å‰10é¡µå†…å®¹
- **æ™ºèƒ½å¯¼èˆª**: è‡ªåŠ¨æ£€æµ‹å’Œç‚¹å‡»ä¸‹ä¸€é¡µæŒ‰é’®
- **å¤šé¡µåˆå¹¶**: è‡ªåŠ¨åˆå¹¶æ‰€æœ‰é¡µé¢çš„æå–ç»“æœ
- **é¡µç æ ‡è®°**: æ¯ä¸ªç»“æœéƒ½æ ‡è®°æ¥æºé¡µç 
- **é”™è¯¯æ¢å¤**: å•é¡µå¤±è´¥ä¸å½±å“æ•´ä½“æå–æµç¨‹

#### ğŸ“„ åˆ†é¡µé…ç½®

```yaml
# æœç´¢ç­–ç•¥é…ç½® (config/config.yaml)
search:
  strategy:
    max_pages: 10              # é»˜è®¤æœ€å¤§é¡µæ•°
    enable_pagination: true    # æ˜¯å¦å¯ç”¨åˆ†é¡µ
    page_delay: 2              # é¡µé¢é—´å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰
    page_delay_max: 5          # é¡µé¢é—´æœ€å¤§å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰
```

#### ğŸ”§ æ ¸å¿ƒåˆ†é¡µæ–¹æ³•

**PageParser æ–°å¢æ–¹æ³•ï¼š**

1. **`has_next_page(driver)`** - æ£€æµ‹ä¸‹ä¸€é¡µæŒ‰é’®
   - æ”¯æŒå¤šç§é€‰æ‹©å™¨ï¼š`.btn_next`, `.next-page`, `.page-next`, `.pager-next`ç­‰
   - æ™ºèƒ½åˆ¤æ–­æŒ‰é’®æ˜¯å¦å¯ç”¨ï¼ˆéç¦ç”¨çŠ¶æ€ï¼‰

2. **`navigate_to_next_page(driver)`** - å¯¼èˆªåˆ°ä¸‹ä¸€é¡µ
   - æ¨¡æ‹Ÿäººç±»ç‚¹å‡»è¡Œä¸ºï¼ˆæ‚¬åœã€æ»šåŠ¨ç­‰ï¼‰
   - éªŒè¯é¡µé¢è·³è½¬æ˜¯å¦æˆåŠŸ
   - æ”¯æŒAJAXåŠ è½½çš„é¡µé¢

3. **`get_current_page_info(driver)`** - è·å–é¡µé¢ä¿¡æ¯
   - ä»URLå‚æ•°å’Œé¡µé¢å…ƒç´ ä¸­æå–é¡µç 
   - è¿”å›å½“å‰é¡µç å’Œé¡µé¢çŠ¶æ€

**ContentExtractor å¢å¼ºæ–¹æ³•ï¼š**

1. **`extract_from_search_url()`** - æ”¯æŒå¤šé¡µæå–
   - æ–°å¢ `max_pages` å‚æ•°
   - å®ç°é¡µé¢å¾ªç¯é€»è¾‘
   - ä¸ºæ¯ä¸ªç»“æœæ·»åŠ  `page_number` å­—æ®µ

2. **`extract_from_keyword()`** - å…³é”®è¯å¤šé¡µæœç´¢
   - æ”¯æŒ `max_pages` å‚æ•°ä¼ é€’

3. **`extract_multiple_keywords()`** - æ‰¹é‡å¤šé¡µæå–
   - æ–°å¢ `max_pages_per_keyword` å‚æ•°

#### ğŸ’¡ ä½¿ç”¨ç¤ºä¾‹

```python
# ä½¿ç”¨é»˜è®¤é…ç½®ï¼ˆ10é¡µï¼‰
results = extractor.extract_from_keyword("AIå·¥ç¨‹å¸ˆ")

# è‡ªå®šä¹‰é¡µæ•°
results = extractor.extract_from_keyword("AIå·¥ç¨‹å¸ˆ", max_pages=5)

# æ‰¹é‡æå–å¤šä¸ªå…³é”®è¯ï¼Œæ¯ä¸ªæœ€å¤š3é¡µ
results = extractor.extract_multiple_keywords(
    ["AIå·¥ç¨‹å¸ˆ", "æ•°æ®å·¥ç¨‹å¸ˆ"],
    max_pages_per_keyword=3
)

# æ£€æŸ¥ç»“æœä¸­çš„é¡µç ä¿¡æ¯
for job in results:
    print(f"èŒä½: {job['title']} - æ¥æº: ç¬¬{job['page_number']}é¡µ")
```

#### ğŸ§ª æµ‹è¯•éªŒè¯

åˆ›å»ºäº†ä¸“é—¨çš„æµ‹è¯•è„šæœ¬éªŒè¯åˆ†é¡µåŠŸèƒ½ï¼š

```bash
# è¿è¡Œåˆ†é¡µåŠŸèƒ½æµ‹è¯•
python simple_pagination_test.py

# æµ‹è¯•ç»“æœç¤ºä¾‹ï¼š
# ğŸ“Š æµ‹è¯•ç»“æœ: 3/3 é€šè¿‡
# ğŸ‰ æ‰€æœ‰åˆ†é¡µåŠŸèƒ½æµ‹è¯•é€šè¿‡ï¼
#
# ğŸ“ åˆ†é¡µåŠŸèƒ½ç‰¹æ€§:
#   âœ… é…ç½®æ–‡ä»¶æ”¯æŒåˆ†é¡µå‚æ•°
#   âœ… PageParser å…·å¤‡åˆ†é¡µå¯¼èˆªèƒ½åŠ›
#   âœ… ContentExtractor æ”¯æŒå¤šé¡µæå–
#   âœ… å¤šç§ä¸‹ä¸€é¡µæŒ‰é’®é€‰æ‹©å™¨
```

#### ğŸ¯ æŠ€æœ¯ç‰¹æ€§

1. **æ™ºèƒ½åˆ†é¡µæ£€æµ‹** - è‡ªåŠ¨è¯†åˆ«å¤šç§ä¸‹ä¸€é¡µæŒ‰é’®æ ·å¼
2. **äººæ€§åŒ–å»¶è¿Ÿ** - é¡µé¢é—´2-5ç§’éšæœºå»¶è¿Ÿï¼Œé¿å…åçˆ¬æ£€æµ‹
3. **çµæ´»é…ç½®** - å¯é€šè¿‡é…ç½®æ–‡ä»¶æˆ–å‚æ•°æ§åˆ¶åˆ†é¡µè¡Œä¸º
4. **ç»“æœè¿½è¸ª** - æ¯ä¸ªèŒä½éƒ½æ ‡è®°æ¥æºé¡µç 
5. **é”™è¯¯æ¢å¤** - å•é¡µå¤±è´¥ä¸å½±å“æ•´ä½“æå–
6. **èµ„æºä¼˜åŒ–** - è¾¾åˆ°é™åˆ¶æ—¶è‡ªåŠ¨åœæ­¢ï¼Œé¿å…æ— æ•ˆè¯·æ±‚

#### ğŸ“ˆ æ€§èƒ½æå‡

- **æ•°æ®è¦†ç›–èŒƒå›´**: ä»å•é¡µæå‡åˆ°å¤šé¡µï¼ˆé»˜è®¤10é¡µï¼‰
- **é‡‡é›†æ•ˆç‡**: è‡ªåŠ¨åŒ–åˆ†é¡µå¯¼èˆªï¼Œæ— éœ€äººå·¥å¹²é¢„
- **æ•°æ®å®Œæ•´æ€§**: æ”¯æŒé¡µç æ ‡è®°ï¼Œä¾¿äºæ•°æ®æº¯æº
- **ç¨³å®šæ€§**: æ™ºèƒ½é”™è¯¯æ¢å¤ï¼Œæé«˜é‡‡é›†æˆåŠŸç‡

### æ–°å¢æ¨¡å—é…ç½®

```yaml
# è¿è¡Œæ¨¡å¼é…ç½®
mode:
  development: true           # å¼€å‘æ¨¡å¼
  skip_login: false          # è·³è¿‡ç™»å½•æ£€æŸ¥
  use_saved_session: true    # ä½¿ç”¨ä¿å­˜çš„ä¼šè¯
  session_file: "data/session.json"
  session_timeout: 3600      # ä¼šè¯è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
  auto_save_session: true    # è‡ªåŠ¨ä¿å­˜ä¼šè¯
  close_on_complete: false   # å®Œæˆåæ˜¯å¦å…³é—­æµè§ˆå™¨

# æ¨¡å—é…ç½®
modules:
  login:
    enabled: true
    auto_save_session: true
  extraction:
    enabled: true
    max_concurrent: 1
    retry_attempts: 3
  browser:
    reuse_session: true
    close_on_complete: false
```

### ä½¿ç”¨æ–¹å¼

#### 1. ç‹¬ç«‹ç™»å½•æµ‹è¯•
```bash
# åŸºæœ¬ç™»å½•æµ‹è¯•
python test_login.py

# ç™»å½•å¹¶ä¿å­˜ä¼šè¯
python test_login.py --save-session

# æ£€æŸ¥ç™»å½•çŠ¶æ€
python test_login.py --check-status
```

#### 2. ç‹¬ç«‹å†…å®¹æå–æµ‹è¯•
```bash
# åŸºäºå…³é”®è¯æå–
python test_extraction.py --keyword "AIå·¥ç¨‹å¸ˆ"

# è·³è¿‡ç™»å½•æ£€æŸ¥ï¼ˆå¼€å‘æ¨¡å¼ï¼‰
python test_extraction.py --keyword "æ•°æ®æ¶æ„å¸ˆ" --skip-login

# æ‰¹é‡æå–å¤šä¸ªå…³é”®è¯
python test_extraction.py --multiple "AIå·¥ç¨‹å¸ˆ,æ•°æ®æ¶æ„å¸ˆ,Pythonå·¥ç¨‹å¸ˆ"
```

#### 3. ç¼–ç¨‹æ¥å£ä½¿ç”¨

**ç‹¬ç«‹ç™»å½•ç®¡ç†ï¼š**
```python
from src.auth.login_manager import LoginManager

with LoginManager(config) as login_manager:
    success = login_manager.start_login_session(save_session=True)
    if success:
        print("ç™»å½•æˆåŠŸï¼Œä¼šè¯å·²ä¿å­˜")
```

**ç‹¬ç«‹å†…å®¹æå–ï¼š**
```python
from src.extraction.content_extractor import ContentExtractor

# å¼€å‘æ¨¡å¼ï¼šè·³è¿‡ç™»å½•
config['mode']['skip_login'] = True

with ContentExtractor(config) as extractor:
    results = extractor.extract_from_keyword("AIå·¥ç¨‹å¸ˆ", max_results=30)
    print(f"æå–åˆ° {len(results)} ä¸ªèŒä½")
```

**ä½¿ç”¨ä¿å­˜çš„ä¼šè¯ï¼š**
```python
# é…ç½®ä½¿ç”¨ä¿å­˜çš„ä¼šè¯
config['mode']['use_saved_session'] = True
config['mode']['session_file'] = 'data/my_session.json'

with ContentExtractor(config) as extractor:
    results = extractor.extract_from_keyword("æ•°æ®æ¶æ„å¸ˆ")
```

### é‡æ„ä¼˜åŠ¿

1. **ç‹¬ç«‹å¼€å‘**: ç™»å½•å’Œå†…å®¹æå–å¯ä»¥ç‹¬ç«‹å¼€å‘å’Œæµ‹è¯•
2. **ä¼šè¯ç®¡ç†**: æ”¯æŒä¼šè¯ä¿å­˜å’Œå¤ç”¨ï¼Œæé«˜æ•ˆç‡
3. **å¼€å‘å‹å¥½**: æ”¯æŒè·³è¿‡ç™»å½•çš„å¼€å‘æ¨¡å¼
4. **æ¨¡å—åŒ–é…ç½®**: ç»†ç²’åº¦çš„é…ç½®æ§åˆ¶
5. **æ˜“äºç»´æŠ¤**: æ¸…æ™°çš„èŒè´£åˆ†ç¦»

### è¿ç§»æŒ‡å—

ä»æ—§ç‰ˆæœ¬è¿ç§»ï¼š
```python
# æ—§ç‰ˆæœ¬
automation = JobSearchAutomation()
automation.start_search_session(keyword="AIå·¥ç¨‹å¸ˆ")

# æ–°ç‰ˆæœ¬ - åˆ†ç¦»å¼
# 1. å…ˆç™»å½•
with LoginManager(config) as login_manager:
    login_manager.start_login_session(save_session=True)

# 2. å†æå–å†…å®¹
with ContentExtractor(config) as extractor:
    results = extractor.extract_from_keyword("AIå·¥ç¨‹å¸ˆ")
```

## Notes

### æ ¸å¿ƒç‰¹æ€§
- ä½¿ç”¨äººå·¥ç™»å½•é¿å…éªŒè¯ç å’Œé£æ§æ£€æµ‹
- é€šè¿‡éšæœºå»¶è¿Ÿå’Œè¡Œä¸ºæ¨¡æ‹Ÿé˜²æ­¢åçˆ¬
- æ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼Œé¿å…é‡å¤å¤„ç†
- å¯é…ç½®åŒ¹é…ç®—æ³•æƒé‡å’Œé˜ˆå€¼
- ç®€åŒ–æŠ•é€’æµç¨‹ï¼Œåªéœ€ç‚¹å‡»æŒ‰é’®å³å¯
- é¡¹ç›®é‡‡ç”¨åº”ç”¨ç¨‹åºæ¶æ„ï¼Œç›´æ¥è¿è¡Œmain.pyï¼Œæ— éœ€å®‰è£…åŒ…

### æ¨¡å—åŒ–æ¶æ„ç‰¹æ€§
- **ç™»å½•åŠŸèƒ½åˆ†ç¦»**: æ”¯æŒç‹¬ç«‹çš„ç™»å½•æ¨¡å—ï¼Œä¾¿äºå¼€å‘å’Œè°ƒè¯•
- **ä¼šè¯ç®¡ç†**: æ”¯æŒä¼šè¯ä¿å­˜å’Œå¤ç”¨ï¼Œæé«˜ä½¿ç”¨æ•ˆç‡
- **å¼€å‘æ¨¡å¼**: æ”¯æŒè·³è¿‡ç™»å½•ç›´æ¥æµ‹è¯•å†…å®¹æå–
- **åˆ†é¡µåŠŸèƒ½**: æ”¯æŒå¤šé¡µå†…å®¹è‡ªåŠ¨é‡‡é›†ï¼Œå¤§å¹…æå‡æ•°æ®è¦†ç›–èŒƒå›´

### RAGæ™ºèƒ½åˆ†æç‰¹æ€§ ğŸš€
- **LangChainé›†æˆ**: åŸºäºLangChainæ„å»ºå®Œæ•´çš„RAGå¤„ç†æµç¨‹
- **è¯­ä¹‰ç†è§£**: ä½¿ç”¨å¤šè¯­è¨€åµŒå…¥æ¨¡å‹è¿›è¡Œæ·±åº¦è¯­ä¹‰åˆ†æ
- **å‘é‡å­˜å‚¨**: ChromaDBæä¾›é«˜æ•ˆçš„å‘é‡æ•°æ®åº“å­˜å‚¨å’Œæ£€ç´¢
- **æ™ºèƒ½åˆ†å‰²**: åŸºäºèŒä½å†…å®¹ç»“æ„çš„è¯­ä¹‰çº§æ–‡æœ¬åˆ†å‰²
- **ç»“æ„åŒ–æå–**: LLMé©±åŠ¨çš„ç²¾ç¡®èŒä½ä¿¡æ¯ç»“æ„åŒ–
- **è¯­ä¹‰åŒ¹é…**: åŸºäºå‘é‡ç›¸ä¼¼åº¦çš„æ·±åº¦è¯­ä¹‰åŒ¹é…
- **æ™ºèƒ½é—®ç­”**: æ”¯æŒåŸºäºèŒä½çŸ¥è¯†åº“çš„æ™ºèƒ½é—®ç­”
- **ä¸ªæ€§åŒ–æ¨è**: æ ¹æ®ç”¨æˆ·ç”»åƒæ¨èæœ€é€‚åˆçš„èŒä½
- **åŒ¹é…è§£é‡Š**: æä¾›è¯¦ç»†çš„åŒ¹é…åŸå› å’Œæ”¹è¿›å»ºè®®
- **æŒç»­å­¦ä¹ **: æ ¹æ®æŠ•é€’åé¦ˆæŒç»­ä¼˜åŒ–åŒ¹é…ç®—æ³•

### æŠ€æœ¯ä¼˜åŠ¿
- **åŒé‡æ•°æ®åº“**: SQLiteå­˜å‚¨ç»“æ„åŒ–æ•°æ®ï¼ŒChromaDBå­˜å‚¨å‘é‡æ•°æ®
- **å¤šæ¨¡å‹æ”¯æŒ**: æ”¯æŒOpenAIã€Claudeã€æœ¬åœ°LLMç­‰å¤šç§æ¨¡å‹
- **çµæ´»é…ç½®**: é€šè¿‡é…ç½®æ–‡ä»¶ç²¾ç¡®æ§åˆ¶RAGç³»ç»Ÿå‚æ•°
- **æ¨¡å—åŒ–è®¾è®¡**: RAGç»„ä»¶å¯ç‹¬ç«‹å¼€å‘ã€æµ‹è¯•å’Œéƒ¨ç½²
- **é«˜æ€§èƒ½æ£€ç´¢**: æ¯«ç§’çº§å‘é‡æ£€ç´¢ï¼Œæ”¯æŒå¤§è§„æ¨¡èŒä½æ•°æ®
- **ä¸­è‹±æ–‡æ”¯æŒ**: å®Œæ•´æ”¯æŒä¸­è‹±æ–‡æ··åˆèŒä½æè¿°çš„å¤„ç†

### ä½¿ç”¨åœºæ™¯
- **ç²¾å‡†æŠ•é€’**: åŸºäºè¯­ä¹‰ç†è§£çš„é«˜ç²¾åº¦èŒä½åŒ¹é…
- **èŒä½åˆ†æ**: æ·±åº¦åˆ†æèŒä½è¦æ±‚å’Œå¸‚åœºè¶‹åŠ¿
- **æŠ€èƒ½è¯„ä¼°**: è¯„ä¼°ä¸ªäººæŠ€èƒ½ä¸å¸‚åœºéœ€æ±‚çš„åŒ¹é…åº¦
- **èŒä¸šè§„åˆ’**: åŸºäºæ•°æ®åˆ†æçš„èŒä¸šå‘å±•å»ºè®®
- **æ‰¹é‡å¤„ç†**: é«˜æ•ˆå¤„ç†å¤§é‡èŒä½ä¿¡æ¯çš„ç»“æ„åŒ–åˆ†æ

## ğŸ”„ æ™ºèƒ½å»é‡ç³»ç»Ÿè®¾è®¡ (2025-01-20)

### ğŸ“‹ èŒä½æŒ‡çº¹å»é‡ç³»ç»Ÿ

ä¸ºäº†å‡å°‘é‡å¤çˆ¬å–å’Œæé«˜çˆ¬å–æ•ˆç‡ï¼Œç³»ç»Ÿå¼•å…¥äº†åŸºäºèŒä½åŸºæœ¬ä¿¡æ¯çš„æ™ºèƒ½å»é‡æœºåˆ¶ã€‚

#### ğŸ¯ è®¾è®¡åŸç†

**æ ¸å¿ƒæ€è·¯**: åœ¨åˆ—è¡¨é¡µå°±èƒ½åˆ¤æ–­èŒä½æ˜¯å¦å·²çˆ¬å–ï¼Œé¿å…ä¸å¿…è¦çš„è¯¦æƒ…é¡µè®¿é—®

```mermaid
graph TD
    A[è§£æåˆ—è¡¨é¡µèŒä½] --> B[ç”ŸæˆèŒä½æŒ‡çº¹]
    B --> C{æ£€æŸ¥æŒ‡çº¹æ˜¯å¦å­˜åœ¨}
    C -->|å­˜åœ¨| D[è·³è¿‡è¯¥èŒä½]
    C -->|ä¸å­˜åœ¨| E[ç‚¹å‡»è·å–è¯¦æƒ…URL]
    E --> F[çˆ¬å–è¯¦æƒ…é¡µå†…å®¹]
    F --> G[ä¿å­˜åˆ°æ•°æ®åº“]
    G --> H[è®°å½•èŒä½æŒ‡çº¹]
    
    style D fill:#ffcccc
    style E fill:#ccffcc
```

#### ğŸ”§ èŒä½æŒ‡çº¹ç®—æ³•

```python
def generate_job_fingerprint(title: str, company: str, salary: str = "", location: str = "") -> str:
    """
    åŸºäºåˆ—è¡¨é¡µå¯è·å–çš„ä¿¡æ¯ç”ŸæˆèŒä½æŒ‡çº¹
    
    Args:
        title: èŒä½æ ‡é¢˜
        company: å…¬å¸åç§°
        salary: è–ªèµ„ä¿¡æ¯ï¼ˆå¯é€‰ï¼‰
        location: å·¥ä½œåœ°ç‚¹ï¼ˆå¯é€‰ï¼‰
        
    Returns:
        12ä½MD5å“ˆå¸ŒæŒ‡çº¹
    """
    import hashlib
    
    # æ ‡å‡†åŒ–å¤„ç†
    title_clean = title.strip().lower().replace(' ', '')
    company_clean = company.strip().lower().replace(' ', '')
    salary_clean = salary.strip() if salary else ""
    location_clean = location.strip() if location else ""
    
    # ç”ŸæˆæŒ‡çº¹
    fingerprint_data = f"{title_clean}|{company_clean}|{salary_clean}|{location_clean}"
    return hashlib.md5(fingerprint_data.encode('utf-8')).hexdigest()[:12]
```

#### ğŸ—„ï¸ æ•°æ®åº“è¡¨ç»“æ„æ‰©å±•

```sql
-- åœ¨jobsè¡¨ä¸­æ·»åŠ job_fingerprintå­—æ®µå’ŒRAGå¤„ç†çŠ¶æ€å­—æ®µ
ALTER TABLE jobs ADD COLUMN job_fingerprint VARCHAR(12) UNIQUE;
ALTER TABLE jobs ADD COLUMN rag_processed BOOLEAN DEFAULT FALSE;
ALTER TABLE jobs ADD COLUMN rag_processed_at TIMESTAMP;
ALTER TABLE jobs ADD COLUMN vector_doc_count INTEGER DEFAULT 0;

CREATE INDEX IF NOT EXISTS idx_jobs_fingerprint ON jobs(job_fingerprint);
CREATE INDEX IF NOT EXISTS idx_jobs_rag_processed ON jobs(rag_processed);
CREATE INDEX IF NOT EXISTS idx_jobs_rag_processed_at ON jobs(rag_processed_at);

-- æ›´æ–°åçš„jobsè¡¨ç»“æ„
CREATE TABLE jobs (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    job_id VARCHAR(100) UNIQUE NOT NULL,
    title VARCHAR(200) NOT NULL,
    company VARCHAR(200) NOT NULL,
    url VARCHAR(500) NOT NULL,
    job_fingerprint VARCHAR(12) UNIQUE,  -- èŒä½æŒ‡çº¹ï¼ˆå»é‡ç”¨ï¼‰
    application_status VARCHAR(50) DEFAULT 'pending',
    match_score FLOAT,
    semantic_score FLOAT,
    vector_id VARCHAR(100),
    structured_data TEXT,
    website VARCHAR(50) NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    submitted_at TIMESTAMP,
    rag_processed BOOLEAN DEFAULT FALSE,  -- RAGå¤„ç†çŠ¶æ€
    rag_processed_at TIMESTAMP,          -- RAGå¤„ç†æ—¶é—´
    vector_doc_count INTEGER DEFAULT 0   -- ç”Ÿæˆçš„å‘é‡æ–‡æ¡£æ•°é‡
);
```

#### ğŸ”„ çˆ¬å–æµç¨‹ä¼˜åŒ–

**ä¼˜åŒ–å‰æµç¨‹**:
1. è§£æåˆ—è¡¨é¡µ â†’ 2. é€ä¸ªç‚¹å‡»èŒä½ â†’ 3. çˆ¬å–è¯¦æƒ…é¡µ â†’ 4. ä¿å­˜æ•°æ®

**ä¼˜åŒ–åæµç¨‹**:
1. è§£æåˆ—è¡¨é¡µ â†’ 2. ç”ŸæˆèŒä½æŒ‡çº¹ â†’ 3. æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ â†’ 4. ä»…çˆ¬å–æ–°èŒä½

#### ğŸ“Š æ€§èƒ½æå‡é¢„æœŸ

- **é‡å¤æ£€æµ‹å‡†ç¡®ç‡**: >95% (åŸºäºæ ‡é¢˜+å…¬å¸çš„ç»„åˆå”¯ä¸€æ€§)
- **çˆ¬å–æ•ˆç‡æå‡**: 50-80% (è·³è¿‡é‡å¤èŒä½çš„è¯¦æƒ…é¡µè®¿é—®)
- **æ•°æ®åº“æŸ¥è¯¢æ€§èƒ½**: <1ms (åŸºäºç´¢å¼•çš„æŒ‡çº¹æŸ¥è¯¢)
- **å­˜å‚¨ç©ºé—´èŠ‚çœ**: é¿å…é‡å¤æ•°æ®å­˜å‚¨

#### ğŸ› ï¸ å®æ–½è®¡åˆ’

**é˜¶æ®µ1: æ•°æ®åº“æ¨¡å¼æ›´æ–°**
- æ›´æ–° `models.py` æ·»åŠ  `job_fingerprint` å­—æ®µ
- åˆ›å»ºæ•°æ®åº“è¿ç§»è„šæœ¬
- æ·»åŠ æŒ‡çº¹ç›¸å…³çš„æ•°æ®åº“æ“ä½œæ–¹æ³•

**é˜¶æ®µ2: æŒ‡çº¹ç”Ÿæˆé›†æˆ**
- åœ¨ `PageParser._parse_job_element()` ä¸­é›†æˆæŒ‡çº¹ç”Ÿæˆ
- åœ¨ `ContentExtractor` ä¸­æ·»åŠ å»é‡æ£€æŸ¥é€»è¾‘
- å®ç°æŒ‡çº¹å­˜å‚¨å’ŒæŸ¥è¯¢æ¥å£

**é˜¶æ®µ3: å­˜å‚¨æ–¹å¼æ”¹é€ **
- å°†JSONæ–‡ä»¶ä¿å­˜æ”¹ä¸ºç›´æ¥æ•°æ®åº“å­˜å‚¨
- ä¼˜åŒ–æ•°æ®åº“æ‰¹é‡æ’å…¥æ€§èƒ½
- æ·»åŠ æ•°æ®å®Œæ•´æ€§æ£€æŸ¥

**é˜¶æ®µ4: æµ‹è¯•éªŒè¯**
- åˆ›å»ºå»é‡åŠŸèƒ½æµ‹è¯•ç”¨ä¾‹
- éªŒè¯æŒ‡çº¹ç®—æ³•çš„å‡†ç¡®æ€§
- æ€§èƒ½åŸºå‡†æµ‹è¯•

#### ğŸ’¡ æŠ€æœ¯ç‰¹æ€§

**æ™ºèƒ½æŒ‡çº¹ç”Ÿæˆ**:
- åŸºäºæ ¸å¿ƒä¿¡æ¯ï¼ˆæ ‡é¢˜+å…¬å¸ï¼‰ç¡®ä¿å”¯ä¸€æ€§
- æ ‡å‡†åŒ–å¤„ç†é¿å…æ ¼å¼å·®å¼‚å½±å“
- 12ä½å“ˆå¸Œé•¿åº¦å¹³è¡¡å­˜å‚¨å’Œå†²çªç‡

**é«˜æ•ˆå»é‡æ£€æŸ¥**:
- åˆ—è¡¨é¡µå³å¯åˆ¤æ–­ï¼Œé¿å…ä¸å¿…è¦çš„ç‚¹å‡»
- æ•°æ®åº“ç´¢å¼•ä¼˜åŒ–ï¼Œæ¯«ç§’çº§æŸ¥è¯¢
- æ‰¹é‡æ£€æŸ¥æ”¯æŒï¼Œæå‡å¤„ç†æ•ˆç‡

**æ•°æ®ä¸€è‡´æ€§**:
- å”¯ä¸€çº¦æŸç¡®ä¿æŒ‡çº¹ä¸é‡å¤
- äº‹åŠ¡å¤„ç†ä¿è¯æ•°æ®å®Œæ•´æ€§
- é”™è¯¯æ¢å¤æœºåˆ¶å¤„ç†å¼‚å¸¸æƒ…å†µ

#### ğŸ¯ é…ç½®ç¤ºä¾‹

```yaml
# å»é‡ç³»ç»Ÿé…ç½®
deduplication:
  enabled: true
  fingerprint_algorithm: "md5"
  fingerprint_length: 12
  check_batch_size: 100
  
  # æŒ‡çº¹ç”Ÿæˆé…ç½®
  fingerprint_fields:
    title: true          # èŒä½æ ‡é¢˜ï¼ˆå¿…éœ€ï¼‰
    company: true        # å…¬å¸åç§°ï¼ˆå¿…éœ€ï¼‰
    salary: true         # è–ªèµ„ä¿¡æ¯ï¼ˆå¯é€‰ï¼‰
    location: true       # å·¥ä½œåœ°ç‚¹ï¼ˆå¯é€‰ï¼‰
  
  # æ ‡å‡†åŒ–é…ç½®
  normalization:
    lowercase: true      # è½¬æ¢ä¸ºå°å†™
    remove_spaces: true  # ç§»é™¤ç©ºæ ¼
    remove_punctuation: false  # ä¿ç•™æ ‡ç‚¹ç¬¦å·
```

#### ğŸ“ˆ é¢„æœŸæ•ˆæœ

**æ•ˆç‡æå‡**:
- å‡å°‘50-80%çš„é‡å¤è¯¦æƒ…é¡µè®¿é—®
- é™ä½ç½‘ç»œè¯·æ±‚å’Œé¡µé¢åŠ è½½æ—¶é—´
- æå‡æ•´ä½“çˆ¬å–é€Ÿåº¦

**æ•°æ®è´¨é‡**:
- é¿å…é‡å¤æ•°æ®å­˜å‚¨
- ä¿æŒæ•°æ®åº“æ•´æ´
- æå‡åç»­åˆ†æå‡†ç¡®æ€§

**ç³»ç»Ÿç¨³å®šæ€§**:
- å‡å°‘ä¸å¿…è¦çš„ç½‘ç»œè¯·æ±‚
- é™ä½è¢«åçˆ¬æ£€æµ‹çš„é£é™©
- æå‡ç³»ç»Ÿæ•´ä½“ç¨³å®šæ€§

#### ğŸ” ä½¿ç”¨ç¤ºä¾‹

```python
# åœ¨ContentExtractorä¸­çš„ä½¿ç”¨
class ContentExtractor:
    def _process_job_list(self, job_elements):
        """å¤„ç†èŒä½åˆ—è¡¨ï¼Œé›†æˆå»é‡æ£€æŸ¥"""
        new_jobs = []
        skipped_count = 0
        
        for job_element in job_elements:
            # è§£æåŸºæœ¬ä¿¡æ¯
            job_data = self.page_parser._parse_job_element(job_element)
            
            # ç”ŸæˆæŒ‡çº¹
            fingerprint = generate_job_fingerprint(
                job_data['title'],
                job_data['company'],
                job_data.get('salary', ''),
                job_data.get('location', '')
            )
            
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            if self.data_storage.fingerprint_exists(fingerprint):
                skipped_count += 1
                self.logger.info(f"è·³è¿‡é‡å¤èŒä½: {job_data['title']} - {job_data['company']}")
                continue
            
            # æ·»åŠ æŒ‡çº¹ä¿¡æ¯
            job_data['job_fingerprint'] = fingerprint
            new_jobs.append(job_data)
        
        self.logger.info(f"å»é‡ç»“æœ: æ–°èŒä½ {len(new_jobs)} ä¸ªï¼Œè·³è¿‡é‡å¤ {skipped_count} ä¸ª")
        return new_jobs
```

è¿™ä¸ªæ™ºèƒ½å»é‡ç³»ç»Ÿå°†æ˜¾è‘—æå‡çˆ¬å–æ•ˆç‡ï¼Œå‡å°‘é‡å¤å·¥ä½œï¼ŒåŒæ—¶ä¿æŒæ•°æ®è´¨é‡å’Œç³»ç»Ÿç¨³å®šæ€§ã€‚

## ğŸ¤– RAGç³»ç»Ÿæ•°æ®æŠ½å–åˆ†æä¸å®ç°æ–¹æ¡ˆ (2025-01-20)

### ğŸ“Š æ•°æ®åº“ç»“æ„åˆ†æ

åŸºäºç°æœ‰ä»£ç åˆ†æï¼Œå½“å‰ç³»ç»Ÿå…·å¤‡å®Œæ•´çš„æ•°æ®å­˜å‚¨åŸºç¡€ï¼š

#### ä¸»è¦æ•°æ®è¡¨ç»“æ„

**jobsè¡¨** - å­˜å‚¨èŒä½åŸºæœ¬ä¿¡æ¯ï¼š
```sql
CREATE TABLE jobs (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    job_id VARCHAR(100) UNIQUE NOT NULL,
    title VARCHAR(200) NOT NULL,
    company VARCHAR(200) NOT NULL,
    url VARCHAR(500) NOT NULL,
    job_fingerprint VARCHAR(12) UNIQUE,
    application_status VARCHAR(50) DEFAULT 'pending',
    match_score FLOAT,
    website VARCHAR(50) NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    submitted_at TIMESTAMP
)
```

**job_detailsè¡¨** - å­˜å‚¨èŒä½è¯¦ç»†ä¿¡æ¯ï¼š
```sql
CREATE TABLE job_details (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    job_id VARCHAR(100) NOT NULL,
    salary TEXT,
    location TEXT,
    experience TEXT,
    education TEXT,
    description TEXT,        -- èŒä½æè¿°ï¼ˆRAGæ ¸å¿ƒæ•°æ®ï¼‰
    requirements TEXT,       -- èŒä½è¦æ±‚ï¼ˆRAGæ ¸å¿ƒæ•°æ®ï¼‰
    benefits TEXT,
    publish_time TEXT,
    company_scale TEXT,
    industry TEXT,
    keyword TEXT,
    extracted_at TEXT,
    FOREIGN KEY (job_id) REFERENCES jobs (job_id)
)
```

#### æ•°æ®å®Œæ•´æ€§è¯„ä¼°

âœ… **æ•°æ®æºç¡®è®¤**ï¼š
- æ•°æ®åº“ä¸­åŒ…å«å°‘é‡æµ‹è¯•èŒä½æ•°æ®
- åŒ…å«å®Œæ•´çš„èŒä½æè¿°å’Œè¦æ±‚ä¿¡æ¯
- å…·å¤‡ä¹‹å‰JSONæ–‡ä»¶éœ€è¦çš„æ‰€æœ‰ä¿¡æ¯å­—æ®µ
- æ•°æ®ç»“æ„é€‚åˆRAGç³»ç»Ÿå¤„ç†

### ğŸ—ï¸ RAGç³»ç»Ÿæ¶æ„è®¾è®¡

#### ç³»ç»Ÿç»„ä»¶æ¶æ„å›¾

```mermaid
graph TB
    subgraph "æ•°æ®å±‚"
        DB[(SQLiteæ•°æ®åº“)]
        VDB[(ChromaDBå‘é‡æ•°æ®åº“)]
    end
    
    subgraph "æ•°æ®æŠ½å–å±‚"
        DBReader[æ•°æ®åº“è¯»å–å™¨]
        DataProcessor[æ•°æ®å¤„ç†å™¨]
        VectorImporter[å‘é‡å¯¼å…¥å™¨]
    end
    
    subgraph "RAGæ ¸å¿ƒå±‚"
        RAGCoordinator[RAGåè°ƒå™¨]
        JobProcessor[èŒä½å¤„ç†å™¨]
        VectorManager[å‘é‡ç®¡ç†å™¨]
        DocumentCreator[æ–‡æ¡£åˆ›å»ºå™¨]
    end
    
    subgraph "åº”ç”¨å±‚"
        JobMatcher[èŒä½åŒ¹é…å™¨]
        ResumeOptimizer[ç®€å†ä¼˜åŒ–å™¨]
        JobQA[èŒä½é—®ç­”ç³»ç»Ÿ]
    end
    
    subgraph "æ¥å£å±‚"
        API[ç»Ÿä¸€APIæ¥å£]
        CLI[å‘½ä»¤è¡Œæ¥å£]
    end
    
    DB --> DBReader
    DBReader --> DataProcessor
    DataProcessor --> VectorImporter
    VectorImporter --> VDB
    
    VDB --> RAGCoordinator
    RAGCoordinator --> JobProcessor
    RAGCoordinator --> VectorManager
    RAGCoordinator --> DocumentCreator
    
    RAGCoordinator --> JobMatcher
    RAGCoordinator --> ResumeOptimizer
    RAGCoordinator --> JobQA
    
    JobMatcher --> API
    ResumeOptimizer --> API
    JobQA --> API
    API --> CLI
```

### ğŸ”§ æ ¸å¿ƒç»„ä»¶è®¾è®¡

#### 1. æ•°æ®åº“èŒä½è¯»å–å™¨ (DatabaseJobReader)

**åŠŸèƒ½èŒè´£**ï¼š
- ä»SQLiteæ•°æ®åº“è¯»å–èŒä½æ•°æ®
- æ”¯æŒæ‰¹é‡è¯»å–å’Œå¢é‡æ›´æ–°
- æ•°æ®é¢„å¤„ç†å’Œæ¸…æ´—
- åˆå¹¶ä¸»è¡¨å’Œè¯¦æƒ…è¡¨æ•°æ®

**æ¥å£è®¾è®¡**ï¼š
```python
class DatabaseJobReader:
    """æ•°æ®åº“èŒä½æ•°æ®è¯»å–å™¨"""
    
    def __init__(self, db_path: str, config: Dict = None):
        self.db_manager = DatabaseManager(db_path)
        self.config = config or {}
    
    def read_all_jobs(self) -> List[Dict]:
        """è¯»å–æ‰€æœ‰èŒä½æ•°æ®"""
        
    def read_jobs_by_batch(self, batch_size: int = 100) -> Iterator[List[Dict]]:
        """æ‰¹é‡è¯»å–èŒä½æ•°æ®"""
        
    def read_new_jobs(self, since: datetime) -> List[Dict]:
        """è¯»å–æŒ‡å®šæ—¶é—´åçš„æ–°èŒä½"""
        
    def get_job_with_details(self, job_id: str) -> Optional[Dict]:
        """è·å–åŒ…å«è¯¦ç»†ä¿¡æ¯çš„å®Œæ•´èŒä½æ•°æ®"""
        
    def get_jobs_for_rag_processing(self, limit: int = None) -> List[Dict]:
        """è·å–éœ€è¦RAGå¤„ç†çš„èŒä½æ•°æ®ï¼ˆæœªå¤„ç†çš„èŒä½ï¼‰"""
        
    def get_unprocessed_jobs(self, batch_size: int = 100) -> Iterator[List[Dict]]:
        """æ‰¹é‡è·å–æœªè¿›è¡ŒRAGå¤„ç†çš„èŒä½"""
        
    def mark_job_as_processed(self, job_id: str, doc_count: int = 0) -> bool:
        """æ ‡è®°èŒä½ä¸ºå·²RAGå¤„ç†"""
        
    def get_rag_processing_stats(self) -> Dict[str, int]:
        """è·å–RAGå¤„ç†ç»Ÿè®¡ä¿¡æ¯"""
        
    def reset_rag_processing_status(self, job_ids: List[str] = None) -> int:
        """é‡ç½®RAGå¤„ç†çŠ¶æ€ï¼ˆç”¨äºé‡æ–°å¤„ç†ï¼‰"""
```

#### ä¼˜åŒ–çš„JobProcessorè®¾è®¡

**æ ¸å¿ƒæ€è·¯**ï¼š
- æ•°æ®åº“ä¸­çš„åŸºæœ¬å­—æ®µï¼ˆtitleã€companyã€locationã€experienceã€educationç­‰ï¼‰ç›´æ¥æ˜ å°„
- ä½¿ç”¨LLMå¤„ç†éœ€è¦æ™ºèƒ½è§£æçš„å­—æ®µï¼šdescription â†’ responsibilitiesã€requirementsã€skills + salary â†’ salary_minã€salary_max
- å¹³è¡¡å¤„ç†æ•ˆç‡å’Œå‡†ç¡®æ€§

**ä¼˜åŒ–åçš„JobProcessor**ï¼š
```python
class OptimizedJobProcessor:
    """ä¼˜åŒ–çš„èŒä½å¤„ç†å™¨ - æ··åˆå¤„ç†æ¨¡å¼"""
    
    def __init__(self, llm_config: Dict = None, config: Dict = None):
        self.config = config or {}
        self.llm_config = llm_config or {}
        
        # åˆå§‹åŒ–LLM
        provider = self.llm_config.get('provider', 'zhipu')
        self.llm = create_llm(
            provider=provider,
            model=self.llm_config.get('model', 'glm-4-flash'),
            temperature=self.llm_config.get('temperature', 0.1),
            max_tokens=self.llm_config.get('max_tokens', 1500),
            **{k: v for k, v in self.llm_config.items() if k not in ['provider', 'model', 'temperature', 'max_tokens']}
        )
        
        # æ„å»ºæ™ºèƒ½æå–é“¾
        self.smart_extraction_chain = self._build_smart_extraction_chain()
        
        logger.info(f"ä¼˜åŒ–èŒä½å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆï¼Œä½¿ç”¨æä¾›å•†: {provider}")
    
    def _build_smart_extraction_chain(self):
        """æ„å»ºæ™ºèƒ½æå–é“¾ - å¤„ç†descriptionå’Œsalary"""
        
        prompt_template = """
ä½ æ˜¯ä¸“ä¸šçš„HRæ•°æ®åˆ†æå¸ˆã€‚è¯·åˆ†æä»¥ä¸‹èŒä½ä¿¡æ¯ï¼Œæå–å²—ä½èŒè´£ã€äººå‘˜è¦æ±‚ã€æŠ€èƒ½è¦æ±‚å’Œè–ªèµ„ä¿¡æ¯ã€‚

èŒä½ä¿¡æ¯ï¼š
æ ‡é¢˜ï¼š{job_title}
å…¬å¸ï¼š{company}
è–ªèµ„ï¼š{salary_text}
èŒä½æè¿°ï¼š{description_text}
èŒä½è¦æ±‚ï¼š{requirements_text}

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–å†…å®¹ï¼š

{{
    "responsibilities": ["èŒè´£1", "èŒè´£2", "èŒè´£3"],
    "requirements": ["è¦æ±‚1", "è¦æ±‚2", "è¦æ±‚3"],
    "skills": ["æŠ€èƒ½1", "æŠ€èƒ½2", "æŠ€èƒ½3"],
    "salary_min": æœ€ä½è–ªèµ„æ•°å­—æˆ–null,
    "salary_max": æœ€é«˜è–ªèµ„æ•°å­—æˆ–null
}}

æå–è¦æ±‚ï¼š
1. ä»èŒä½æè¿°ä¸­æå–å²—ä½èŒè´£åˆ°responsibilitiesæ•°ç»„
2. ä»èŒä½è¦æ±‚ä¸­æå–äººå‘˜è¦æ±‚åˆ°requirementsæ•°ç»„
3. è¯†åˆ«æ‰€æœ‰æŠ€æœ¯æŠ€èƒ½ã€å·¥å…·ã€ç¼–ç¨‹è¯­è¨€ç­‰åˆ°skillsæ•°ç»„
4. ä»è–ªèµ„æ–‡æœ¬ä¸­æå–æ•°å­—èŒƒå›´ï¼š
   - "10k-15k" â†’ salary_min: 10000, salary_max: 15000
   - "é¢è®®" â†’ salary_min: null, salary_max: null
   - "15000ä»¥ä¸Š" â†’ salary_min: 15000, salary_max: null
   - "8000-12000å…ƒ/æœˆ" â†’ salary_min: 8000, salary_max: 12000
5. æ¯ä¸ªæ•°ç»„åŒ…å«3-8ä¸ªå…·ä½“æ˜ç¡®çš„æ¡ç›®
6. åªè¾“å‡ºJSONï¼Œä¸è¦å…¶ä»–è§£é‡Šæ–‡å­—

JSONè¾“å‡ºï¼š
"""
        
        prompt = PromptTemplate(
            template=prompt_template,
            input_variables=["job_title", "company", "salary_text", "description_text", "requirements_text"]
        )
        
        return prompt | self.llm | StrOutputParser()
    
    async def process_database_job(self, db_record: Dict) -> JobStructure:
        """
        å¤„ç†æ•°æ®åº“èŒä½è®°å½• - æ··åˆå¤„ç†æ¨¡å¼
        
        Args:
            db_record: æ•°æ®åº“è®°å½•ï¼ˆåŒ…å«jobså’Œjob_detailsçš„åˆå¹¶æ•°æ®ï¼‰
            
        Returns:
            JobStructure: ç»“æ„åŒ–çš„èŒä½ä¿¡æ¯
        """
        try:
            # 1. ç›´æ¥æ˜ å°„åŸºæœ¬å­—æ®µï¼ˆæ— éœ€LLMå¤„ç†ï¼‰
            job_data = {
                'job_title': db_record.get('title', ''),
                'company': db_record.get('company', ''),
                'location': db_record.get('location', ''),
                'education': db_record.get('education', 'ä¸é™'),
                'experience': db_record.get('experience', 'ä¸é™'),
                'company_size': db_record.get('company_scale', ''),
            }
            
            # 2. ä½¿ç”¨LLMå¤„ç†éœ€è¦æ™ºèƒ½è§£æçš„å­—æ®µ
            llm_result = await self.smart_extraction_chain.ainvoke({
                "job_title": db_record.get('title', ''),
                "company": db_record.get('company', ''),
                "salary_text": db_record.get('salary', ''),
                "description_text": db_record.get('description', ''),
                "requirements_text": db_record.get('requirements', '')
            })
            
            # 3. è§£æLLMç»“æœå¹¶åˆå¹¶
            extracted_data = self._parse_llm_result(llm_result)
            job_data.update(extracted_data)
            
            # 4. åˆ›å»ºJobStructureå¯¹è±¡
            job_structure = JobStructure(**job_data)
            
            logger.info(f"æˆåŠŸå¤„ç†èŒä½: {job_structure.job_title} - {job_structure.company}")
            return job_structure
            
        except Exception as e:
            logger.error(f"å¤„ç†èŒä½å¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ: {e}")
            return self._fallback_extraction_from_db(db_record)
    
    def _parse_llm_result(self, llm_result: str) -> Dict:
        """è§£æLLMè¿”å›çš„ç»“æœ"""
        try:
            # æ¸…ç†ç»“æœå­—ç¬¦ä¸²
            cleaned_result = llm_result.strip()
            
            # æå–JSONéƒ¨åˆ†
            if "```json" in cleaned_result:
                start = cleaned_result.find("```json") + 7
                end = cleaned_result.find("```", start)
                if end != -1:
                    cleaned_result = cleaned_result[start:end].strip()
            elif "```" in cleaned_result:
                start = cleaned_result.find("```") + 3
                end = cleaned_result.find("```", start)
                if end != -1:
                    cleaned_result = cleaned_result[start:end].strip()
            
            # è§£æJSON
            parsed = json.loads(cleaned_result)
            
            # éªŒè¯å’Œæ¸…ç†å­—æ®µ
            result = {
                'responsibilities': parsed.get('responsibilities', []),
                'requirements': parsed.get('requirements', []),
                'skills': parsed.get('skills', []),
                'salary_min': parsed.get('salary_min'),
                'salary_max': parsed.get('salary_max')
            }
            
            # ç¡®ä¿åˆ—è¡¨å­—æ®µéƒ½æ˜¯åˆ—è¡¨
            for key in ['responsibilities', 'requirements', 'skills']:
                if not isinstance(result[key], list):
                    result[key] = []
            
            # ç¡®ä¿è–ªèµ„å­—æ®µæ˜¯æ•°å­—æˆ–None
            for key in ['salary_min', 'salary_max']:
                if result[key] is not None:
                    try:
                        result[key] = int(result[key])
                    except (ValueError, TypeError):
                        result[key] = None
            
            return result
            
        except Exception as e:
            logger.error(f"LLMç»“æœè§£æå¤±è´¥: {e}")
            return {
                'responsibilities': [],
                'requirements': [],
                'skills': [],
                'salary_min': None,
                'salary_max': None
            }
    
    def _fallback_extraction_from_db(self, db_record: Dict) -> JobStructure:
        """ä»æ•°æ®åº“è®°å½•çš„å¤‡ç”¨æå–æ–¹æ¡ˆ"""
        return JobStructure(
            job_title=db_record.get('title', 'æœªçŸ¥èŒä½'),
            company=db_record.get('company', 'æœªçŸ¥å…¬å¸'),
            location=db_record.get('location'),
            education=db_record.get('education', 'ä¸é™'),
            experience=db_record.get('experience', 'ä¸é™'),
            company_size=db_record.get('company_scale'),
            responsibilities=[],
            requirements=[],
            skills=[],
            salary_min=None,
            salary_max=None
        )
```

#### å¤„ç†å­—æ®µåˆ†ç±»

**ç›´æ¥æ˜ å°„å­—æ®µ**ï¼ˆæ— éœ€LLMï¼‰ï¼š
- `title` â†’ `job_title`
- `company` â†’ `company`
- `location` â†’ `location`
- `education` â†’ `education`
- `experience` â†’ `experience`
- `company_scale` â†’ `company_size`

**LLMæ™ºèƒ½å¤„ç†å­—æ®µ**ï¼š
- `description` + `requirements` â†’ `responsibilities`ã€`requirements`ã€`skills`
- `salary` â†’ `salary_min`ã€`salary_max`

**æ€§èƒ½ä¼˜åŒ–æ•ˆæœ**ï¼š
- **å‡å°‘LLMå¤„ç†å­—æ®µ**ï¼šä»11ä¸ªå­—æ®µå‡å°‘åˆ°5ä¸ªå­—æ®µ
- **æå‡å¤„ç†å‡†ç¡®æ€§**ï¼šåŸºæœ¬ä¿¡æ¯ç›´æ¥æ˜ å°„ï¼Œé¿å…LLMè§£æé”™è¯¯
- **ä¿æŒæ™ºèƒ½è§£æ**ï¼šå¤æ‚å­—æ®µï¼ˆèŒè´£ã€æŠ€èƒ½ã€è–ªèµ„ï¼‰ä»ä½¿ç”¨LLMå¤„ç†
- **é™ä½æˆæœ¬**ï¼šå‡å°‘çº¦50%çš„LLM tokenæ¶ˆè€—

#### 2. RAGç³»ç»Ÿåè°ƒå™¨ (RAGSystemCoordinator)

**åŠŸèƒ½èŒè´£**ï¼š
- ç»Ÿä¸€ç®¡ç†RAGç³»ç»Ÿå„ç»„ä»¶
- åè°ƒæ•°æ®æµå’Œå¤„ç†æµç¨‹
- æä¾›ç»Ÿä¸€çš„APIæ¥å£
- ç®¡ç†ç³»ç»Ÿé…ç½®å’ŒçŠ¶æ€

**æ¥å£è®¾è®¡**ï¼š
```python
class RAGSystemCoordinator:
    """RAGç³»ç»Ÿåè°ƒå™¨"""
    
    def __init__(self, config: Dict):
        self.config = config
        self.db_reader = DatabaseJobReader(config['database']['path'])
        self.job_processor = LangChainJobProcessor(config['llm'])
        self.vector_manager = ChromaDBManager(config['vector_db'])
        self.document_creator = DocumentCreator(config.get('documents', {}))
    
    def initialize_system(self) -> bool:
        """åˆå§‹åŒ–RAGç³»ç»Ÿ"""
        
    def import_database_jobs(self, batch_size: int = 100, force_reprocess: bool = False) -> Dict[str, int]:
        """ä»æ•°æ®åº“å¯¼å…¥èŒä½æ•°æ®åˆ°å‘é‡æ•°æ®åº“"""
        
    def process_single_job(self, job_data: Dict) -> bool:
        """å¤„ç†å•ä¸ªèŒä½æ•°æ®"""
        
    def get_processing_progress(self) -> Dict[str, Any]:
        """è·å–RAGå¤„ç†è¿›åº¦"""
        
    def resume_processing(self, batch_size: int = 100) -> Dict[str, int]:
        """æ¢å¤ä¸­æ–­çš„RAGå¤„ç†"""
        
    def match_jobs(self, resume: Dict, top_k: int = 10) -> List[Dict]:
        """æ™ºèƒ½èŒä½åŒ¹é…"""
        
    def optimize_resume(self, resume: Dict, target_jobs: List[str]) -> Dict:
        """ç®€å†ä¼˜åŒ–å»ºè®®"""
        
    def query_jobs(self, question: str, filters: Dict = None) -> str:
        """èŒä½æ™ºèƒ½é—®ç­”"""
        
    def get_system_status(self) -> Dict:
        """è·å–ç³»ç»ŸçŠ¶æ€"""
```

#### 3. æ™ºèƒ½åº”ç”¨ç»„ä»¶

**èŒä½åŒ¹é…å™¨ (IntelligentJobMatcher)**ï¼š
```python
class IntelligentJobMatcher:
    """æ™ºèƒ½èŒä½åŒ¹é…å™¨"""
    
    def __init__(self, vector_manager: ChromaDBManager, config: Dict):
        self.vector_manager = vector_manager
        self.config = config
    
    def match_by_skills(self, skills: List[str], top_k: int = 10) -> List[Dict]:
        """åŸºäºæŠ€èƒ½åŒ¹é…èŒä½"""
        
    def match_by_experience(self, experience: str, top_k: int = 10) -> List[Dict]:
        """åŸºäºç»éªŒåŒ¹é…èŒä½"""
        
    def comprehensive_match(self, user_profile: Dict, top_k: int = 10) -> List[Dict]:
        """ç»¼åˆåŒ¹é…åˆ†æ"""
        
    def explain_match(self, job_id: str, user_profile: Dict) -> Dict:
        """è§£é‡ŠåŒ¹é…åŸå› """
```

**ç®€å†ä¼˜åŒ–å™¨ (ResumeOptimizer)**ï¼š
```python
class ResumeOptimizer:
    """ç®€å†ä¼˜åŒ–å™¨"""
    
    def __init__(self, rag_coordinator: RAGSystemCoordinator):
        self.rag_coordinator = rag_coordinator
    
    def analyze_resume_gaps(self, resume: Dict, target_jobs: List[str]) -> Dict:
        """åˆ†æç®€å†ä¸ç›®æ ‡èŒä½çš„å·®è·"""
        
    def suggest_skill_improvements(self, resume: Dict, market_data: Dict) -> List[str]:
        """å»ºè®®æŠ€èƒ½æå‡æ–¹å‘"""
        
    def optimize_resume_content(self, resume: Dict, target_job: str) -> Dict:
        """ä¼˜åŒ–ç®€å†å†…å®¹"""
        
    def generate_cover_letter(self, resume: Dict, job_id: str) -> str:
        """ç”Ÿæˆæ±‚èŒä¿¡"""
```

**èŒä½é—®ç­”ç³»ç»Ÿ (JobQASystem)**ï¼š
```python
class JobQASystem:
    """èŒä½æ™ºèƒ½é—®ç­”ç³»ç»Ÿ"""
    
    def __init__(self, rag_coordinator: RAGSystemCoordinator):
        self.rag_coordinator = rag_coordinator
        self.qa_chain = self._build_qa_chain()
    
    def ask_about_job(self, job_id: str, question: str) -> str:
        """è¯¢é—®ç‰¹å®šèŒä½ä¿¡æ¯"""
        
    def ask_about_market(self, question: str, filters: Dict = None) -> str:
        """è¯¢é—®å¸‚åœºè¶‹åŠ¿ä¿¡æ¯"""
        
    def compare_jobs(self, job_ids: List[str], criteria: str) -> str:
        """æ¯”è¾ƒå¤šä¸ªèŒä½"""
        
    def get_job_insights(self, job_id: str) -> Dict:
        """è·å–èŒä½æ·±åº¦æ´å¯Ÿ"""
```

### ğŸ“‹ æ•°æ®æŠ½å–æµç¨‹è®¾è®¡

#### æ•°æ®æŠ½å–æ­¥éª¤

```mermaid
graph TD
    A[è¯»å–æ•°æ®åº“æ•°æ®] --> B[æ•°æ®é¢„å¤„ç†]
    B --> C[åˆå¹¶ä¸»è¡¨å’Œè¯¦æƒ…è¡¨]
    C --> D[æ•°æ®æ¸…æ´—å’ŒéªŒè¯]
    D --> E[ç»“æ„åŒ–å¤„ç†]
    E --> F[åˆ›å»ºæ–‡æ¡£å¯¹è±¡]
    F --> G[å‘é‡åŒ–å¤„ç†]
    G --> H[å­˜å‚¨åˆ°ChromaDB]
    H --> I[æ›´æ–°å¤„ç†çŠ¶æ€]
    
    style A fill:#e1f5fe
    style E fill:#f3e5f5
    style G fill:#e8f5e8
    style H fill:#fff3e0
```

#### æ•°æ®å¤„ç†æµæ°´çº¿

```python
async def extract_and_import_jobs(coordinator: RAGSystemCoordinator, batch_size: int = 50, force_reprocess: bool = False):
    """æ•°æ®æŠ½å–å’Œå¯¼å…¥æµæ°´çº¿ï¼ˆæ”¯æŒå¢é‡å¤„ç†ï¼‰"""
    
    # 1. è·å–å¤„ç†ç»Ÿè®¡
    db_reader = coordinator.db_reader
    stats = db_reader.get_rag_processing_stats()
    logger.info(f"RAGå¤„ç†ç»Ÿè®¡: æ€»è®¡ {stats['total']} ä¸ªèŒä½ï¼Œå·²å¤„ç† {stats['processed']} ä¸ªï¼Œå¾…å¤„ç† {stats['unprocessed']} ä¸ª")
    
    total_imported = 0
    total_skipped = 0
    total_errors = 0
    
    # 2. æ‰¹é‡å¤„ç†æœªå¤„ç†çš„èŒä½
    batch_iterator = db_reader.get_unprocessed_jobs(batch_size) if not force_reprocess else db_reader.read_jobs_by_batch(batch_size)
    
    for batch in batch_iterator:
        batch_results = []
        
        for job_data in batch:
            try:
                # 3. æ£€æŸ¥æ˜¯å¦éœ€è¦è·³è¿‡ï¼ˆéå¼ºåˆ¶é‡å¤„ç†æ¨¡å¼ï¼‰
                if not force_reprocess and job_data.get('rag_processed'):
                    total_skipped += 1
                    continue
                
                # 4. ç»“æ„åŒ–å¤„ç†
                job_structure = await coordinator.job_processor.process_job_data(job_data)
                
                # 5. åˆ›å»ºæ–‡æ¡£
                documents = coordinator.document_creator.create_job_documents(
                    job_structure,
                    job_data['job_id'],
                    job_data.get('url')
                )
                
                # 6. å‘é‡åŒ–å­˜å‚¨
                doc_ids = coordinator.vector_manager.add_job_documents(
                    documents,
                    job_data['job_id']
                )
                
                # 7. æ›´æ–°å¤„ç†çŠ¶æ€ï¼ˆè®°å½•æ–‡æ¡£æ•°é‡ï¼‰
                coordinator.db_reader.mark_job_as_processed(job_data['job_id'], len(documents))
                
                batch_results.append({
                    'job_id': job_data['job_id'],
                    'title': job_data['title'],
                    'documents_created': len(documents),
                    'doc_ids': doc_ids
                })
                
                total_imported += 1
                
            except Exception as e:
                logger.error(f"å¤„ç†èŒä½å¤±è´¥ {job_data.get('job_id', 'unknown')}: {e}")
                total_errors += 1
                continue
        
        # æ‰¹é‡å®Œæˆæ—¥å¿—
        logger.info(f"æ‰¹é‡å¤„ç†å®Œæˆ: å¯¼å…¥ {len(batch_results)} ä¸ªèŒä½")
        
        # å¦‚æœæ²¡æœ‰æ›´å¤šæœªå¤„ç†çš„èŒä½ï¼Œé€€å‡ºå¾ªç¯
        if not batch_results and not force_reprocess:
            logger.info("æ‰€æœ‰èŒä½å·²å¤„ç†å®Œæˆ")
            break
    
    # æœ€ç»ˆç»Ÿè®¡
    final_stats = db_reader.get_rag_processing_stats()
    
    return {
        'total_imported': total_imported,
        'total_skipped': total_skipped,
        'total_errors': total_errors,
        'success_rate': total_imported / (total_imported + total_errors) if (total_imported + total_errors) > 0 else 0,
        'processing_stats': final_stats
    }
```

### âš™ï¸ é…ç½®ç®¡ç†è®¾è®¡

#### RAGç³»ç»Ÿé…ç½®ç»“æ„

```yaml
# RAGç³»ç»Ÿé…ç½® (config/config.yaml æ‰©å±•)
rag_system:
  # æ•°æ®åº“é…ç½®
  database:
    path: "./data/jobs.db"
    batch_size: 100
    enable_incremental: true
    
  # RAGå¤„ç†é…ç½®
  processing:
    skip_processed: true          # è·³è¿‡å·²å¤„ç†çš„èŒä½
    force_reprocess: false        # å¼ºåˆ¶é‡æ–°å¤„ç†æ‰€æœ‰èŒä½
    auto_resume: true             # è‡ªåŠ¨æ¢å¤ä¸­æ–­çš„å¤„ç†
    checkpoint_interval: 50       # æ£€æŸ¥ç‚¹é—´éš”ï¼ˆå¤„ç†å¤šå°‘ä¸ªèŒä½åä¿å­˜è¿›åº¦ï¼‰
    max_retry_attempts: 3         # å•ä¸ªèŒä½æœ€å¤§é‡è¯•æ¬¡æ•°
    
  # LLMé…ç½®
  llm:
    provider: "zhipu"  # zhipu, openai, claude
    model: "glm-4-flash"
    api_key: "${ZHIPU_API_KEY}"
    temperature: 0.1
    max_tokens: 2000
    
  # å‘é‡æ•°æ®åº“é…ç½®
  vector_db:
    persist_directory: "./chroma_db"
    collection_name: "job_positions"
    embeddings:
      model_name: "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
      device: "cpu"
      normalize_embeddings: true
      
  # æ–‡æ¡£åˆ›å»ºé…ç½®
  documents:
    types: ["overview", "responsibility", "requirement", "skills", "basic_requirements"]
    create_comprehensive_doc: false
    max_chunk_size: 500
    chunk_overlap: 50
    
  # åº”ç”¨é…ç½®
  applications:
    job_matching:
      enabled: true
      top_k: 10
      similarity_threshold: 0.7
      use_reranking: true
      
    resume_optimization:
      enabled: true
      max_suggestions: 5
      include_market_analysis: true
      
    job_qa:
      enabled: true
      context_window: 5
      max_response_length: 1000
      
  # æ€§èƒ½é…ç½®
  performance:
    max_concurrent_jobs: 5
    cache_embeddings: true
    batch_vector_operations: true
```

### ğŸš€ å®ç°è®¡åˆ’

#### å¼€å‘é˜¶æ®µè§„åˆ’

**é˜¶æ®µ1ï¼šæ•°æ®æŠ½å–åŸºç¡€è®¾æ–½** (ä¼˜å…ˆçº§ï¼šé«˜)
- [x] åˆ†ææ•°æ®åº“ä¸­çš„èŒä½æ•°æ®ç»“æ„å’Œå†…å®¹
- [ ] åˆ›å»ºDatabaseJobReaderç±»
- [ ] å®ç°æ•°æ®è¯»å–å’Œé¢„å¤„ç†åŠŸèƒ½
- [ ] æµ‹è¯•æ•°æ®æŠ½å–æµç¨‹

**é˜¶æ®µ2ï¼šRAGç³»ç»Ÿæ ¸å¿ƒ** (ä¼˜å…ˆçº§ï¼šé«˜)
- [ ] å®ç°RAGSystemCoordinator
- [ ] é›†æˆç°æœ‰çš„JobProcessorå’ŒVectorManager
- [ ] å®ç°æ‰¹é‡æ•°æ®å¯¼å…¥åŠŸèƒ½
- [ ] æ·»åŠ ç³»ç»ŸçŠ¶æ€ç›‘æ§

**é˜¶æ®µ3ï¼šæ™ºèƒ½åº”ç”¨å¼€å‘** (ä¼˜å…ˆçº§ï¼šä¸­)
- [ ] å¼€å‘IntelligentJobMatcher
- [ ] å¼€å‘ResumeOptimizer
- [ ] å¼€å‘JobQASystem
- [ ] å®ç°åº”ç”¨é—´çš„åè°ƒæœºåˆ¶

**é˜¶æ®µ4ï¼šç³»ç»Ÿé›†æˆä¸ä¼˜åŒ–** (ä¼˜å…ˆçº§ï¼šä¸­)
- [ ] åˆ›å»ºç»Ÿä¸€APIæ¥å£
- [ ] ç¼–å†™æµ‹è¯•ç”¨ä¾‹
- [ ] æ€§èƒ½ä¼˜åŒ–å’Œé”™è¯¯å¤„ç†
- [ ] åˆ›å»ºä½¿ç”¨ç¤ºä¾‹å’Œæ–‡æ¡£

#### æŠ€æœ¯å®ç°è¦ç‚¹

**æ•°æ®å¤„ç†ä¼˜åŒ–**ï¼š
- ä½¿ç”¨æ‰¹é‡å¤„ç†å‡å°‘å†…å­˜å ç”¨
- å®ç°å¢é‡æ›´æ–°æœºåˆ¶ï¼Œé¿å…é‡å¤å¤„ç†
- æ·»åŠ æ•°æ®éªŒè¯å’Œé”™è¯¯æ¢å¤
- æ”¯æŒæ–­ç‚¹ç»­ä¼ åŠŸèƒ½

**æ€§èƒ½ä¼˜åŒ–**ï¼š
- å‘é‡æ£€ç´¢ç¼“å­˜æœºåˆ¶
- å¼‚æ­¥å¤„ç†æ”¯æŒ
- æ•°æ®åº“è¿æ¥æ± ç®¡ç†
- å†…å­˜ä½¿ç”¨ä¼˜åŒ–

**å¯æ‰©å±•æ€§è®¾è®¡**ï¼š
- æ’ä»¶åŒ–æ¶æ„è®¾è®¡
- å¤šç§LLMæä¾›å•†æ”¯æŒ
- çµæ´»çš„é…ç½®ç®¡ç†
- æ¨¡å—åŒ–ç»„ä»¶è®¾è®¡

### ğŸ“Š é¢„æœŸæ•ˆæœ

#### åŠŸèƒ½ç‰¹æ€§
- âœ… **æ™ºèƒ½èŒä½åŒ¹é…**ï¼šåŸºäºè¯­ä¹‰ç†è§£çš„ç²¾å‡†åŒ¹é…ï¼ŒåŒ¹é…å‡†ç¡®ç‡æå‡30%
- âœ… **ç®€å†ä¼˜åŒ–å»ºè®®**ï¼šä¸ªæ€§åŒ–çš„ç®€å†æ”¹è¿›æ–¹æ¡ˆï¼Œæå‡æŠ•é€’æˆåŠŸç‡
- âœ… **èŒä½æ™ºèƒ½é—®ç­”**ï¼šè‡ªç„¶è¯­è¨€æŸ¥è¯¢èŒä½ä¿¡æ¯ï¼Œæ”¯æŒå¤æ‚æŸ¥è¯¢
- âœ… **å®æ—¶æ•°æ®æ›´æ–°**ï¼šæ”¯æŒæ–°èŒä½æ•°æ®çš„è‡ªåŠ¨å¯¼å…¥å’Œå¤„ç†

#### æŠ€æœ¯ä¼˜åŠ¿
- ğŸš€ **é«˜æ€§èƒ½**ï¼šæ‰¹é‡å¤„ç†å’Œå‘é‡æ£€ç´¢ä¼˜åŒ–ï¼Œå¤„ç†é€Ÿåº¦æå‡50%
- ğŸ”§ **æ˜“æ‰©å±•**ï¼šæ¨¡å—åŒ–è®¾è®¡ï¼Œæ”¯æŒåŠŸèƒ½æ‰©å±•å’Œæ¨¡å‹æ›¿æ¢
- ğŸ›¡ï¸ **é«˜å¯é **ï¼šå®Œå–„çš„é”™è¯¯å¤„ç†å’Œæ•°æ®éªŒè¯æœºåˆ¶
- ğŸ“Š **å¯ç›‘æ§**ï¼šè¯¦ç»†çš„æ—¥å¿—å’Œæ€§èƒ½æŒ‡æ ‡ï¼Œä¾¿äºç³»ç»Ÿç›‘æ§

#### ä¸šåŠ¡ä»·å€¼
- **æå‡æŠ•é€’ç²¾å‡†åº¦**ï¼šé€šè¿‡è¯­ä¹‰åŒ¹é…å‡å°‘æ— æ•ˆæŠ•é€’
- **ä¼˜åŒ–æ±‚èŒä½“éªŒ**ï¼šæ™ºèƒ½é—®ç­”å’Œç®€å†ä¼˜åŒ–æå‡ç”¨æˆ·ä½“éªŒ
- **æ•°æ®ä»·å€¼æŒ–æ˜**ï¼šå°†çˆ¬å–æ•°æ®è½¬åŒ–ä¸ºæ™ºèƒ½åº”ç”¨ä»·å€¼
- **ç³»ç»Ÿå¯æŒç»­æ€§**ï¼šå»ºç«‹å¯æŒç»­çš„æ•°æ®å¤„ç†å’Œåº”ç”¨æ¡†æ¶

### ğŸ¯ ä¸‹ä¸€æ­¥è¡ŒåŠ¨

1. **ç«‹å³æ‰§è¡Œ**ï¼šå®Œæˆæ•°æ®åº“å†…å®¹åˆ†æï¼Œç¡®è®¤æ•°æ®è´¨é‡å’Œç»“æ„
2. **ä¼˜å…ˆå¼€å‘**ï¼šDatabaseJobReaderå’Œæ•°æ®æŠ½å–æµç¨‹å®ç°
3. **é€æ­¥å®ç°**ï¼šæŒ‰é˜¶æ®µå®Œæˆå„ä¸ªç»„ä»¶çš„å¼€å‘å’Œé›†æˆ
4. **æŒç»­ä¼˜åŒ–**ï¼šæ ¹æ®ä½¿ç”¨åé¦ˆä¸æ–­æ”¹è¿›ç³»ç»Ÿæ€§èƒ½å’Œç”¨æˆ·ä½“éªŒ

è¿™ä¸ªRAGç³»ç»Ÿå°†æŠŠç°æœ‰çš„çˆ¬è™«æ•°æ®æœ‰æ•ˆè½¬æ¢ä¸ºæ™ºèƒ½åº”ç”¨ï¼Œå®ç°ä»æ•°æ®æ”¶é›†åˆ°æ™ºèƒ½åˆ†æçš„å®Œæ•´é—­ç¯ï¼Œä¸ºç”¨æˆ·æä¾›æ›´ç²¾å‡†ã€æ›´æ™ºèƒ½çš„æ±‚èŒæœåŠ¡ã€‚