#!/usr/bin/env python3
"""
æµ‹è¯•ç‚¹å‡»æ”¹è¿›æ•ˆæœ
éªŒè¯å¤±è´¥jobæ—¥å¿—è®°å½•å’Œå¤šé‡ç‚¹å‡»ç­–ç•¥
"""

import asyncio
import logging
import yaml
import json
import os
from datetime import datetime
from src.integration.master_controller import MasterController, PipelineConfig

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

def check_failed_jobs_log():
    """æ£€æŸ¥å¤±è´¥jobæ—¥å¿—"""
    failed_jobs_file = "logs/failed_jobs.json"
    
    if os.path.exists(failed_jobs_file):
        try:
            with open(failed_jobs_file, 'r', encoding='utf-8') as f:
                failed_jobs = json.load(f)
            
            print(f"\nğŸ“‹ å¤±è´¥jobæ—¥å¿—åˆ†æ ({failed_jobs_file}):")
            print(f"   æ€»å¤±è´¥æ•°é‡: {len(failed_jobs)}")
            
            # æŒ‰å¤±è´¥åŸå› åˆ†ç»„
            failure_reasons = {}
            for job in failed_jobs:
                reason = job.get('reason', 'æœªçŸ¥åŸå› ')
                if reason not in failure_reasons:
                    failure_reasons[reason] = []
                failure_reasons[reason].append(job)
            
            print(f"\nğŸ“Š å¤±è´¥åŸå› åˆ†æ:")
            for reason, jobs in failure_reasons.items():
                print(f"   {reason}: {len(jobs)} ä¸ª")
                # æ˜¾ç¤ºå‰3ä¸ªå¤±è´¥çš„jobè¯¦æƒ…
                for i, job in enumerate(jobs[:3]):
                    print(f"     - {job.get('title', 'N/A')} @ {job.get('company', 'N/A')}")
                if len(jobs) > 3:
                    print(f"     ... è¿˜æœ‰ {len(jobs) - 3} ä¸ª")
            
            return failed_jobs
            
        except Exception as e:
            print(f"âŒ è¯»å–å¤±è´¥jobæ—¥å¿—å‡ºé”™: {e}")
            return []
    else:
        print(f"ğŸ“‹ æš‚æ— å¤±è´¥jobæ—¥å¿—æ–‡ä»¶: {failed_jobs_file}")
        return []

async def test_click_improvements():
    """æµ‹è¯•ç‚¹å‡»æ”¹è¿›æ•ˆæœ"""
    
    print("ğŸš€ å¼€å§‹æµ‹è¯•ç‚¹å‡»æ”¹è¿›æ•ˆæœ")
    print("=" * 60)
    
    # æ¸…ç†ä¹‹å‰çš„å¤±è´¥æ—¥å¿—
    failed_jobs_file = "logs/failed_jobs.json"
    if os.path.exists(failed_jobs_file):
        os.remove(failed_jobs_file)
        print("ğŸ§¹ å·²æ¸…ç†ä¹‹å‰çš„å¤±è´¥jobæ—¥å¿—")
    
    # åŠ è½½é…ç½®
    try:
        with open('config/integration_config.yaml', 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        print("âœ… é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"âŒ é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
        return
    
    # åˆ›å»º MasterController å®ä¾‹
    try:
        controller = MasterController(config)
        print("âœ… MasterController å®ä¾‹åˆ›å»ºæˆåŠŸ")
    except Exception as e:
        print(f"âŒ MasterController åˆ›å»ºå¤±è´¥: {e}")
        return
    
    # åŠ è½½ç®€å†é…ç½®
    try:
        with open('testdata/resume.json', 'r', encoding='utf-8') as f:
            resume_profile = json.load(f)
        print("âœ… ç®€å†é…ç½®åŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"âŒ ç®€å†é…ç½®åŠ è½½å¤±è´¥: {e}")
        return
    
    # é…ç½®æµæ°´çº¿å‚æ•°ï¼ˆåªæµ‹è¯•ç¬¬ä¸€é¡µï¼Œå‡å°‘æµ‹è¯•æ—¶é—´ï¼‰
    pipeline_config = PipelineConfig(
        search_keywords=["Pythonå¼€å‘"],
        search_locations=["ä¸Šæµ·"],
        max_jobs_per_keyword=20,  # åªæµ‹è¯•ç¬¬ä¸€é¡µ
        max_pages=1,              # åªæµ‹è¯•1é¡µ
        resume_profile=resume_profile,
        decision_criteria={
            "min_salary": 15000,
            "preferred_locations": ["ä¸Šæµ·", "åŒ—äº¬"]
        },
        submission_config={
            "dry_run": True,
            "max_submissions": 5
        }
    )
    
    print(f"ğŸ“Š æµ‹è¯•å‚æ•°:")
    print(f"   å…³é”®è¯: {pipeline_config.search_keywords}")
    print(f"   æœ€å¤§èŒä½æ•°: {pipeline_config.max_jobs_per_keyword}")
    print(f"   æœ€å¤§é¡µæ•°: {pipeline_config.max_pages}")
    print()
    
    # è®°å½•å¼€å§‹æ—¶é—´
    start_time = datetime.now()
    print(f"â° å¼€å§‹æ—¶é—´: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
    
    try:
        # åªæ‰§è¡ŒèŒä½æå–é˜¶æ®µ
        print("ğŸ¯ åªæ‰§è¡ŒèŒä½æå–é˜¶æ®µä»¥æµ‹è¯•ç‚¹å‡»æ”¹è¿›...")
        extraction_result = await controller._execute_job_extraction(pipeline_config)
        
        # è®°å½•ç»“æŸæ—¶é—´
        end_time = datetime.now()
        execution_time = (end_time - start_time).total_seconds()
        
        print("\n" + "=" * 60)
        print("ğŸ‰ æµ‹è¯•å®Œæˆ!")
        print(f"â° ç»“æŸæ—¶é—´: {end_time.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"â±ï¸ æ€»æ‰§è¡Œæ—¶é—´: {execution_time:.2f} ç§’")
        
        # æ‰“å°æå–ç»“æœ
        print(f"\nğŸ“‹ èŒä½æå–ç»“æœ:")
        print(f"   æå–æˆåŠŸ: {extraction_result.get('success', False)}")
        print(f"   æå–æ•°é‡: {extraction_result.get('total_extracted', 0)}")
        print(f"   æå–è€—æ—¶: {extraction_result.get('extraction_time', 0):.2f} ç§’")
        
        # æ£€æŸ¥å¤±è´¥jobæ—¥å¿—
        failed_jobs = check_failed_jobs_log()
        
        # è®¡ç®—æˆåŠŸç‡
        total_expected = 20  # ç¬¬ä¸€é¡µé¢„æœŸ20ä¸ªèŒä½
        actual_extracted = extraction_result.get('total_extracted', 0)
        failed_count = len(failed_jobs)
        success_rate = (actual_extracted / total_expected) * 100 if total_expected > 0 else 0
        
        print(f"\nğŸ“ˆ æ”¹è¿›æ•ˆæœåˆ†æ:")
        print(f"   é¢„æœŸèŒä½æ•°: {total_expected}")
        print(f"   æˆåŠŸæå–æ•°: {actual_extracted}")
        print(f"   å¤±è´¥æ•°é‡: {failed_count}")
        print(f"   æˆåŠŸç‡: {success_rate:.1f}%")
        
        if success_rate >= 95:
            print("ğŸ‰ ä¼˜ç§€ï¼æˆåŠŸç‡è¾¾åˆ°95%ä»¥ä¸Š")
        elif success_rate >= 90:
            print("âœ… è‰¯å¥½ï¼æˆåŠŸç‡è¾¾åˆ°90%ä»¥ä¸Š")
        elif success_rate >= 85:
            print("âš ï¸ ä¸€èˆ¬ï¼ŒæˆåŠŸç‡åœ¨85%ä»¥ä¸Šï¼Œè¿˜æœ‰æ”¹è¿›ç©ºé—´")
        else:
            print("âŒ éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–ï¼ŒæˆåŠŸç‡ä½äº85%")
        
        return {
            'success_rate': success_rate,
            'extracted_count': actual_extracted,
            'failed_count': failed_count,
            'execution_time': execution_time
        }
        
    except Exception as e:
        end_time = datetime.now()
        execution_time = (end_time - start_time).total_seconds()
        
        print(f"\nâŒ æµ‹è¯•å¤±è´¥!")
        print(f"â° å¤±è´¥æ—¶é—´: {end_time.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"â±ï¸ æ‰§è¡Œæ—¶é—´: {execution_time:.2f} ç§’")
        print(f"âŒ é”™è¯¯ä¿¡æ¯: {e}")
        
        # ä»ç„¶æ£€æŸ¥å¤±è´¥jobæ—¥å¿—
        check_failed_jobs_log()
        
        import traceback
        print(f"\nğŸ” è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
        traceback.print_exc()
        
        return None

if __name__ == "__main__":
    # è¿è¡Œæµ‹è¯•
    result = asyncio.run(test_click_improvements())
    
    if result:
        print(f"\nğŸ“‹ æµ‹è¯•æ€»ç»“:")
        print(f"   æˆåŠŸç‡: {result['success_rate']:.1f}%")
        print(f"   æå–æ•°é‡: {result['extracted_count']}")
        print(f"   å¤±è´¥æ•°é‡: {result['failed_count']}")
        print(f"   æ‰§è¡Œæ—¶é—´: {result['execution_time']:.2f} ç§’")