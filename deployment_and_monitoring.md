
# éƒ¨ç½²ä¸ç›‘æ§è¿ç»´æŒ‡å—

## ğŸš€ ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²

### éƒ¨ç½²æ¶æ„è®¾è®¡

```mermaid
graph TB
    subgraph "è´Ÿè½½å‡è¡¡å±‚"
        LB[è´Ÿè½½å‡è¡¡å™¨]
    end
    
    subgraph "åº”ç”¨å±‚"
        APP1[åº”ç”¨å®ä¾‹1]
        APP2[åº”ç”¨å®ä¾‹2]
        APP3[åº”ç”¨å®ä¾‹3]
    end
    
    subgraph "æ•°æ®å±‚"
        DB[(ä¸»æ•°æ®åº“)]
        VDB[(å‘é‡æ•°æ®åº“)]
        CACHE[(Redisç¼“å­˜)]
    end
    
    subgraph "ç›‘æ§å±‚"
        PROM[Prometheus]
        GRAF[Grafana]
        ALERT[AlertManager]
    end
    
    subgraph "æ—¥å¿—å±‚"
        ELK[ELK Stack]
    end
    
    LB --> APP1
    LB --> APP2
    LB --> APP3
    
    APP1 --> DB
    APP1 --> VDB
    APP1 --> CACHE
    
    APP2 --> DB
    APP2 --> VDB
    APP2 --> CACHE
    
    APP3 --> DB
    APP3 --> VDB
    APP3 --> CACHE
    
    PROM --> APP1
    PROM --> APP2
    PROM --> APP3
    
    GRAF --> PROM
    ALERT --> PROM
    
    APP1 --> ELK
    APP2 --> ELK
    APP3 --> ELK
```

### Dockerå®¹å™¨åŒ–éƒ¨ç½²

#### ä¸»åº”ç”¨Dockerfile

```dockerfile
# Dockerfile
FROM python:3.9-slim

# è®¾ç½®å·¥ä½œç›®å½•
WORKDIR /app

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    wget \
    curl \
    chromium \
    chromium-driver \
    && rm -rf /var/lib/apt/lists/*

# å¤åˆ¶ä¾èµ–æ–‡ä»¶
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# å¤åˆ¶åº”ç”¨ä»£ç 
COPY . .

# åˆ›å»ºå¿…è¦ç›®å½•
RUN mkdir -p logs data/chroma_db backups

# è®¾ç½®ç¯å¢ƒå˜é‡
ENV PYTHONPATH=/app
ENV INTEGRATION_ENV=production
ENV CHROME_BIN=/usr/bin/chromium
ENV CHROME_DRIVER=/usr/bin/chromedriver

# æš´éœ²ç«¯å£
EXPOSE 8000

# å¥åº·æ£€æŸ¥
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# å¯åŠ¨å‘½ä»¤
CMD ["python", "-m", "src.integration.main"]
```

#### Docker Composeé…ç½®

```yaml
# docker-compose.yml
version: '3.8'

services:
  # ä¸»åº”ç”¨æœåŠ¡
  app:
    build: .
    ports:
      - "8000:8000"
    environment:
      - INTEGRATION_ENV=production
      - DATABASE_URL=postgresql://user:password@postgres:5432/resume_system
      - REDIS_URL=redis://redis:6379
      - VECTOR_DB_PATH=/app/data/chroma_db
    volumes:
      - ./data:/app/data
      - ./logs:/app/logs
      - ./config:/app/config
    depends_on:
      - postgres
      - redis
      - chromadb
    restart: unless-stopped
    deploy:
      replicas: 3
      resources:
        limits:
          cpus: '2.0'
          memory: 4G
        reservations:
          cpus: '1.0'
          memory: 2G

  # æ•°æ®åº“æœåŠ¡
  postgres:
    image: postgres:13
    environment:
      - POSTGRES_DB=resume_system
      - POSTGRES_USER=user
      - POSTGRES_PASSWORD=password
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./init.sql:/docker-entrypoint-initdb.d/init.sql
    ports:
      - "5432:5432"
    restart: unless-stopped

  # Redisç¼“å­˜
  redis:
    image: redis:6-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    restart: unless-stopped
    command: redis-server --appendonly yes

  # å‘é‡æ•°æ®åº“
  chromadb:
    image: chromadb/chroma:latest
    ports:
      - "8001:8000"
    volumes:
      - chroma_data:/chroma/chroma
    environment:
      - CHROMA_SERVER_HOST=0.0.0.0
      - CHROMA_SERVER_HTTP_PORT=8000
    restart: unless-stopped

  # è´Ÿè½½å‡è¡¡å™¨
  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
      - ./ssl:/etc/nginx/ssl
    depends_on:
      - app
    restart: unless-stopped

  # ç›‘æ§æœåŠ¡
  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
    restart: unless-stopped

  # å¯è§†åŒ–ç›‘æ§
  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin123
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana/dashboards:/etc/grafana/provisioning/dashboards
      - ./monitoring/grafana/datasources:/etc/grafana/provisioning/datasources
    depends_on:
      - prometheus
    restart: unless-stopped

volumes:
  postgres_data:
  redis_data:
  chroma_data:
  prometheus_data:
  grafana_data:
```

### Kuberneteséƒ¨ç½²é…ç½®

#### åº”ç”¨éƒ¨ç½²æ¸…å•

```yaml
# k8s/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: resume-system
  labels:
    app: resume-system
spec:
  replicas: 3
  selector:
    matchLabels:
      app: resume-system
  template:
    metadata:
      labels:
        app: resume-system
    spec:
      containers:
      - name: resume-system
        image: resume-system:latest
        ports:
        - containerPort: 8000
        env:
        - name: INTEGRATION_ENV
          value: "production"
        - name: DATABASE_URL
          valueFrom:
            secretKeyRef:
              name: db-secret
              key: url
        - name: REDIS_URL
          value: "redis://redis-service:6379"
        resources:
          requests:
            memory: "2Gi"
            cpu: "1000m"
          limits:
            memory: "4Gi"
            cpu: "2000m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 60
          periodSeconds: 30
        readinessProbe:
          httpGet:
            path: /ready
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
        volumeMounts:
        - name: data-volume
          mountPath: /app/data
        - name: config-volume
          mountPath: /app/config
      volumes:
      - name: data-volume
        persistentVolumeClaim:
          claimName: resume-system-pvc
      - name: config-volume
        configMap:
          name: resume-system-config
---
apiVersion: v1
kind: Service
metadata:
  name: resume-system-service
spec:
  selector:
    app: resume-system
  ports:
  - protocol: TCP
    port: 80
    targetPort: 8000
  type: LoadBalancer
```

#### é…ç½®ç®¡ç†

```yaml
# k8s/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: resume-system-config
data:
  integration_config.yaml: |
    integration_system:
      global:
        system_name: "æ™ºèƒ½ç®€å†æŠ•é€’ç³»ç»Ÿ"
        version: "2.0.0"
        environment: "production"
        debug_mode: false
        log_level: "INFO"
      
      master_controller:
        max_concurrent_pipelines: 5
        pipeline_timeout: 3600
        checkpoint_interval: 100
        error_retry_attempts: 3
        
      performance:
        caching:
          enabled: true
          cache_type: "redis"
          cache_size: 10000
          ttl_seconds: 3600
        
        concurrency:
          max_workers: 10
          semaphore_limit: 5
          
      monitoring:
        enabled: true
        metrics_collection_interval: 30
        real_time_dashboard: true
---
apiVersion: v1
kind: Secret
metadata:
  name: db-secret
type: Opaque
data:
  url: cG9zdGdyZXNxbDovL3VzZXI6cGFzc3dvcmRAcG9zdGdyZXM6NTQzMi9yZXN1bWVfc3lzdGVt
```

## ğŸ“Š ç›‘æ§ç³»ç»Ÿè®¾è®¡

### Prometheusç›‘æ§é…ç½®

```yaml
# monitoring/prometheus.yml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

rule_files:
  - "alert_rules.yml"

alerting:
  alertmanagers:
    - static_configs:
        - targets:
          - alertmanager:9093

scrape_configs:
  # åº”ç”¨ç›‘æ§
  - job_name: 'resume-system'
    static_configs:
      - targets: ['app:8000']
    metrics_path: '/metrics'
    scrape_interval: 30s
    
  # ç³»ç»Ÿç›‘æ§
  - job_name: 'node-exporter'
    static_configs:
      - targets: ['node-exporter:9100']
      
  # æ•°æ®åº“ç›‘æ§
  - job_name: 'postgres-exporter'
    static_configs:
      - targets: ['postgres-exporter:9187']
      
  # Redisç›‘æ§
  - job_name: 'redis-exporter'
    static_configs:
      - targets: ['redis-exporter:9121']
```

### å‘Šè­¦è§„åˆ™é…ç½®

```yaml
# monitoring/alert_rules.yml
groups:
- name: resume_system_alerts
  rules:
  # åº”ç”¨å¯ç”¨æ€§å‘Šè­¦
  - alert: ApplicationDown
    expr: up{job="resume-system"} == 0
    for: 1m
    labels:
      severity: critical
    annotations:
      summary: "åº”ç”¨å®ä¾‹ä¸å¯ç”¨"
      description: "åº”ç”¨å®ä¾‹ {{ $labels.instance }} å·²ç»ä¸‹çº¿è¶…è¿‡1åˆ†é’Ÿ"

  # é”™è¯¯ç‡å‘Šè­¦
  - alert: HighErrorRate
    expr: rate(http_requests_total{status=~"5.."}[5m]) > 0.1
    for: 2m
    labels:
      severity: warning
    annotations:
      summary: "é«˜é”™è¯¯ç‡å‘Šè­¦"
      description: "é”™è¯¯ç‡è¶…è¿‡10%ï¼Œå½“å‰å€¼: {{ $value }}"

  # å“åº”æ—¶é—´å‘Šè­¦
  - alert: HighResponseTime
    expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 2
    for: 3m
    labels:
      severity: warning
    annotations:
      summary: "å“åº”æ—¶é—´è¿‡é•¿"
      description: "95%åˆ†ä½å“åº”æ—¶é—´è¶…è¿‡2ç§’ï¼Œå½“å‰å€¼: {{ $value }}s"

  # å†…å­˜ä½¿ç”¨å‘Šè­¦
  - alert: HighMemoryUsage
    expr: (process_resident_memory_bytes / 1024 / 1024) > 3000
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "å†…å­˜ä½¿ç”¨è¿‡é«˜"
      description: "å†…å­˜ä½¿ç”¨è¶…è¿‡3GBï¼Œå½“å‰å€¼: {{ $value }}MB"

  # CPUä½¿ç”¨å‘Šè­¦
  - alert: HighCPUUsage
    expr: rate(process_cpu_seconds_total[5m]) * 100 > 80
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "CPUä½¿ç”¨ç‡è¿‡é«˜"
      description: "CPUä½¿ç”¨ç‡è¶…è¿‡80%ï¼Œå½“å‰å€¼: {{ $value }}%"

  # æ•°æ®åº“è¿æ¥å‘Šè­¦
  - alert: DatabaseConnectionHigh
    expr: pg_stat_activity_count > 80
    for: 2m
    labels:
      severity: warning
    annotations:
      summary: "æ•°æ®åº“è¿æ¥æ•°è¿‡é«˜"
      description: "æ•°æ®åº“è¿æ¥æ•°è¶…è¿‡80ï¼Œå½“å‰å€¼: {{ $value }}"

  # ç£ç›˜ç©ºé—´å‘Šè­¦
  - alert: DiskSpaceLow
    expr: (node_filesystem_avail_bytes / node_filesystem_size_bytes) * 100 < 10
    for: 1m
    labels:
      severity: critical
    annotations:
      summary: "ç£ç›˜ç©ºé—´ä¸è¶³"
      description: "ç£ç›˜å¯ç”¨ç©ºé—´ä½äº10%ï¼Œå½“å‰å€¼: {{ $value }}%"
```

### Grafanaä»ªè¡¨æ¿é…ç½®

```json
{
  "dashboard": {
    "id": null,
    "title": "æ™ºèƒ½ç®€å†æŠ•é€’ç³»ç»Ÿç›‘æ§",
    "tags": ["resume-system"],
    "timezone": "Asia/Shanghai",
    "panels": [
      {
        "id": 1,
        "title": "ç³»ç»Ÿæ¦‚è§ˆ",
        "type": "stat",
        "targets": [
          {
            "expr": "up{job=\"resume-system\"}",
            "legendFormat": "å®ä¾‹çŠ¶æ€"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "color": {
              "mode": "thresholds"
            },
            "thresholds": {
              "steps": [
                {"color": "red", "value": 0},
                {"color": "green", "value": 1}
              ]
            }
          }
        }
      },
      {
        "id": 2,
        "title": "è¯·æ±‚é€Ÿç‡",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(http_requests_total[5m])",
            "legendFormat": "{{method}} {{status}}"
          }
        ]
      },
      {
        "id": 3,
        "title": "å“åº”æ—¶é—´åˆ†å¸ƒ",
        "type": "heatmap",
        "targets": [
          {
            "expr": "rate(http_request_duration_seconds_bucket[5m])",
            "legendFormat": "{{le}}"
          }
        ]
      },
      {
        "id": 4,
        "title": "æµæ°´çº¿æ‰§è¡Œç»Ÿè®¡",
        "type": "graph",
        "targets": [
          {
            "expr": "pipeline_executions_total",
            "legendFormat": "æ€»æ‰§è¡Œæ•°"
          },
          {
            "expr": "pipeline_executions_success_total",
            "legendFormat": "æˆåŠŸæ‰§è¡Œæ•°"
          }
        ]
      },
      {
        "id": 5,
        "title": "èµ„æºä½¿ç”¨æƒ…å†µ",
        "type": "graph",
        "targets": [
          {
            "expr": "process_resident_memory_bytes / 1024 / 1024",
            "legendFormat": "å†…å­˜ä½¿ç”¨(MB)"
          },
          {
            "expr": "rate(process_cpu_seconds_total[5m]) * 100",
            "legendFormat": "CPUä½¿ç”¨ç‡(%)"
          }
        ]
      }
    ],
    "time": {
      "from": "now-1h",
      "to": "now"
    },
    "refresh": "30s"
  }
}
```

## ğŸ“ˆ æ€§èƒ½ç›‘æ§æŒ‡æ ‡

### åº”ç”¨çº§æŒ‡æ ‡

```python
# src/integration/metrics.py
from prometheus_client import Counter, Histogram, Gauge, start_http_server
import time
import functools

# å®šä¹‰ç›‘æ§æŒ‡æ ‡
REQUEST_COUNT = Counter('http_requests_total', 'Total HTTP requests', ['method', 'endpoint', 'status'])
REQUEST_DURATION = Histogram('http_request_duration_seconds', 'HTTP request duration')
PIPELINE_EXECUTIONS = Counter('pipeline_executions_total', 'Total pipeline executions', ['status'])
PIPELINE_DURATION = Histogram('pipeline_duration_seconds', 'Pipeline execution duration')
ACTIVE_PIPELINES = Gauge('active_pipelines', 'Number of active pipelines')
JOB_PROCESSING_RATE = Gauge('job_processing_rate', 'Jobs processed per second')
MEMORY_USAGE = Gauge('memory_usage_bytes', 'Memory usage in bytes')
CPU_USAGE = Gauge('cpu_usage_percent', 'CPU usage percentage')

class MetricsCollector:
    """æŒ‡æ ‡æ”¶é›†å™¨"""
    
    def __init__(self):
        self.start_time = time.time()
        
    def record_request(self, method: str, endpoint: str, status: int, duration: float):
        """è®°å½•HTTPè¯·æ±‚æŒ‡æ ‡"""
        REQUEST_COUNT.labels(method=method, endpoint=endpoint, status=status).inc()
        REQUEST_DURATION.observe(duration)
    
    def record_pipeline_execution(self, status: str, duration: float):
        """è®°å½•æµæ°´çº¿æ‰§è¡ŒæŒ‡æ ‡"""
        PIPELINE_EXECUTIONS.labels(status=status).inc()
        PIPELINE_DURATION.observe(duration)
    
    def update_active_pipelines(self, count: int):
        """æ›´æ–°æ´»è·ƒæµæ°´çº¿æ•°é‡"""
        ACTIVE_PIPELINES.set(count)
    
    def update_processing_rate(self, rate: float):
        """æ›´æ–°å¤„ç†é€Ÿç‡"""
        JOB_PROCESSING_RATE.set(rate)
    
    def update_resource_usage(self, memory_bytes: int, cpu_percent: float):
        """æ›´æ–°èµ„æºä½¿ç”¨æƒ…å†µ"""
        MEMORY_USAGE.set(memory_bytes)
        CPU_USAGE.set(cpu_percent)

def monitor_performance(func):
    """æ€§èƒ½ç›‘æ§è£…é¥°å™¨"""
    @functools.wraps(func)
    async def wrapper(*args, **kwargs):
        start_time = time.time()
        try:
            result = await func(*args, **kwargs)
            duration = time.time() - start_time
            
            # è®°å½•æˆåŠŸæ‰§è¡Œ
            if hasattr(func, '__name__') and 'pipeline' in func.__name__:
                metrics_collector.record_pipeline_execution('success', duration)
            
            return result
        except Exception as e:
            duration = time.time() - start_time
            
            # è®°å½•å¤±è´¥æ‰§è¡Œ
            if hasattr(func, '__name__') and 'pipeline' in func.__name__:
                metrics_collector.record_pipeline_execution('failure', duration)
            
            raise e
    return wrapper

# å…¨å±€æŒ‡æ ‡æ”¶é›†å™¨å®ä¾‹
metrics_collector = MetricsCollector()

def start_metrics_server(port: int = 8001):
    """å¯åŠ¨æŒ‡æ ‡æœåŠ¡å™¨"""
    start_http_server(port)
    logger.info(f"æŒ‡æ ‡æœåŠ¡å™¨å·²å¯åŠ¨ï¼Œç«¯å£: {port}")
```

### ä¸šåŠ¡æŒ‡æ ‡ç›‘æ§

```python
# src/integration/business_metrics.py
from prometheus_client import Counter, Histogram, Gauge

# ä¸šåŠ¡æŒ‡æ ‡å®šä¹‰
JOB_EXTRACTION_COUNT = Counter('job_extraction_total', 'Total jobs extracted', ['source', 'status'])
JOB_EXTRACTION_DURATION = Histogram('job_extraction_duration_seconds', 'Job extraction duration')

RAG_PROCESSING_COUNT = Counter('rag_processing_total', 'Total RAG processing', ['status'])
RAG_PROCESSING_DURATION = Histogram('rag_processing_duration_seconds', 'RAG processing duration')

RESUME_MATCHING_COUNT = Counter('resume_matching_total', 'Total resume matching', ['status'])
RESUME_MATCHING_DURATION = Histogram('resume_matching_duration_seconds', 'Resume matching duration')

SUBMISSION_COUNT = Counter('submission_total', 'Total submissions', ['status'])
SUBMISSION_SUCCESS_RATE = Gauge('submission_success_rate', 'Submission success rate')

VECTOR_DB_OPERATIONS = Counter('vector_db_operations_total', 'Vector DB operations', ['operation', 'status'])
CACHE_OPERATIONS = Counter('cache_operations_total', 'Cache operations', ['operation', 'status'])

class BusinessMetricsCollector:
    """ä¸šåŠ¡æŒ‡æ ‡æ”¶é›†å™¨"""
    
    def record_job_extraction(self, source: str, status: str, duration: float, count: int):
        """è®°å½•èŒä½æå–æŒ‡æ ‡"""
        JOB_EXTRACTION_COUNT.labels(source=source, status=status).inc(count)
        JOB_EXTRACTION_DURATION.observe(duration)
    
    def record_rag_processing(self, status: str, duration: float, count: int):
        """è®°å½•RAGå¤„ç†æŒ‡æ ‡"""
        RAG_PROCESSING_COUNT.labels(status=status).inc(count)
        RAG_PROCESSING_DURATION.observe(duration)
    
    def record_resume_matching(self, status: str, duration: float, matches: int):
        """è®°å½•ç®€å†åŒ¹é…æŒ‡æ ‡"""
        RESUME_MATCHING_COUNT.labels(status=status).inc(matches)
        RESUME_MATCHING_DURATION.observe(duration)
    
    def record_submission(self, status: str, count: int):
        """è®°å½•æŠ•é€’æŒ‡æ ‡"""
        SUBMISSION_COUNT.labels(status=status).inc(count)
    
    def update_submission_success_rate(self, rate: float):
        """æ›´æ–°æŠ•é€’æˆåŠŸç‡"""
        SUBMISSION_SUCCESS_RATE.set(rate)
    
    def record_vector_db_operation(self, operation: str, status: str):
        """è®°å½•å‘é‡æ•°æ®åº“æ“ä½œ"""
        VECTOR_DB_OPERATIONS.labels(operation=operation, status=status).inc()
    
    def record_cache_operation(self, operation: str, status: str):
        """è®°å½•ç¼“å­˜æ“ä½œ"""
        CACHE_OPERATIONS.labels(operation=operation, status=status).inc()

# å…¨å±€ä¸šåŠ¡æŒ‡æ ‡æ”¶é›†å™¨
business_metrics = BusinessMetricsCollector()
```

## ğŸ”§ è¿ç»´è‡ªåŠ¨åŒ–

### å¥åº·æ£€æŸ¥ç«¯ç‚¹

```python
# src/integration/health_check.py
from fastapi import FastAPI, HTTPException
from typing import Dict, Any
import asyncio
import time

app = FastAPI()

class HealthChecker:
    """å¥åº·æ£€æŸ¥å™¨"""
    
    def __init__(self, master_controller):
        self.master_controller = master_controller
        self.last_check_time = None
        self.health_status = {}
    
    async def check_system_health(self) -> Dict[str, Any]:
        """æ£€æŸ¥ç³»ç»Ÿå¥åº·çŠ¶æ€"""
        health_status = {
            'timestamp': time.time(),
            'overall_status': 'healthy',
            'components': {}
        }
        
        try:
            # æ£€æŸ¥æ•°æ®åº“è¿æ¥
            db_status = await self._check_database_health()
            health_status['components']['database'] = db_status
            
            # æ£€æŸ¥å‘é‡æ•°æ®åº“
            vector_db_status = await self._check_vector_db_health()
            health_status['components']['vector_db'] = vector_db_status
            
            # æ£€æŸ¥ç¼“å­˜ç³»ç»Ÿ
            cache_status = await self._check_cache_health()
            health_status['components']['cache'] = cache_status
            
            # æ£€æŸ¥RAGç³»ç»Ÿ
            rag_status = await self._check_rag_system_health()
            health_status['components']['rag_system'] = rag_status
            
            # æ£€æŸ¥å¤–éƒ¨æœåŠ¡
            external_status = await self._check_external_services()
            health_status['components']['external_services'] = external_status
            
            # è®¡ç®—æ•´ä½“çŠ¶æ€
            component_statuses = [comp['status'] for comp in health_status['components'].values()]
            if all(status == 'healthy' for status in component_statuses):
                health_status['overall_status'] = 'healthy'
            elif any(status == 'unhealthy' for status in component_statuses):
                health_status['overall_status'] = 'unhealthy'
            else:
                health_status['overall_status'] = 'degraded'
            
            self.health_status = health_status
            self.last_check_time = time.time()
            
            return health_status
            
        except Exception as e:
            return {
                'timestamp': time.time(),
                'overall_status': 'unhealthy',
                'error': str(e)
            }
    
    async def _check_database_health(self) -> Dict[str, Any]:
        """æ£€æŸ¥æ•°æ®åº“å¥åº·çŠ¶æ€"""
        try:
            # æ‰§è¡Œç®€å•æŸ¥è¯¢æµ‹è¯•è¿æ¥
            result = await self.master_controller.rag_coordinator.db_reader.test_connection()
            return {
                'status': 'healthy' if result else 'unhealthy',
                'response_time': 0.1,  # å®é™…æµ‹é‡
                'details': 'Database connection successful'
            }
        except Exception as e:
            return {
                'status': 'unhealthy',
                'error': str(e)
            }
    
    async def _check_vector_db_health(self) -> Dict[str, Any]:
        """æ£€æŸ¥å‘é‡æ•°æ®åº“å¥åº·çŠ¶æ€"""
        try:
            stats = self.master_controller.rag_coordinator.vector_manager.get_collection_stats()
            return {
                'status': 'healthy',
                'document_count': stats.get('document_count', 0),
                'collection_name': stats.get('collection_name', 'unknown')
            }
        except Exception as e:
            return {
                'status': 'unhealthy',
                'error': str(e)
            }

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    health_checker = HealthChecker(master_controller)
    health_status = await health_checker.check_system_health()
    
    if health_status['overall_status'] == 'healthy':
        return health_status
    else:
        raise HTTPException(status_code=503, detail=health_status)

@app.get("/ready")
async def readiness_check():
    """å°±ç»ªæ£€æŸ¥ç«¯ç‚¹"""
    if master_controller.is_initialized:
        return {"status": "ready"}
    else:
        raise HTTPException(status_code=503, detail="System not ready")

@app.get("/metrics")
async def metrics_endpoint():
    """æŒ‡æ ‡ç«¯ç‚¹"""
    from prometheus_client import generate_latest, CONTENT_TYPE_LATEST
    return Response(generate_latest(), media_type=CONTENT_TYPE_LATEST)
```

### è‡ªåŠ¨åŒ–éƒ¨ç½²è„šæœ¬

```bash
#!/bin/bash
# deploy.sh - è‡ªåŠ¨åŒ–éƒ¨ç½²è„šæœ¬

set -e

# é…ç½®å˜é‡
APP_NAME="resume-system"
VERSION=${1:-latest}
ENVIRONMENT=${2:-production}
NAMESPACE=${3:-default}

echo "å¼€å§‹éƒ¨ç½² $APP_NAME:$VERSION åˆ° $ENVIRONMENT ç¯å¢ƒ"

# 1. æ„å»ºDockeré•œåƒ
echo "æ„å»ºDockeré•œåƒ..."
docker build -t $APP_NAME:$VERSION .
docker tag $APP_NAME:$VERSION $APP_NAME:latest

# 2. æ¨é€åˆ°é•œåƒä»“åº“
echo "æ¨é€é•œåƒåˆ°ä»“åº“..."
docker push $APP_NAME:$VERSION
docker push $APP_NAME:latest

# 3. æ›´æ–°Kubernetesé…ç½®
echo "æ›´æ–°Kubernetesé…ç½®..."
sed -i "s|image: $APP_NAME:.*|image: $APP_NAME:$VERSION|g" k8s/deployment.yaml

# 4. åº”ç”¨é…ç½®
echo "åº”ç”¨Kubernetesé…ç½®..."
kubectl apply -f k8s/namespace.yaml
kubectl apply -f k8s/configmap.yaml
kubectl apply -f k8s/secret.yaml
kubectl apply -f k8s/pvc.yaml
kubectl apply -f k8s/deployment.yaml
kubectl apply -f k8s/service.yaml
kubectl apply -f k8s/ingress.yaml

# 5. ç­‰å¾…éƒ¨ç½²å®Œæˆ
echo "ç­‰å¾…éƒ¨ç½²å®Œæˆ..."
kubectl rollout status deployment/$APP_NAME -n $NAMESPACE --timeout=600s

# 6. éªŒè¯éƒ¨ç½²
echo "éªŒè¯éƒ¨ç½²çŠ¶æ€..."
kubectl get pods -n $NAMESPACE -l app=$APP_NAME
kubectl get services -n $NAMESPACE -l app=$APP_NAME

# 7. è¿è¡Œå¥åº·æ£€æŸ¥
echo "è¿è¡Œå¥åº·æ£€æŸ¥..."
HEALTH_URL=$(kubectl get service $APP_NAME-service -n $NAMESPACE -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
if [ -n "$HEALTH_URL" ]; then
    curl -f http://$HEALTH_URL/health || echo "å¥åº·æ£€æŸ¥å¤±è´¥"
else
    echo "æ— æ³•è·å–æœåŠ¡IPï¼Œè·³è¿‡å¥åº·æ£€æŸ¥"
fi

echo "éƒ¨ç½²å®Œæˆï¼"
```

### å¤‡ä»½å’Œæ¢å¤è„šæœ¬

```bash
#!/bin/bash
# backup.sh - æ•°æ®å¤‡ä»½è„šæœ¬

set -e

BACKUP_DIR="/backups"
DATE=$(date +%Y%m%d_%H%M%S)
APP_NAME="resume-system"

echo "å¼€å§‹æ•°æ®å¤‡ä»½ - $DATE"

# 1. åˆ›å»ºå¤‡ä»½ç›®å½•
mkdir -p $BACKUP_DIR/$DATE

# 2. å¤‡ä»½æ•°æ®åº“
echo "å¤‡ä»½PostgreSQLæ•°æ®åº“..."
kubectl exec -n default deployment/postgres -- pg_dump -U user resume_system > $BACKUP_DIR/$DATE/database.sql

# 3. å¤‡ä»½å‘é‡æ•°æ®åº“
echo "å¤‡ä»½ChromaDBæ•°æ®..."
kubectl cp default/chromadb-pod:/chroma/chroma $BACKUP_DIR/$DATE/chroma_data

# 4. å¤‡ä»½é…ç½®æ–‡ä»¶
echo "å¤‡ä»½é…ç½®æ–‡ä»¶..."
kubectl get configmap resume-system-config -o yaml > $BACKUP_DIR/$DATE/configmap.yaml
kubectl get secret db-secret -o yaml > $BACKUP_DIR/$DATE/secret.yaml

# 5. å¤‡ä»½åº”ç”¨æ•°æ®
echo "å¤‡ä»½åº”ç”¨æ•°æ®..."
kubectl cp default/resume-system-pod:/app/data $BACKUP_DIR/$DATE/app_data

# 6. å‹ç¼©å¤‡ä»½
echo "å‹ç¼©å¤‡ä»½æ–‡ä»¶..."
cd $BACKUP_DIR
tar -czf $DATE.tar.gz $DATE/
rm -rf $DATE/

# 7. æ¸…ç†æ—§å¤‡ä»½ï¼ˆä¿ç•™30å¤©ï¼‰
echo