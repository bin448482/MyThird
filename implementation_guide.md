# ç³»ç»Ÿé›†æˆå®æ–½æŒ‡å—

## ğŸ“‹ å®æ–½æ¦‚è¿°

æœ¬æŒ‡å—æä¾›äº†æ™ºèƒ½ç®€å†æŠ•é€’ç³»ç»Ÿç«¯åˆ°ç«¯é›†æˆçš„è¯¦ç»†å®æ–½æ­¥éª¤å’ŒæŠ€æœ¯è§„èŒƒã€‚

## ğŸ—ï¸ æ ¸å¿ƒç»„ä»¶å®æ–½è§„èŒƒ

### 1. ç»Ÿä¸€ä¸»æ§åˆ¶å™¨ (MasterController)

#### æ–‡ä»¶ä½ç½®
```
src/integration/master_controller.py
```

#### æ ¸å¿ƒç±»è®¾è®¡

```python
class MasterController:
    """ç»Ÿä¸€ä¸»æ§åˆ¶å™¨ - åè°ƒæ•´ä¸ªç«¯åˆ°ç«¯æµç¨‹"""
    
    def __init__(self, config: Dict[str, Any]):
        # åˆå§‹åŒ–æ‰€æœ‰å­æ¨¡å—
        self.job_extractor = ContentExtractor(config)
        self.rag_coordinator = RAGSystemCoordinator(config)
        self.resume_matcher = GenericResumeJobMatcher(config)
        self.decision_engine = DecisionEngine(config)
        self.auto_submitter = AutoSubmissionEngine(config)
        self.data_bridge = DataBridge(config)
        self.job_scheduler = JobScheduler(config)
    
    async def run_full_pipeline(self, resume_profile, pipeline_config):
        """æ‰§è¡Œå®Œæ•´çš„ç«¯åˆ°ç«¯æµæ°´çº¿"""
        # é˜¶æ®µ1: èŒä½æå–
        extraction_result = await self._execute_job_extraction(pipeline_config)
        
        # é˜¶æ®µ2: RAGå¤„ç†
        rag_result = await self._execute_rag_processing(extraction_result)
        
        # é˜¶æ®µ3: ç®€å†åŒ¹é…
        matching_result = await self._execute_resume_matching(rag_result, resume_profile)
        
        # é˜¶æ®µ4: æ™ºèƒ½å†³ç­–
        decision_result = await self._execute_intelligent_decision(matching_result)
        
        # é˜¶æ®µ5: è‡ªåŠ¨æŠ•é€’
        submission_result = await self._execute_auto_submission(decision_result)
        
        return self._generate_execution_report(extraction_result, rag_result, matching_result, submission_result)
```

#### å…³é”®æ–¹æ³•å®ç°

```python
async def _execute_job_extraction(self, pipeline_config):
    """æ‰§è¡ŒèŒä½æå–é˜¶æ®µ"""
    try:
        extraction_tasks = []
        for keyword in pipeline_config.search_keywords:
            task = self.job_extractor.extract_from_keyword(
                keyword=keyword,
                max_results=pipeline_config.max_jobs_per_keyword,
                max_pages=pipeline_config.max_pages
            )
            extraction_tasks.append(task)
        
        # å¹¶å‘æ‰§è¡Œæå–ä»»åŠ¡
        results = await asyncio.gather(*extraction_tasks, return_exceptions=True)
        
        # åˆå¹¶å’ŒéªŒè¯ç»“æœ
        combined_results = self._merge_extraction_results(results)
        
        return {
            'success': True,
            'total_extracted': len(combined_results),
            'jobs': combined_results,
            'extraction_time': time.time() - start_time
        }
    except Exception as e:
        logger.error(f"èŒä½æå–å¤±è´¥: {e}")
        return {'success': False, 'error': str(e)}

async def _execute_rag_processing(self, extraction_result):
    """æ‰§è¡ŒRAGå¤„ç†é˜¶æ®µ"""
    if not extraction_result['success']:
        return {'success': False, 'error': 'Extraction failed'}
    
    try:
        # å°†æå–çš„èŒä½æ•°æ®è½¬æ¢ä¸ºRAGè¾“å…¥æ ¼å¼
        rag_input = self.data_bridge.transform_extraction_to_rag(extraction_result)
        
        # æ‰¹é‡å¤„ç†èŒä½æ•°æ®
        rag_result = await self.rag_coordinator.import_database_jobs(
            batch_size=50,
            force_reprocess=False
        )
        
        return {
            'success': True,
            'processed_count': rag_result.get('total_imported', 0),
            'processing_time': rag_result.get('processing_time', 0),
            'success_rate': rag_result.get('success_rate', 0)
        }
    except Exception as e:
        logger.error(f"RAGå¤„ç†å¤±è´¥: {e}")
        return {'success': False, 'error': str(e)}

async def _execute_resume_matching(self, rag_result, resume_profile):
    """æ‰§è¡Œç®€å†åŒ¹é…é˜¶æ®µ"""
    if not rag_result['success']:
        return {'success': False, 'error': 'RAG processing failed'}
    
    try:
        # æ‰§è¡Œç®€å†åŒ¹é…
        matching_result = await self.resume_matcher.find_matching_jobs(
            resume_profile=resume_profile,
            top_k=50
        )
        
        return {
            'success': True,
            'total_matches': matching_result.matching_summary.total_matches,
            'high_priority': matching_result.matching_summary.high_priority,
            'medium_priority': matching_result.matching_summary.medium_priority,
            'low_priority': matching_result.matching_summary.low_priority,
            'matches': matching_result.matches,
            'processing_time': matching_result.matching_summary.processing_time
        }
    except Exception as e:
        logger.error(f"ç®€å†åŒ¹é…å¤±è´¥: {e}")
        return {'success': False, 'error': str(e)}
```

### 2. æ•°æ®ä¼ é€’æ¥å£ (DataBridge)

#### æ–‡ä»¶ä½ç½®
```
src/integration/data_bridge.py
```

#### æ ¸å¿ƒåŠŸèƒ½è®¾è®¡

```python
@dataclass
class PipelineData:
    """æµæ°´çº¿æ•°æ®å®¹å™¨"""
    pipeline_id: str
    resume_profile: GenericResumeProfile
    config: PipelineConfig
    start_time: float
    extraction_result: Optional[Dict] = None
    rag_result: Optional[Dict] = None
    matching_result: Optional[Dict] = None
    decision_result: Optional[Dict] = None
    submission_result: Optional[Dict] = None
    metadata: Dict[str, Any] = field(default_factory=dict)

class DataBridge:
    """æ•°æ®ä¼ é€’æ¡¥æ¥å™¨"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.validation_enabled = config.get('data_bridge', {}).get('validation_enabled', True)
        self.transformation_cache = {}
    
    def transform_extraction_to_rag(self, extraction_result: Dict) -> Dict:
        """è½¬æ¢èŒä½æå–ç»“æœä¸ºRAGè¾“å…¥æ ¼å¼"""
        if not self._validate_extraction_result(extraction_result):
            raise ValueError("Invalid extraction result format")
        
        jobs = extraction_result.get('jobs', [])
        transformed_jobs = []
        
        for job in jobs:
            transformed_job = {
                'job_id': job.get('id') or self._generate_job_id(job),
                'title': job.get('title', ''),
                'company': job.get('company', ''),
                'location': job.get('location', ''),
                'description': job.get('description', ''),
                'requirements': job.get('requirements', ''),
                'salary': job.get('salary', ''),
                'url': job.get('url', ''),
                'source': job.get('source', 'extraction'),
                'extracted_at': datetime.now().isoformat()
            }
            transformed_jobs.append(transformed_job)
        
        return {
            'jobs': transformed_jobs,
            'total_count': len(transformed_jobs),
            'transformation_time': datetime.now().isoformat()
        }
    
    def transform_rag_to_matching(self, rag_result: Dict) -> Dict:
        """è½¬æ¢RAGç»“æœä¸ºåŒ¹é…è¾“å…¥æ ¼å¼"""
        if not self._validate_rag_result(rag_result):
            raise ValueError("Invalid RAG result format")
        
        return {
            'processed_jobs_count': rag_result.get('processed_count', 0),
            'vector_db_ready': rag_result.get('success', False),
            'processing_quality': rag_result.get('success_rate', 0),
            'ready_for_matching': True
        }
```

### 3. ä½œä¸šè°ƒåº¦å™¨ (JobScheduler)

#### æ–‡ä»¶ä½ç½®
```
src/integration/job_scheduler.py
```

#### æ ¸å¿ƒè®¾è®¡

```python
class JobScheduler:
    """ä½œä¸šè°ƒåº¦å™¨ - ç®¡ç†ä»»åŠ¡é˜Ÿåˆ—å’Œæ‰§è¡Œé¡ºåº"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.task_queue = asyncio.Queue(maxsize=config.get('queue_size', 1000))
        self.running_tasks = {}
        self.completed_tasks = {}
        self.failed_tasks = {}
        self.max_concurrent = config.get('max_concurrent_jobs', 10)
        self.semaphore = asyncio.Semaphore(self.max_concurrent)
    
    async def schedule_pipeline_task(self, task_type: str, task_data: Dict, priority: int = 1):
        """è°ƒåº¦æµæ°´çº¿ä»»åŠ¡"""
        task = PipelineTask(
            task_id=f"{task_type}_{int(time.time())}",
            task_type=task_type,
            data=task_data,
            priority=priority,
            created_at=datetime.now()
        )
        
        await self.task_queue.put(task)
        logger.info(f"ä»»åŠ¡å·²è°ƒåº¦: {task.task_id}")
        return task.task_id
    
    async def execute_task_batch(self, tasks: List[PipelineTask]):
        """æ‰¹é‡æ‰§è¡Œä»»åŠ¡"""
        async def execute_single_task(task):
            async with self.semaphore:
                try:
                    result = await self._execute_task(task)
                    self.completed_tasks[task.task_id] = result
                    return result
                except Exception as e:
                    self.failed_tasks[task.task_id] = str(e)
                    logger.error(f"ä»»åŠ¡æ‰§è¡Œå¤±è´¥ {task.task_id}: {e}")
                    return None
        
        # å¹¶å‘æ‰§è¡Œä»»åŠ¡
        results = await asyncio.gather(
            *[execute_single_task(task) for task in tasks],
            return_exceptions=True
        )
        
        return results
```

### 4. æ™ºèƒ½å†³ç­–å¼•æ“ (DecisionEngine)

#### æ–‡ä»¶ä½ç½®
```
src/integration/decision_engine.py
```

#### æ ¸å¿ƒç®—æ³•è®¾è®¡

```python
class DecisionEngine:
    """æ™ºèƒ½å†³ç­–å¼•æ“ - åŸºäºå¤šç»´åº¦è¯„åˆ†çš„æŠ•é€’å†³ç­–"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.decision_criteria = config.get('decision_criteria', {})
        self.learning_enabled = config.get('enable_learning', True)
        self.decision_history = []
    
    async def make_submission_decisions(self, matching_result: Dict) -> Dict:
        """åˆ¶å®šæŠ•é€’å†³ç­–"""
        matches = matching_result.get('matches', [])
        decisions = []
        
        for match in matches:
            decision = await self._evaluate_single_match(match)
            decisions.append(decision)
        
        # åº”ç”¨å…¨å±€çº¦æŸ
        final_decisions = self._apply_global_constraints(decisions)
        
        return {
            'total_evaluated': len(matches),
            'recommended_submissions': len([d for d in final_decisions if d['should_submit']]),
            'decisions': final_decisions,
            'decision_time': datetime.now().isoformat()
        }
    
    async def _evaluate_single_match(self, match: Dict) -> Dict:
        """è¯„ä¼°å•ä¸ªåŒ¹é…ç»“æœ"""
        # å¤šç»´åº¦è¯„åˆ†
        scores = {
            'match_score': match.get('overall_score', 0),
            'company_reputation': await self._evaluate_company_reputation(match.get('company', '')),
            'salary_attractiveness': self._evaluate_salary_attractiveness(match.get('salary_range', {})),
            'location_preference': self._evaluate_location_preference(match.get('location', '')),
            'career_growth_potential': self._evaluate_career_growth(match),
            'application_competition': await self._evaluate_competition_level(match)
        }
        
        # åŠ æƒè®¡ç®—æœ€ç»ˆåˆ†æ•°
        weights = self.decision_criteria.get('weights', {})
        final_score = sum(scores[key] * weights.get(key, 0.1) for key in scores)
        
        # å†³ç­–é€»è¾‘
        should_submit = final_score >= self.decision_criteria.get('submission_threshold', 0.7)
        
        return {
            'job_id': match.get('job_id'),
            'job_title': match.get('job_title'),
            'company': match.get('company'),
            'final_score': final_score,
            'dimension_scores': scores,
            'should_submit': should_submit,
            'submission_priority': self._calculate_priority(final_score),
            'decision_reasoning': self._generate_reasoning(scores, should_submit)
        }
```

### 5. è‡ªåŠ¨æŠ•é€’å¼•æ“ (AutoSubmissionEngine)

#### æ–‡ä»¶ä½ç½®
```
src/integration/auto_submission_engine.py
```

#### æ ¸å¿ƒåŠŸèƒ½è®¾è®¡

```python
class AutoSubmissionEngine:
    """è‡ªåŠ¨æŠ•é€’å¼•æ“ - æ‰§è¡Œæ™ºèƒ½æŠ•é€’æ“ä½œ"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.submission_config = config.get('auto_submission', {})
        self.browser_manager = None
        self.session_manager = None
        self.submission_stats = {
            'total_attempts': 0,
            'successful_submissions': 0,
            'failed_submissions': 0,
            'daily_submission_count': 0
        }
    
    async def initialize(self):
        """åˆå§‹åŒ–æŠ•é€’å¼•æ“"""
        # åˆå§‹åŒ–æµè§ˆå™¨ç®¡ç†å™¨
        from ..auth.browser_manager import BrowserManager
        from ..auth.session_manager import SessionManager
        
        self.browser_manager = BrowserManager(self.config)
        self.session_manager = SessionManager(self.config)
        
        # å¯åŠ¨æµè§ˆå™¨å¹¶ç­‰å¾…ç™»å½•
        await self.browser_manager.start_browser()
        await self._wait_for_user_login()
    
    async def submit_applications(self, decision_result: Dict) -> Dict:
        """æ‰¹é‡æäº¤ç®€å†ç”³è¯·"""
        decisions = decision_result.get('decisions', [])
        submission_candidates = [d for d in decisions if d['should_submit']]
        
        # æŒ‰ä¼˜å…ˆçº§æ’åº
        submission_candidates.sort(key=lambda x: x['submission_priority'], reverse=True)
        
        # åº”ç”¨æ¯æ—¥é™åˆ¶
        daily_limit = self.submission_config.get('max_submissions_per_day', 50)
        if self.submission_stats['daily_submission_count'] >= daily_limit:
            logger.warning(f"å·²è¾¾åˆ°æ¯æ—¥æŠ•é€’é™åˆ¶: {daily_limit}")
            return self._create_submission_result([], "Daily limit reached")
        
        # æ‰§è¡ŒæŠ•é€’
        submission_results = []
        for candidate in submission_candidates[:daily_limit]:
            if self.submission_stats['daily_submission_count'] >= daily_limit:
                break
            
            result = await self._submit_single_application(candidate)
            submission_results.append(result)
            
            # æ·»åŠ å»¶è¿Ÿé¿å…è¢«æ£€æµ‹
            delay = self.submission_config.get('submission_delay', 5)
            await asyncio.sleep(delay)
        
        return self._create_submission_result(submission_results)
    
    async def _submit_single_application(self, candidate: Dict) -> Dict:
        """æäº¤å•ä¸ªç”³è¯·"""
        try:
            job_url = candidate.get('job_url')
            if not job_url:
                return {'success': False, 'error': 'Missing job URL'}
            
            # å¯¼èˆªåˆ°èŒä½é¡µé¢
            await self.browser_manager.navigate_to_job(job_url)
            
            # æ£€æŸ¥æ˜¯å¦å·²ç»ç”³è¯·è¿‡
            if await self._check_already_applied():
                return {'success': False, 'error': 'Already applied'}
            
            # æ‰§è¡ŒæŠ•é€’æ“ä½œ
            success = await self._perform_application_submission()
            
            if success:
                self.submission_stats['successful_submissions'] += 1
                self.submission_stats['daily_submission_count'] += 1
                
                return {
                    'success': True,
                    'job_id': candidate.get('job_id'),
                    'job_title': candidate.get('job_title'),
                    'company': candidate.get('company'),
                    'submitted_at': datetime.now().isoformat()
                }
            else:
                self.submission_stats['failed_submissions'] += 1
                return {'success': False, 'error': 'Submission failed'}
                
        except Exception as e:
            logger.error(f"æŠ•é€’å¤±è´¥: {e}")
            self.submission_stats['failed_submissions'] += 1
            return {'success': False, 'error': str(e)}
```

## ğŸ”„ é›†æˆå·¥ä½œæµç¨‹

### å®Œæ•´æ‰§è¡Œæµç¨‹å›¾

```mermaid
graph TD
    A[å¼€å§‹] --> B[åˆå§‹åŒ–ç³»ç»Ÿ]
    B --> C[åŠ è½½ç®€å†æ¡£æ¡ˆ]
    C --> D[é…ç½®æµæ°´çº¿å‚æ•°]
    D --> E[å¯åŠ¨ä¸»æ§åˆ¶å™¨]
    
    E --> F[é˜¶æ®µ1: èŒä½æå–]
    F --> G[éªŒè¯æå–ç»“æœ]
    G --> H{æå–æˆåŠŸ?}
    H -->|å¦| I[é”™è¯¯å¤„ç†]
    H -->|æ˜¯| J[é˜¶æ®µ2: RAGå¤„ç†]
    
    J --> K[æ‰¹é‡å¤„ç†èŒä½]
    K --> L[å‘é‡åŒ–å­˜å‚¨]
    L --> M{RAGå¤„ç†æˆåŠŸ?}
    M -->|å¦| I
    M -->|æ˜¯| N[é˜¶æ®µ3: ç®€å†åŒ¹é…]
    
    N --> O[æ‰§è¡Œè¯­ä¹‰åŒ¹é…]
    O --> P[ç”ŸæˆåŒ¹é…æŠ¥å‘Š]
    P --> Q{åŒ¹é…æˆåŠŸ?}
    Q -->|å¦| I
    Q -->|æ˜¯| R[é˜¶æ®µ4: æ™ºèƒ½å†³ç­–]
    
    R --> S[å¤šç»´åº¦è¯„åˆ†]
    S --> T[åº”ç”¨å†³ç­–è§„åˆ™]
    T --> U[ç”ŸæˆæŠ•é€’å»ºè®®]
    U --> V[é˜¶æ®µ5: è‡ªåŠ¨æŠ•é€’]
    
    V --> W[æ‰¹é‡æŠ•é€’æ‰§è¡Œ]
    W --> X[æ›´æ–°æŠ•é€’çŠ¶æ€]
    X --> Y[ç”Ÿæˆæ‰§è¡ŒæŠ¥å‘Š]
    Y --> Z[ç»“æŸ]
    
    I --> AA[è®°å½•é”™è¯¯]
    AA --> BB[å°è¯•æ¢å¤]
    BB --> CC{æ¢å¤æˆåŠŸ?}
    CC -->|æ˜¯| D
    CC -->|å¦| Z
```

### æ•°æ®æµè½¬è§„èŒƒ

```python
# é˜¶æ®µ1: èŒä½æå– -> RAGå¤„ç†
extraction_output = {
    'success': True,
    'total_extracted': 150,
    'jobs': [
        {
            'id': 'job_001',
            'title': 'Pythonå¼€å‘å·¥ç¨‹å¸ˆ',
            'company': 'ç§‘æŠ€å…¬å¸A',
            'location': 'åŒ—äº¬',
            'description': '...',
            'requirements': '...',
            'salary': '20-30K',
            'url': 'https://...'
        }
    ]
}

# é˜¶æ®µ2: RAGå¤„ç† -> ç®€å†åŒ¹é…
rag_output = {
    'success': True,
    'processed_count': 145,
    'processing_time': 120.5,
    'success_rate': 0.97,
    'vector_db_stats': {
        'document_count': 725,
        'collection_name': 'job_positions'
    }
}

# é˜¶æ®µ3: ç®€å†åŒ¹é… -> æ™ºèƒ½å†³ç­–
matching_output = {
    'success': True,
    'total_matches': 45,
    'high_priority': 12,
    'medium_priority': 20,
    'low_priority': 13,
    'matches': [
        {
            'job_id': 'job_001',
            'overall_score': 0.85,
            'match_level': 'excellent',
            'recommendation_priority': 'high'
        }
    ]
}

# é˜¶æ®µ4: æ™ºèƒ½å†³ç­– -> è‡ªåŠ¨æŠ•é€’
decision_output = {
    'total_evaluated': 45,
    'recommended_submissions': 15,
    'decisions': [
        {
            'job_id': 'job_001',
            'should_submit': True,
            'final_score': 0.82,
            'submission_priority': 'high'
        }
    ]
}

# é˜¶æ®µ5: è‡ªåŠ¨æŠ•é€’ç»“æœ
submission_output = {
    'total_attempts': 15,
    'successful_submissions': 12,
    'failed_submissions': 3,
    'submission_details': [
        {
            'job_id': 'job_001',
            'success': True,
            'submitted_at': '2025-08-22T13:30:00'
        }
    ]
}
```

## ğŸ“Š ç›‘æ§å’ŒæŠ¥å‘Š

### å®æ—¶ç›‘æ§æŒ‡æ ‡

```python
monitoring_metrics = {
    'pipeline_status': {
        'current_stage': 'rag_processing',
        'progress_percentage': 65.5,
        'estimated_completion': '2025-08-22T14:15:00'
    },
    'performance_metrics': {
        'extraction_rate': 2.5,  # jobs/second
        'rag_processing_rate': 1.2,  # jobs/second
        'matching_rate': 5.8,  # matches/second
        'submission_rate': 0.3  # submissions/second
    },
    'resource_usage': {
        'cpu_usage': 45.2,
        'memory_usage': 1024,  # MB
        'disk_usage': 2048  # MB
    },
    'error_statistics': {
        'total_errors': 5,
        'error_rate': 0.03,
        'recovery_rate': 0.8
    }
}
```

### æ‰§è¡ŒæŠ¥å‘Šæ¨¡æ¿

```python
execution_report = {
    'pipeline_summary': {
        'pipeline_id': 'pipeline_1692696000',
        'execution_time': 1800.5,  # seconds
        'total_jobs_processed': 150,
        'success_rate': 0.94
    },
    'stage_results': {
        'extraction': {
            'jobs_extracted': 150,
            'extraction_time': 300.2,
            'success_rate': 1.0
        },
        'rag_processing': {
            'jobs_processed': 145,
            'processing_time': 720.8,
            'success_rate': 0.97
        },
        'matching': {
            'matches_found': 45,
            'matching_time': 180.3,
            'average_score': 0.72
        },
        'decision': {
            'jobs_evaluated': 45,
            'recommended_submissions': 15,
            'decision_time': 45.1
        },
        'submission': {
            'submission_attempts': 15,
            'successful_submissions': 12,
            'submission_time': 450.2
        }
    },
    'recommendations': [
        'å»ºè®®å¢åŠ RAGå¤„ç†çš„æ‰¹æ¬¡å¤§å°ä»¥æé«˜æ•ˆç‡',
        'è€ƒè™‘è°ƒæ•´åŒ¹é…é˜ˆå€¼ä»¥è·å¾—æ›´å¤šå€™é€‰èŒä½',
        'ä¼˜åŒ–æŠ•é€’é—´éš”ä»¥é¿å…è¢«æ£€æµ‹'
    ]
}
```

## ğŸš€ éƒ¨ç½²é…ç½®

### ç”Ÿäº§ç¯å¢ƒé…ç½®

```yaml
# production_config.yaml
integration_system:
  master_controller:
    max_concurrent_jobs: 20
    checkpoint_interval: 50
    error_retry_attempts: 5
    enable_monitoring: true
    
  job_scheduler:
    queue_size: 2000
    batch_size: 100
    priority_levels: 5
    max_concurrent_tasks: 15
    
  data_bridge:
    validation_enabled: true
    transformation_cache: true
    cache_ttl: 7200
    data_retention_days: 90

performance_optimization:
  caching:
    enabled: true
    cache_size: 50000
    ttl_seconds: 7200
    
  concurrency:
    max_workers: 20
    semaphore_limit: 10
    
  database:
    connection_pool_size: 50
    query_timeout: 60
    batch_insert_size: 1000

monitoring:
  metrics_collection: true
  real_time_dashboard: true
  alert_thresholds:
    error_rate: 0.05
    processing_speed: 200
    memory_usage: 0.85
    cpu_usage: 0.80
  
  logging:
    level: INFO
    file_rotation: daily
    max_file_size: 100MB
    retention_days: 30
```

## ğŸ“‹ å®æ–½æ£€æŸ¥æ¸…å•

### å¼€å‘é˜¶æ®µæ£€æŸ¥é¡¹

- [ ] åˆ›å»º `src/integration/` ç›®å½•ç»“æ„
- [ ] å®ç° `MasterController` ç±»
- [ ] å®ç° `DataBridge` ç±»
- [ ] å®ç° `JobScheduler` ç±»
- [ ] å®ç° `DecisionEngine` ç±»
- [ ] å®ç° `AutoSubmissionEngine` ç±»
- [ ] åˆ›å»ºæ•°æ®æ¨¡å‹å’Œæ¥å£å®šä¹‰
- [ ] å®ç°é”™è¯¯å¤„ç†å’Œæ¢å¤æœºåˆ¶
- [ ] æ·»åŠ æ—¥å¿—å’Œç›‘æ§åŠŸèƒ½
- [ ] ç¼–å†™å•å…ƒæµ‹è¯•
- [ ] ç¼–å†™é›†æˆæµ‹è¯•
- [ ] æ€§èƒ½æµ‹è¯•å’Œä¼˜åŒ–

### éƒ¨ç½²é˜¶æ®µæ£€æŸ¥é¡¹

- [ ] é…ç½®ç”Ÿäº§ç¯å¢ƒå‚æ•°
- [ ] è®¾ç½®æ•°æ®åº“è¿æ¥æ± 
- [ ] é…ç½®ç¼“å­˜ç³»ç»Ÿ
- [ ] è®¾ç½®ç›‘æ§å‘Šè­¦
- [ ] é…ç½®æ—¥å¿—è½®è½¬
- [ ] è®¾ç½®å¤‡ä»½ç­–ç•¥
- [ ] é…ç½®è´Ÿè½½å‡è¡¡
- [ ] è®¾ç½®å¥åº·æ£€æŸ¥
- [ ] é…ç½®è‡ªåŠ¨é‡å¯
- [ ] æ–‡æ¡£å’Œè¿ç»´æ‰‹å†Œ

### éªŒæ”¶æµ‹è¯•æ£€æŸ¥é¡¹

- [ ] ç«¯åˆ°ç«¯æµç¨‹æµ‹è¯•
- [ ] æ€§èƒ½åŸºå‡†æµ‹è¯•
- [ ] é”™è¯¯æ¢å¤æµ‹è¯•
- [ ] å¹¶å‘å¤„ç†æµ‹è¯•
- [ ] æ•°æ®ä¸€è‡´æ€§æµ‹è¯•
- [ ] å®‰å…¨æ€§æµ‹è¯•
- [ ] ç”¨æˆ·æ¥å—åº¦æµ‹è¯•
- [ ] ç”Ÿäº§ç¯å¢ƒéªŒè¯

è¿™ä¸ªå®æ–½æŒ‡å—æä¾›äº†å®Œæ•´çš„æŠ€æœ¯è§„èŒƒå’Œå®æ–½æ­¥éª¤ï¼Œå¯ä»¥æŒ‡å¯¼å¼€å‘å›¢é˜Ÿå®Œæˆç³»ç»Ÿé›†æˆå·¥ä½œã€‚